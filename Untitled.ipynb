{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'reppy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-4d633f9f4e50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#from lib.robots import check_robots_txt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshingling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbuild_shingles\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcrawl_url\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatabase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Notebooks\\tech-seo-boost\\lib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m#from crawler import *\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mrobots\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_robots_txt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mshingling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbuild_minhash\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjaccard_similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Notebooks\\tech-seo-boost\\lib\\robots.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m '''\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mreppy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrobots\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRobots\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'reppy'"
     ]
    }
   ],
   "source": [
    "#from lib.robots import check_robots_txt\n",
    "from lib.shingling import build_shingles\n",
    "from lib.renderer import *\n",
    "from lib.crawler import crawl_url\n",
    "from lib.database import *\n",
    "\n",
    "from web.site_generator import build_site_data, build_sites\n",
    "from web.publish import Publish\n",
    "\n",
    "import config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting reppy\n",
      "  Downloading https://files.pythonhosted.org/packages/b0/b9/8bb8a4cd95dfc6038fb721fad95da8e9558ec936688150302cbd7874c45c/reppy-0.4.14.tar.gz (93kB)\n",
      "Requirement already satisfied: cachetools in c:\\users\\jroak\\anaconda3\\lib\\site-packages (from reppy) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil!=2.0,>=1.5 in c:\\users\\jroak\\anaconda3\\lib\\site-packages (from reppy) (2.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\jroak\\anaconda3\\lib\\site-packages (from reppy) (2.19.1)\n",
      "Requirement already satisfied: six in c:\\users\\jroak\\anaconda3\\lib\\site-packages (from reppy) (1.12.0)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in c:\\users\\jroak\\anaconda3\\lib\\site-packages (from requests->reppy) (1.23)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\jroak\\anaconda3\\lib\\site-packages (from requests->reppy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jroak\\anaconda3\\lib\\site-packages (from requests->reppy) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in c:\\users\\jroak\\anaconda3\\lib\\site-packages (from requests->reppy) (2.7)\n",
      "Building wheels for collected packages: reppy\n",
      "  Running setup.py bdist_wheel for reppy: started\n",
      "  Running setup.py bdist_wheel for reppy: finished with status 'error'\n",
      "  Complete output from command C:\\Users\\jroak\\Anaconda3\\python.exe -u -c \"import setuptools, tokenize;__file__='C:\\\\Users\\\\jroak\\\\AppData\\\\Local\\\\Temp\\\\pip-install-9u5p16hk\\\\reppy\\\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d C:\\Users\\jroak\\AppData\\Local\\Temp\\pip-wheel-bf72svqw --python-tag cp37:\n",
      "  Building from Cython\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.7\n",
      "  creating build\\lib.win-amd64-3.7\\reppy\n",
      "  copying reppy\\exceptions.py -> build\\lib.win-amd64-3.7\\reppy\n",
      "  copying reppy\\ttl.py -> build\\lib.win-amd64-3.7\\reppy\n",
      "  copying reppy\\util.py -> build\\lib.win-amd64-3.7\\reppy\n",
      "  copying reppy\\__init__.py -> build\\lib.win-amd64-3.7\\reppy\n",
      "  creating build\\lib.win-amd64-3.7\\reppy\\cache\n",
      "  copying reppy\\cache\\policy.py -> build\\lib.win-amd64-3.7\\reppy\\cache\n",
      "  copying reppy\\cache\\__init__.py -> build\\lib.win-amd64-3.7\\reppy\\cache\n",
      "  running build_ext\n",
      "  skipping 'reppy\\robots.cpp' Cython extension (up-to-date)\n",
      "  building 'reppy.robots' extension\n",
      "  error: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": https://visualstudio.microsoft.com/downloads/\n",
      "  \n",
      "  ----------------------------------------\n",
      "  Running setup.py clean for reppy\n",
      "Failed to build reppy\n",
      "Installing collected packages: reppy\n",
      "  Running setup.py install for reppy: started\n",
      "    Running setup.py install for reppy: finished with status 'error'\n",
      "    Complete output from command C:\\Users\\jroak\\Anaconda3\\python.exe -u -c \"import setuptools, tokenize;__file__='C:\\\\Users\\\\jroak\\\\AppData\\\\Local\\\\Temp\\\\pip-install-9u5p16hk\\\\reppy\\\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record C:\\Users\\jroak\\AppData\\Local\\Temp\\pip-record-8932pi38\\install-record.txt --single-version-externally-managed --compile:\n",
      "    Building from Cython\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.7\n",
      "    creating build\\lib.win-amd64-3.7\\reppy\n",
      "    copying reppy\\exceptions.py -> build\\lib.win-amd64-3.7\\reppy\n",
      "    copying reppy\\ttl.py -> build\\lib.win-amd64-3.7\\reppy\n",
      "    copying reppy\\util.py -> build\\lib.win-amd64-3.7\\reppy\n",
      "    copying reppy\\__init__.py -> build\\lib.win-amd64-3.7\\reppy\n",
      "    creating build\\lib.win-amd64-3.7\\reppy\\cache\n",
      "    copying reppy\\cache\\policy.py -> build\\lib.win-amd64-3.7\\reppy\\cache\n",
      "    copying reppy\\cache\\__init__.py -> build\\lib.win-amd64-3.7\\reppy\\cache\n",
      "    running build_ext\n",
      "    skipping 'reppy\\robots.cpp' Cython extension (up-to-date)\n",
      "    building 'reppy.robots' extension\n",
      "    error: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": https://visualstudio.microsoft.com/downloads/\n",
      "    \n",
      "    ----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Failed building wheel for reppy\n",
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "pywikibot 3.0.dev0 has requirement requests>=2.20.0, but you'll have requests 2.19.1 which is incompatible.\n",
      "Command \"C:\\Users\\jroak\\Anaconda3\\python.exe -u -c \"import setuptools, tokenize;__file__='C:\\\\Users\\\\jroak\\\\AppData\\\\Local\\\\Temp\\\\pip-install-9u5p16hk\\\\reppy\\\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record C:\\Users\\jroak\\AppData\\Local\\Temp\\pip-record-8932pi38\\install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in C:\\Users\\jroak\\AppData\\Local\\Temp\\pip-install-9u5p16hk\\reppy\\\n",
      "You are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install reppy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "\n",
    "data['domain.com'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_config={'type': 'dict'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ordered_storage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-9c8441c11094>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mordered_storage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mb'_keys'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ordered_storage' is not defined"
     ]
    }
   ],
   "source": [
    "ordered_storage(storage_config, name=b'_keys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking up: Python Software\n",
      "\tGot: ActiveState\n",
      "\tGot: Anaconda Python distribution\n",
      "\tGot: Biopython\n",
      "\tGot: CircuitPython\n",
      "\tGot: Comparison of integrated development environments\n",
      "\tGot: Core Python Programming\n",
      "\tGot: Eric software\n",
      "\tGot: History of Python\n",
      "\tGot: IDLE\n",
      "\tGot: IronPython\n",
      "Looking up: Data Science\n",
      "\tGot: Berkeley Institute for Data Science\n",
      "\tGot: Big data\n",
      "\tGot: Black Swan Data\n",
      "\tGot: Chief data officer\n",
      "\tGot: Coding bootcamp\n",
      "\tGot: Committee on Data for Science and Technology\n",
      "\tGot: Consistency database systems\n",
      "\tGot: Customer data platform\n",
      "\tGot: Data\n",
      "\tGot: Dataintensive computing\n",
      "Looking up: Search Engine Optimization\n",
      "\tGot: Archie search engine\n",
      "\tGot: Audio search engine\n",
      "\tGot: Barry Schwartz technologist\n",
      "\tGot: Clean URL\n",
      "\tGot: Contextual advertising\n",
      "\tGot: Danny Sullivan technologist\n",
      "\tGot: Dragonfly search engine\n",
      "\tGot: Fulltext search\n",
      "\tGot: Google Custom Search\n",
      "\tGot: Google Search\n"
     ]
    }
   ],
   "source": [
    "site_data = build_sites(cfg.sg_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python-software'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.basename(site_data['folders'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def add_links(sites, page):\n",
    "\n",
    "    slug    = page['page_slug']\n",
    "    content = page['page_content']\n",
    "\n",
    "    other_pages = [p for fdr in sites for p in sites[fdr] if p['page_slug'] != slug]\n",
    "\n",
    "    for op in other_pages:\n",
    "\n",
    "        op_topic = op['page_topic']\n",
    "        op_url = op['page_url']\n",
    "\n",
    "        content = re.sub(r\"({})+\".format(op_topic), r\"(\\1)[{}]\".format(op_url), content, 1, flags=re.I)\n",
    "\n",
    "\n",
    "    page['page_content'] = content\n",
    "\n",
    "    return page\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = ['aa', 'aaa', 'aaaa', 'bbbbb']\n",
    "print(tt)\n",
    "ttt = [t for t in tt if len(t) == max([len(t) for t in tt])][0]\n",
    "print(ttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in sites:\n",
    "\n",
    "    pages = sites[site]\n",
    "\n",
    "    for page in pages:\n",
    "        \n",
    "        slug    = page['page_slug']\n",
    "        \n",
    "        #other_pages = [p for fdr in sites for p in sites[fdr] if p['page_slug'] != slug]\n",
    "        page = add_links(sites, page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'folders': ['site\\\\files\\\\python_software',\n",
       "  'site\\\\files\\\\data_science',\n",
       "  'site\\\\files\\\\search_engine_optimization'],\n",
       " 'pages': {'site\\\\files\\\\python_software': [{'page_topic': 'Activestate',\n",
       "    'page_title': 'Activestate | Python Software',\n",
       "    'page_content': 'ActiveState (Software)[https://python-software.github.io/Eric-Software.md] Inc. is a Canadian software company headquartered in Vancouver, British Columbia. It develops, sells, and supports cross-platform development tools for dynamic languages such as Perl, PHP, Python, Ruby, and Tcl, as well as language distributions and enterprise services. ActiveState is owned by its employees and Pender Financial Group, an investment company focused on (technology)[https://data-science-blog.github.io/Committee-On-Data-For-Science-And-Technology.md] and healthcare sectors in British Columbiaafter briefly being a member of the Sophos group. On 2003-09-24, Sophos Plc announced the acquisition of ActiveState Corp.\\n\\nAs part of the acquisition, ActiveState\\'s president Steve Munford would become a member of Sophos\\'s executive management team as Global VP Messaging. On 2006-01-29, Pender Financial Group announced on that it has entered into an agreement with Sophos, Inc. to acquire ActiveState Software Inc. On 2006-02-21, ActiveState Software Inc. announced its acquisition by Pender Financial Group Corporation from Sophos, Inc. , a subsidiary of Sophos Plc.\\n\\n, for the purchase price of US$2,250,000. Following the acquisition, Bart Copeland would become ActiveState Software Inc. \\'s President and CEO, and Dr. David Ascher would become ActiveState Software Inc.\\n\\n\\'s CTO and VP of Engineering. Following the sales of ActiveState to PFG, David Ascher of ActiveState revealed that Sophos agreed to sell ActiveState because developing (programming)[https://python-software.github.io/Core-Python-Programming.md] tools did not fit Sophos\\'s business model. In October 2006, ActiveState was named one of Canada\\'s Top 100 Employers, as published in Maclean\\'s magazine, along with several other software companies. Somewhere around 2013 the licensing model for ActiveState products changed from paid support to paid commercial use.\\n\\n• Phenona: On 2011-06-14, ActiveState Software Inc. announced the acquisition of Phenona. • Appsecute Limited: On 2013-06-04, ActiveState Software Inc. announced the acquisition of Appsecute. The acquisition would become ActiveState\\'s strategy to pair Appsecute with Stackato.\\n\\nActiveState products include: ActiveState Komodo, an integrated development environment (IDE) for dynamic languages; Perl Dev Kit (PDK) and Tcl Dev Kit (TDK), productivity and deployment tools for Perl and Tcl programmers; and free and commercial language distributions, ActivePerl, ActivePython, and ActiveTcl for AIX, HP-UX, Linux, OS X, Solaris, and Windows. ActivePython is a software package consisting of the Python (programming language) implementation CPython and a set of extensions, packaged to facilitate installation. As of 2006, it ran on Windows, Mac OS X, Linux, Solaris, AIX and HP-UX platforms. ActivePython for Windows includes the PyWin32 extensions for programming with the Win32 API.\\n\\nIt also includes the integrated development environment IDLE, although this requires manual setup. In February 2012, ActiveState announced general availability of Stackato. According to the announcement, Stackato \"makes it easy to develop, deploy, migrate, scale, manage, and monitor applications on any cloud\", and is available in Enterprise, Micro Cloud, and Sandbox editions. In December 2012, ActiveState announced the OEM integration of Stackato with HP Cloud Services, specifically the HP Cloud Application Platform as a Service.\\n\\nHP describes the product as \"an application platform for development, deployment, and management of cloud applications using any language on any stack\". In July 28, 2015, Hewlett-Packard Development Company, L. P. announced the acquisition of Stackato\\'s business from ActiveState Software Inc. .',\n",
       "    'local_folder': 'site\\\\files\\\\python_software',\n",
       "    'local_filename': 'site\\\\files\\\\python_software\\\\2019-11-13-Activestate.md',\n",
       "    'site_host': 'https://python-software.github.io',\n",
       "    'site_topic': 'Python Software',\n",
       "    'page_slug': 'Activestate.md',\n",
       "    'page_filename': '2019-11-13-Activestate.md',\n",
       "    'page_url': 'https://python-software.github.io/Activestate.md'},\n",
       "   {'page_topic': 'Anaconda Python Distribution',\n",
       "    'page_title': 'Anaconda Python Distribution | Python Software',\n",
       "    'page_content': 'Anaconda is a free and open-sourcedistribution of the Python and R (programming)[https://python-software.github.io/Core-Python-Programming.md] languages for scientific computing (data science, machine learning applications, large-scale (data)[https://data-science-blog.github.io/Big-Data.md] processing, predictive analytics, etc. ), that aims to simplify package management and deployment. Package versions are managed by the package management system conda. The Anaconda distribution is used by over 15 million users and includes more than 1500 popular data-science packages suitable for Windows, Linux, and MacOS. Anaconda distribution comes with more than 1,500 packages as well as the conda package and virtual environment manager. It also includes a GUI, Anaconda Navigator, as a graphical alternative to the command line interface (CLI).\\n\\nThe big difference between conda and the pip package manager is in how package dependencies are managed, which is a significant challenge for Python (data)[https://data-science-blog.github.io/Data.md] science and the reason conda exists. When pip installs a package, it automatically installs any dependent Python packages without checking if these conflict with previously installed packages. It will install a package and any of its dependencies regardless of the state of the existing installation. Because of this, a user with a working installation of, for example, (Google)[https://search-engine-optimization-blog.github.io/Google-Custom-Search.md] Tensorflow, can find that it stops working having used pip to install a different package that requires a different version of the dependent numpy library than the one used by Tensorflow. In some cases, the package may appear to work but produce different results in detail.\\n\\nIn contrast, conda analyses the current environment including everything currently installed, and, together with any version limitations specified (e. g. the user may wish to have Tensorflow version 2,0 or higher), works out how to install a compatible set of dependencies, warning if this cannot be done. Open source packages can be individually installed from the Anaconda repository, Anaconda Cloud (anaconda. org), or your own private repository or mirror, using the command. Anaconda Inc compiles and builds all the packages in the Anaconda repository itself, and provides binaries for Windows 32/64 bit, Linux 64 bit and MacOS 64-bit.\\n\\nAnything available on PyPI may be installed into a conda environment using pip, and conda will keep track of what it has installed itself and what pip has installed. Custom packages can be made using the command, and can be shared with others by uploading them to Anaconda Cloud,PyPI or other repositories. The default installation of Anaconda2 includes Python 2. 7 and Anaconda3 includes Python 3.\\n\\n7. However, it is possible to create new (environments)[https://python-software.github.io/Comparison-Of-Integrated-Development-Environments.md] that include any version of Python packaged with conda. Anaconda Navigator is a desktop graphical user interface (GUI) included in Anaconda distribution that allows users to launch applications and manage conda packages, environments and channels without using command-line commands. Navigator can (search)[https://search-engine-optimization-blog.github.io/Audio-Search-Engine.md] for packages on Anaconda Cloud or in a local Anaconda Repository, install them in an environment, run the packages and update them.\\n\\nIt is available for Windows, macOS and Linux. The following applications are available by default in Navigator: Conda is an open source,cross-platform,language-agnosticpackage manager and environment management systemthat installs, runs, and updates packages and their dependencies. It was created for Python programs, but it can package and distribute (software)[https://python-software.github.io/Eric-Software.md] for any language (e. g. , R), including multi-language projects.\\n\\nThe conda package and environment manager is included in all versions of Anaconda, Miniconda,and Anaconda Repository. Anaconda Cloud is a package management service by Anaconda where you can find, access, store and share public and private notebooks, environments, and conda and PyPI packages. Cloud hosts useful Python packages, notebooks and environments for a wide variety of applications. You do not need to log in or to have a Cloud account, to search for public packages, download and install them.\\n\\nYou can build new packages using the Anaconda Client command line interface (CLI), then manually or automatically upload the packages to Cloud. .',\n",
       "    'local_folder': 'site\\\\files\\\\python_software',\n",
       "    'local_filename': 'site\\\\files\\\\python_software\\\\2019-11-13-Anaconda-Python-Distribution.md',\n",
       "    'site_host': 'https://python-software.github.io',\n",
       "    'site_topic': 'Python Software',\n",
       "    'page_slug': 'Anaconda-Python-Distribution.md',\n",
       "    'page_filename': '2019-11-13-Anaconda-Python-Distribution.md',\n",
       "    'page_url': 'https://python-software.github.io/Anaconda-Python-Distribution.md'},\n",
       "   {'page_topic': 'Biopython',\n",
       "    'page_title': 'Biopython | Python Software',\n",
       "    'page_content': \"The Biopython Project is an open-source collection of non-commercial Python tools for computational biology and bioinformatics, created by an international association of developers. It contains classes to represent biological sequences and sequence annotations, and it is able to read and write to a variety of file formats. It also allows for a programmatic means of accessing online databases of biological information, such as those at NCBI. Separate modules extend Biopython's capabilities to sequence alignment, protein structure, population genetics, phylogenetics, sequence motifs, and machine learning. Biopython is one of a number of Bio* projects designed to reduce code duplication in computational biology.\\n\\nBiopython development began in 1999 and it was first released in July 2000. It was developed during a similar time frame and with analogous goals to other projects that added bioinformatics capabilities to their respective (programming)[https://python-software.github.io/Core-Python-Programming.md] languages, including BioPerl, BioRuby and BioJava. Early developers on the project included Jeff Chang, Andrew Dalke and Brad Chapman, though over 100 people have made contributions to date. In 2007, a similar Python project, namely PyCogent, was established.\\n\\nThe initial scope of Biopython involved accessing, indexing and processing biological sequence files. While this is still a major focus, over the following years added modules have extended its functionality to cover additional areas of biology (see Key features and examples). As of version 1. 62, Biopython supports running on Python 3 as well as Python 2. Wherever possible, Biopython follows the conventions used by the Python programming language to make it easier for users familiar with Python. For example, and objects can be manipulated via slicing, in a manner similar to Python’s strings and lists.\\n\\nIt is also designed to be functionally similar to other Bio* projects, such as BioPerl. Biopython is able to read and write most common file formats for each of its functional areas, and its license is permissive and compatible with most other (software)[https://python-software.github.io/Eric-Software.md] licenses, which allow Biopython to be used in a variety of software projects. A core concept in Biopython is the biological sequence, and this is represented by the class. A Biopython object is similar to a Python string in many respects: it supports the Python slice notation, can be concatenated with other sequences and is immutable. In addition, it includes sequence-specific methods and specifies the particular biological alphabet used. The class describes sequences, along with information such as name, description and features in the form of objects.\\n\\nEach object specifies the type of the feature and its location. Feature types can be ‘gene’, ‘CDS’ (coding sequence), ‘repeat_region’, ‘mobile_element’ or others, and the position of features in the sequence can be exact or approximate. Biopython can read and write to a number of common sequence formats, including FASTA, FASTQ, GenBank, Clustal, PHYLIP and NEXUS. When reading files, descriptive information in the file is used to populate the members of Biopython classes, such as. This allows records of one file format to be converted into others.\\n\\nVery large sequence files can exceed a computer's memory resources, so Biopython provides various options for accessing records in large files. They can be loaded entirely into memory in Python (data)[https://data-science-blog.github.io/Big-Data.md] structures, such as lists or dictionaries, providing fast access at the cost of memory usage. Alternatively, the files can be read from disk as needed, with slower performance but lower memory requirements. Through the Bio. Entrez module, users of Biopython can download biological (data)[https://data-science-blog.github.io/Data.md] from NCBI databases.\\n\\nEach of the functions provided by the Entrez (search)[https://search-engine-optimization-blog.github.io/Audio-Search-Engine.md] engine is available through functions in this module, including searching for and downloading records. The Bio. Phylo module provides tools for working with and visualising phylogenetic trees. A variety of file formats are supported for reading and writing, including Newick, NEXUS and phyloXML. Common tree manipulations and traversals are supported via the and objects.\\n\\nExamples include converting and collating tree files, extracting subsets from a tree, changing a tree's root, and analysing branch features such as length or score. Rooted trees can be drawn in ASCII or using matplotlib (see Figure 1), and the Graphviz library can be used to create unrooted layouts (see Figure 2). The GenomeDiagram module provides methods of visualising sequences within Biopython. Sequences can be drawn in a linear or circular form (see Figure 3), and many output formats are supported, including PDF and PNG. Diagrams are created by making tracks and then adding sequence features to those tracks.\\n\\nBy looping over a sequence's features and using their attributes to decide if and how they are added to the diagram's tracks, one can exercise much control over the appearance of the final diagram. Cross-links can be drawn between different tracks, allowing one to compare multiple sequences in a single diagram. The Bio. PDB module can load molecular structures from PDB and mmCIF files, and was added to Biopython in 2003. The object is central to this module, and it organises macromolecular structure in a hierarchical fashion: objects contain objects which contain objects which contain objects which contain objects.\\n\\nDisordered residues and atoms get their own classes, and , that describe their uncertain positions. Using Bio. PDB, one can navigate through individual components of a macromolecular structure file, such as examining each atom in a protein. Common analyses can be carried out, such as measuring distances or angles, comparing residues and calculating residue depth.\\n\\nThe Bio. PopGen module adds support to Biopython for Genepop, a software package for statistical analysis of population genetics. This allows for analyses of Hardy–Weinberg equilibrium, linkage disequilibrium and other features of a population's allele frequencies. This module can also carry out population genetic simulations using coalescent theory with the fastsimcoal2 program. Many of Biopython's modules contain command line wrappers for commonly used tools, allowing these tools to be used from within Biopython. These wrappers include BLAST, Clustal, PhyML, EMBOSS and SAMtools.\\n\\nUsers can subclass a generic wrapper class to add support for any other command line tool. .\",\n",
       "    'local_folder': 'site\\\\files\\\\python_software',\n",
       "    'local_filename': 'site\\\\files\\\\python_software\\\\2019-11-13-Biopython.md',\n",
       "    'site_host': 'https://python-software.github.io',\n",
       "    'site_topic': 'Python Software',\n",
       "    'page_slug': 'Biopython.md',\n",
       "    'page_filename': '2019-11-13-Biopython.md',\n",
       "    'page_url': 'https://python-software.github.io/Biopython.md'},\n",
       "   {'page_topic': 'Circuitpython',\n",
       "    'page_title': 'Circuitpython | Python Software',\n",
       "    'page_content': 'CircuitPythonis an open source derivative of the MicroPython (programming)[https://python-software.github.io/Core-Python-Programming.md] language targeted towards the student and beginner. Development of CircuitPython is supported by Adafruit Industries. It is a (software)[https://python-software.github.io/Eric-Software.md] implementation of the Python 3 programming language, written in C. It has been ported to run on several modern microcontrollers. CircuitPython is a full Python compiler and runtime that runs on the microcontroller hardware. The user is presented with an interactive prompt (the REPL) to execute supported commands immediately.\\n\\nIncluded are a selection of core Python libraries. CircuitPython includes modules which give the programmer access to the low-level hardware of Adafruit compatible products as well as higher level libraries for beginners. CircuitPython is a fork of MicroPython, originally created by Damien George. The MicroPython community continues to discussforks of MicroPython into variants such as CircuitPython. CircuitPython is targeted to be compliant with CPython, the reference implementation of the Python programming language.\\n\\nPrograms written for CircuitPython compatible boards may not run unmodified on other platforms such as the Raspberry Pi. CircuitPython is being used where in the past the code may have been done in the Arduino development environment. The language has also seen uptake in making small, handheld video game devices. Developer Chris Young has ported his infrared receive/transmit software to CircuitPython to provide interactivity and to aid those with accessibility issues. The user community support includes a Discord chat room and product support forums. A Twitter account dedicated to CircuitPython news was established in 2018.\\n\\nThe current stable version is 4. 0. 1 with support for the Microchip (Technology)[https://data-science-blog.github.io/Committee-On-Data-For-Science-And-Technology.md] Atmel SAMD21 and SAMD51 microcontrollersand the Nordic Semiconductor nRF52840 microcontroller. Previous versions supported the ESP8266 microcontroller, but support was dropped in version 4. .',\n",
       "    'local_folder': 'site\\\\files\\\\python_software',\n",
       "    'local_filename': 'site\\\\files\\\\python_software\\\\2019-11-13-Circuitpython.md',\n",
       "    'site_host': 'https://python-software.github.io',\n",
       "    'site_topic': 'Python Software',\n",
       "    'page_slug': 'Circuitpython.md',\n",
       "    'page_filename': '2019-11-13-Circuitpython.md',\n",
       "    'page_url': 'https://python-software.github.io/Circuitpython.md'},\n",
       "   {'page_topic': 'Comparison Of Integrated Development Environments',\n",
       "    'page_title': 'Comparison Of Integrated Development Environments | Python Software',\n",
       "    'page_content': 'The following tables list notable (software)[https://python-software.github.io/Eric-Software.md] packages that are nominal IDEs; standalone tools such as source code editors and GUI builders are not included. These IDEs are listed in alphabetical order of the supported language. Go to this page: Source code editors for Erlang Go to this page: Comparison of IDE choices for Haxe programmers Java has strong IDE support, due not only to its historical and economic importance, but also due to a combination of reflection and static-typing making it well-suited for IDE support. Some of the leading Java IDEs (such as IntelliJ and Eclipse) are also the basis for leading IDEs in other (programming)[https://python-software.github.io/Core-Python-Programming.md] languages (e. g.\\n\\nfor Python, IntelliJ is rebranded as PyCharm, and Eclipse has the PyDev plugin. ).',\n",
       "    'local_folder': 'site\\\\files\\\\python_software',\n",
       "    'local_filename': 'site\\\\files\\\\python_software\\\\2019-11-13-Comparison-Of-Integrated-Development-Environments.md',\n",
       "    'site_host': 'https://python-software.github.io',\n",
       "    'site_topic': 'Python Software',\n",
       "    'page_slug': 'Comparison-Of-Integrated-Development-Environments.md',\n",
       "    'page_filename': '2019-11-13-Comparison-Of-Integrated-Development-Environments.md',\n",
       "    'page_url': 'https://python-software.github.io/Comparison-Of-Integrated-Development-Environments.md'},\n",
       "   {'page_topic': 'Core Python Programming',\n",
       "    'page_title': 'Core Python Programming | Python Software',\n",
       "    'page_content': 'Core Python Programming is a textbook on the Python programming language, written by Wesley J. Chun. The first edition of the book was released on December 14, 2000. The second edition was released several years later on September 18, 2006. Core Python Programming is targeted mainly at higher education students and IT professionals.\\n\\nWith each printing, the book is updated and errors are corrected. The official site has updates and errata for those with the older printings as well as changes since the last printing. As of February 2011, this edition was in its fifth printing. Core Python Programming has been translated into French, Chinese (simplified) and Hindi. Core Python Programming is divided into two parts with a total of 23 chapters, as well as an index.\\n\\nThe first part of Core Python Programming, Core Python, deals with the basic aspects of the Python programming language. Chapters One and Two, named What is Python? and Getting Started respectively, give instructions on how to install and configure Python, as well as detailing the basic operators and simple statements. Part One continues to cover Sequences (Lists, Strings and Tuples), Built-in Functions and creating functions, Loops (for-loop, while loop and if-statement being the most common); Modules, a full explanation on what Object Oriented Programming is; and syntax. Part Two, Advanced Topics, contains information the more complex aspects of Python, such as GUI programming. Other topics covered include regular expressions, network programming, multithreaded programming, web programming and database programming.\\n\\nCore Python Programming has been generally well received by reviewers. Jeremy Turner, from www. freesoftwaremagazine. com, rated Core Python Programming 10 out of 10, stating: \"You should buy this book because it is the best reference guide and learning material on the Python programming language.\\n\\nThe book is easy to follow and is an excellent place to start or brush up on your Python skills. \" On the other hand, he also stated that: \"Python pros, or those looking for answers and examples to specific problems may not enjoy this book. The book is also not a complete reference of the Python language, but does do a very good job of explaining it. \"Michael Baxter, in a review of the first edition of Core Python Programming on Linux Journal, was also very positive about the book. .',\n",
       "    'local_folder': 'site\\\\files\\\\python_software',\n",
       "    'local_filename': 'site\\\\files\\\\python_software\\\\2019-11-13-Core-Python-Programming.md',\n",
       "    'site_host': 'https://python-software.github.io',\n",
       "    'site_topic': 'Python Software',\n",
       "    'page_slug': 'Core-Python-Programming.md',\n",
       "    'page_filename': '2019-11-13-Core-Python-Programming.md',\n",
       "    'page_url': 'https://python-software.github.io/Core-Python-Programming.md'},\n",
       "   {'page_topic': 'Eric Software',\n",
       "    'page_title': 'Eric Software | Python Software',\n",
       "    'page_content': 'eric is a free integrated development environment (IDE) used for computer programming. Since it is a full featured IDE, it provides by default all necessary tools needed for the writing of code and for the professional management of a software project. eric is written in the (programming)[https://python-software.github.io/Core-Python-Programming.md] language Python and its primary use is for developing software written in Python. It is usable for development of any combination of Python 3 or Python 2, Qt 5 or Qt 4 and PyQt 5 or PyQt 4 projects, on Linux, macOS and Microsoft Windows platforms.\\n\\neric is licensed under the GNU General Public License version 3 or later and is thereby Free Software. This means in general terms that the source code of eric can be studied, changed and improved by anyone, that eric can be run for any purpose by anyone and that eric - and any changes or improvements that may have been made to it - can be redistributed by anyone to anyone as long as the license is not changed (copyleft). eric can be downloaded at Sourceforge and installed manually with a python installer script. Most major GNU/Linux distributions include eric in their software repositories, so when using such GNU/Linux distributions eric can be obtained and installed automatically by using the package manager of the particular distribution. Additionally, the author offers access to the source code via a public Mercurial repository.\\n\\neric is written in Python and uses the PyQt Python bindings for the Qt GUI toolkit. By design, eric acts as a front end for several programs, for example the QScintilla editor widget. • GUI designing: • Integration of Qt Designer, a Graphical user interface builder for the creation of Qt-based Graphical user interfaces • Debugging, checking, testing and documenting: • Integrated graphical python debugger which supports both interactive probing while suspended and auto breaking on exceptions as well as debugging multi-threaded and multiprocessing applications • Integrated automatic code checkers (syntax, errors and style, PEP-8) for static program analysis as well as support of Pylint via plug-in • Integrated unit testing support by having the option to run python code with command-line parameters • Version control: • Integrated version control support for Mercurial and Subversion repositories (as core plug-ins) and git (as optional plug-in) • Other: • Running external applications from within the IDE • Many integrated wizards for regex and Qt dialogs (as core plug-ins) Prior to the release of eric version 5. 5. 0, eric version 4 and eric version 5 coexisted and were maintained simultaneously, while eric 4 was the variant for writing software in Python version 2 and eric version 5 was the variant for writing software in Python version 3.\\n\\nWith the release of eric version 5. 5. 0 both variants had been merged into one, so that all versions as of eric version 5. 5. 0 support writing software in Python 2 as well as in Python 3, making the separate development lanes of eric version 4 and 5 obsolete.\\n\\nThose two separate development lanes are no longer maintained, and the last versions prior to merging them both to 5. 5. 0 were versions 4. 5.\\n\\n25 and 5. 4. 7. Until 2016, eric used a software versioning scheme with a three-sequence identifier, e.\\n\\ng. 5. 0. 1.\\n\\nThe first sequence represents the major version number which is increased when there are significant jumps in functionality, the second sequence represents the minor number, which is incremented when only some features or significant fixes have been added, and the third sequence is the revision number, which is incremented when minor bugs are fixed or minor features have been added. From late 2016, the version numbers show the year and month of release, e. g. 16.\\n\\n11 for November 2016. eric follows the development philosophy of Release early, release often, following loosely a time-based release schedule. Currently a revision version is released around the first weekend of every month, a minor version is released annually, in most cases approximately between December and February. The following table shows the version (history)[https://python-software.github.io/History-Of-Python.md] of eric, starting from version 4. 0. 0.\\n\\nOnly major (e. g. 6. 0. 0) and minor (e.\\n\\ng. 6. 1. 0) releases are listed; revision releases (e. g. 6.\\n\\n0. 1) are omitted. Several allusions are made to the British comedy group Monty Python, which the Python programming language is named after. Eric alludes to Eric Idle, a member of the group, and IDLE, the standard python IDE shipped with most distributions. .',\n",
       "    'local_folder': 'site\\\\files\\\\python_software',\n",
       "    'local_filename': 'site\\\\files\\\\python_software\\\\2019-11-13-Eric-Software.md',\n",
       "    'site_host': 'https://python-software.github.io',\n",
       "    'site_topic': 'Python Software',\n",
       "    'page_slug': 'Eric-Software.md',\n",
       "    'page_filename': '2019-11-13-Eric-Software.md',\n",
       "    'page_url': 'https://python-software.github.io/Eric-Software.md'},\n",
       "   {'page_topic': 'History Of Python',\n",
       "    'page_title': 'History Of Python | Python Software',\n",
       "    'page_content': 'The (programming)[https://python-software.github.io/Core-Python-Programming.md] language Python was conceived in the late 1980s,and its implementation was started in December 1989by Guido van Rossum at CWI in the Netherlands as a successor to ABC capable of exception handling and interfacing with the Amoeba operating system. Van Rossum is Python\\'s principal author, and his continuing central role in deciding the direction of Python is reflected in the title given to him by the Python community, Benevolent Dictator for Life (BDFL). (However, van Rossum stepped down as leader on July 12, 2018. ) Python was named for the BBC TV show Monty Python\\'s Flying Circus.\\n\\nPython 2. 0 was released on October 16, 2000, with many major new features, including a cycle-detecting garbage collector (in addition to reference counting) for memory management and support for Unicode. However, the most important change was to the development process itself, with a shift to a more transparent and community-backed process. Python 3.\\n\\n0, a major, backwards-incompatible release, was released on December 3, 2008after a long period of testing. Many of its major features have also been backported to the backwards-compatible Python 2. 6 and 2. 7. In February 1991, Van Rossum published the code (labeled version 0.\\n\\n9. 0) to alt. sources. Already present at this stage in development were classes with inheritance, exception handling, functions, and the core datatypes of , , and so on. Also in this initial release was a module system borrowed from Modula-3; Van Rossum describes the module as \"one of Python\\'s major programming units\".\\n\\nPython\\'s exception model also resembles Modula-3\\'s, with the addition of an clause. In 1994 comp. lang. python, the primary discussion forum for Python, was formed, marking a milestone in the growth of Python\\'s userbase.\\n\\nPython reached version 1. 0 in January 1994. The major new features included in this release were the functional programming tools , , and. Van Rossum stated that \"Python acquired lambda, reduce(), filter() and map(), courtesy of a Lisp hacker who missed them and submitted working patches\". The last version released while Van Rossum was at CWI was Python 1.\\n\\n2. In 1995, Van Rossum continued his work on Python at the Corporation for National Research Initiatives (CNRI) in Reston, Virginia from where he released several versions. By version 1. 4, Python had acquired several new features. Notable among these are the Modula-3 inspired keyword arguments (which are also similar to Common Lisp\\'s keyword arguments) and built-in support for complex numbers.\\n\\nAlso included is a basic form of (data)[https://data-science-blog.github.io/Big-Data.md] hiding by name mangling, though this is easily bypassed. During Van Rossum\\'s stay at CNRI, he launched the Computer Programming for Everybody (CP4E) initiative, intending to make programming more accessible to more people, with a basic \"literacy\" in programming languages, similar to the basic English literacy and mathematics skills required by most employers. Python served a central role in this: because of its focus on (clean)[https://search-engine-optimization-blog.github.io/Clean-Url.md] syntax, it was already suitable, and CP4E\\'s goals bore similarities to its predecessor, ABC. The project was funded by DARPA.\\n\\nAs of 2007 , the CP4E project is inactive, and while Python attempts to be easily learnable and not too arcane in its syntax and semantics, reaching out to non-programmers is not an active concern. In 2000, the Python core development team moved to BeOpen. com to form the BeOpen PythonLabs team. CNRI requested that a version 1. 6 be released, summarizing Python\\'s development up to the point at which the development team left CNRI. Consequently, the release schedules for 1.\\n\\n6 and 2. 0 had a significant amount of overlap. Python 2. 0 was the only release from BeOpen. com.\\n\\nAfter Python 2. 0 was released by BeOpen. com, Guido van Rossum and the other PythonLabs developers joined Digital Creations. The Python 1. 6 release included a new CNRI license that was substantially longer than the CWI license that had been used for earlier releases.\\n\\nThe new license included a clause stating that the license was governed by the laws of the State of Virginia. The Free (Software)[https://python-software.github.io/Eric-Software.md] Foundation argued that the choice-of-law clause was incompatible with the GNU General Public License. BeOpen, CNRI and the FSF negotiated a change to Python\\'s free software license that would make it GPL-compatible. Python 1.\\n\\n6. 1 is essentially the same as Python 1. 6, with a few minor bug fixes, and with the new GPL-compatible license. Python 2.\\n\\n0, released October 2000, introduced list comprehensions, a feature borrowed from the functional programming languages SETL and Haskell. Python\\'s syntax for this construct is very similar to Haskell\\'s, apart from Haskell\\'s preference for punctuation characters and Python\\'s preference for alphabetic keywords. Python 2. 0 also introduced a garbage collection system capable of collecting reference cycles. Python 2.\\n\\n1 was close to Python 1. 6. 1, as well as Python 2. 0. Its license was renamed Python Software Foundation License.\\n\\nAll code, documentation and specifications added, from the time of Python 2. 1\\'s alpha release on, is owned by the Python Software Foundation (PSF), a non-profit organization formed in 2001, modeled after the Apache Software Foundation. The release included a change to the language specification to support nested scopes, like other statically scoped languages. (The feature was turned off by default, and not required, until Python 2. 2.\\n\\n) A major innovation in Python 2. 2 was the unification of Python\\'s types (types written in C) and classes (types written in Python) into one hierarchy. This single unification made Python\\'s object model purely and consistently object oriented. Also added were generators which were inspired by Icon.\\n\\nPython 2. 5 was released on September 2006and introduced the statement, which encloses a code block within a context manager (for example, acquiring a lock before the block of code is run and releasing the lock afterwards, or opening a file and then closing it), allowing Resource Acquisition Is Initialization (RAII)-like behavior and replacing a common try/finally idiom. Python 2. 6 was released to coincide with Python 3. 0, and included some features from that release, as well as a \"warnings\" mode that highlighted the use of features that were removed in Python 3. 0.\\n\\nSimilarly, Python 2. 7 coincided with and included features from Python 3. 1,which was released on June 26, 2009. Parallel 2. x and 3.\\n\\nx releases then ceased, and Python 2. 7 was the last release in the 2. x series. In November 2014, it was announced that Python 2.\\n\\n7 would be supported until 2020, but users were encouraged to move to Python 3 as soon as possible. Python 3. 0 (also called \"Python 3000\" or \"Py3K\") was released on December 3, 2008. It was designed to rectify fundamental design flaws in the language—the changes required could not be implemented while retaining full backwards compatibility with the 2.\\n\\nx series, which necessitated a new major version number. The guiding principle of Python 3 was: \"reduce feature duplication by removing old ways of doing things\". Python 3. 0 was developed with the same philosophy as in prior versions. However, as Python had accumulated new and redundant ways to program the same task, Python 3.\\n\\n0 had an emphasis on removing duplicative constructs and modules, in keeping with \"There should be one— and preferably only one —obvious way to do it\". Nonetheless, Python 3. 0 remained a multi-paradigm language. Coders could still follow object-oriented, structured, and functional programming paradigms, among others, but within such broad choices, the details were intended to be more obvious in Python 3.\\n\\n0 than they were in Python 2. x. Python 3. 0 broke backward compatibility, and much Python 2 code does not run unmodified on Python 3. Python\\'s dynamic typing combined with the plans to change the semantics of certain methods of dictionaries, for example, made perfect mechanical translation from Python 2. x to Python 3.\\n\\n0 very difficult. A tool called \"2to3\" does the parts of translation that can be done automatically. At this, 2to3 appeared to be fairly successful, though an early review noted that there were aspects of translation that such a tool would never be able to handle. Prior to the roll-out of Python 3, projects requiring compatibility with both the 2. x and 3.\\n\\nx series were recommended to have one source (for the 2. x series), and produce releases for the Python 3. x platform using 2to3. Edits to the Python 3. x code were discouraged for so long as the code needed to run on Python 2.\\n\\nx. This is no longer recommended; as of 2012 the preferred approach is to create a single code base that can run under both Python 2 and 3 using compatibility modules. Some of the major changes included for Python 3. 0 were: • Changing so that it is a built-in function, not a statement.\\n\\nThis made it easier to change a module to use a different print function, as well as making the syntax more regular. In Python 2. 6 and 2. 7 is available as a builtin but is masked by the print statement syntax, which can be disabled by entering at the top of the file. • Removal of the Python 2 function, and the renaming of the function to.\\n\\nPython 3\\'s function behaves like Python 2\\'s function, in that the input is always returned as a string rather than being evaluated as an expression. • Moving (but not or ) out of the built-in namespace and into (the rationale being that operations using reduce are expressed more clearly using an accumulation loop);• Adding support for optional function annotations that can be used for informal type declarations or other purposes;• Unifying the / types, representing text, and introducing a separate immutable type; and a mostly corresponding mutable type, both of which represent arrays of bytes;• A change in integer division functionality: in Python 2, is ; in Python 3, is. (In both Python 2 (2. 2 onwards) and Python 3, is ).\\n\\nSubsequent releases in the Python 3. x series have included additional, substantial new features; all ongoing development of the language is done in the 3. x series. Release dates for the major and minor versions:.',\n",
       "    'local_folder': 'site\\\\files\\\\python_software',\n",
       "    'local_filename': 'site\\\\files\\\\python_software\\\\2019-11-13-History-Of-Python.md',\n",
       "    'site_host': 'https://python-software.github.io',\n",
       "    'site_topic': 'Python Software',\n",
       "    'page_slug': 'History-Of-Python.md',\n",
       "    'page_filename': '2019-11-13-History-Of-Python.md',\n",
       "    'page_url': 'https://python-software.github.io/History-Of-Python.md'},\n",
       "   {'page_topic': 'Idle',\n",
       "    'page_title': 'Idle | Python Software',\n",
       "    'page_content': 'IDLE (short for Integrated DeveLopment Environmentor Integrated Development and Learning Environment) is an integrated development environment for Python, which has been bundled with the default implementation of the language since 1. 5. 2b1. It is packaged as an optional part of the Python packaging with many Linux distributions.\\n\\nIt is completely written in Python and the Tkinter GUI toolkit (wrapper functions for Tcl/Tk). IDLE is intended to be a simple IDE and suitable for beginners, especially in an educational environment. To that end, it is cross-platform, and avoids feature clutter. According to the included README, its main features are: • Multi-window text editor with syntax highlighting, autocompletion, smart indent and other.\\n\\nIDLE has been criticized for various usability issues, including losing focus, lack of copying to clipboard feature, lack of line numbering options, and general user interface design; it has been called a \"disposable\" IDE, because users frequently move on to a more advanced IDE as they gain experience. Author Guido van Rossum says IDLE stands for \"Integrated DeveLopment Environment\",and since Van Rossum named the language Python partly to honor British comedy group Monty Python, the name IDLE was probably also chosen partly to honor Eric Idle, one of Monty Python\\'s founding members. .',\n",
       "    'local_folder': 'site\\\\files\\\\python_software',\n",
       "    'local_filename': 'site\\\\files\\\\python_software\\\\2019-11-13-Idle.md',\n",
       "    'site_host': 'https://python-software.github.io',\n",
       "    'site_topic': 'Python Software',\n",
       "    'page_slug': 'Idle.md',\n",
       "    'page_filename': '2019-11-13-Idle.md',\n",
       "    'page_url': 'https://python-software.github.io/Idle.md'},\n",
       "   {'page_topic': 'Ironpython',\n",
       "    'page_title': 'Ironpython | Python Software',\n",
       "    'page_content': \"IronPython is an implementation of the Python (programming)[https://python-software.github.io/Core-Python-Programming.md] language targeting the. NET Framework and Mono. Jim Hugunin created the project and actively contributed to it up until Version 1. 0 which was released on September 5, 2006. IronPython 2.\\n\\n0 was released on December 10, 2008. After version 1. 0 it was maintained by a small team at Microsoft until the 2. 7 Beta 1 release.\\n\\nMicrosoft abandoned IronPython (and its sister project IronRuby) in late 2010, after which Hugunin left to work at Google. The project is currently maintained by a group of volunteers at GitHub. It is free and open-source software, and can be implemented with Python Tools for Visual Studio, which is a free and open-source extension for Microsoft's Visual Studio IDE. IronPython is written entirely in C#, although some of its code is automatically generated by a code generator written in Python. IronPython is implemented on top of the Dynamic Language Runtime (DLR), a library running on top of the Common Language Infrastructure that provides dynamic typing and dynamic method dispatch, among other things, for dynamic languages.\\n\\nThe DLR is part of the. NET Framework 4. 0 and is also a part of Mono since version 2. 4 from 2009.\\n\\nThe DLR can also be used as a library on older CLI implementations. • Release 2. 0, released on December 10, 2008, and updated as 2. 0.\\n\\n3 on October 23, 2009, targets CPython 2. 5. IronPython 2. 0. 3 is only compatible up to.\\n\\nNET Framework 3. 5. • Release 2. 6, released on December 11, 2009, and updated on April 12, 2010, targets CPython 2. 6. IronPython 2.\\n\\n6. 1 versions is binary compatible only with. NET Framework 4. 0. IronPython 2.\\n\\n6. 1 must be compiled from sources to run on. NET Framework 3. 5.\\n\\nIron Python 2. 6. 2, released on October 21, 2010, is binary compatible with both. NET Framework 4. 0 and. NET Framework 3.\\n\\n5. • Release 2. 7 was released on March 12, 2011 and it targets CPython 2. 7. • Release 2.\\n\\n7. 1 was released on October 21, 2011 and it targets CPython 2. 7. • Release 2.\\n\\n7. 2. 1 was released on March 13, 2012. It enables support for ZIP file format libraries, SQLite, and compiled executables.\\n\\n• Release 2. 7. 5 was released on December 6, 2014 and mostly consists of bug fixes. • Release 2.\\n\\n7. 6 was released on August 21, 2016 and only consists of bug fixes. • Release 2. 7. 7 was released on December 7, 2016 and only consists of bug fixes.\\n\\n• Release 2. 7. 8 was released on February 16, 2018 and consists of bug fixes, reorganized code, and an updated test infrastructure (including significant testing on Linux under Mono). It is also the first release to support. NET Core. • Release 2.\\n\\n7. 9 was released on October 9, 2018 and consists of bug fixes, reorganized code. It is intended to be the last release before IPY3. There are some differences between the Python reference implementation CPython and IronPython. Some projects built on top of IronPython are known not to work under CPython.\\n\\nConversely, CPython applications that depend on extensions to the language that are implemented in C are not compatible with IronPython, unless they are implemented in a. NET interop. For example, NumPy was wrapped by Microsoft in 2011, allowing code and libraries dependent on it to be run directly from. NET Framework. IronPython is supported on Silverlight.\\n\\nIt can be used as a scripting engine in the browser just like the JavaScript engine. IronPython scripts are passed like simple client-side JavaScript scripts in -tags. It is then also possible to modify embedded XAML markup. The (technology)[https://data-science-blog.github.io/Committee-On-Data-For-Science-And-Technology.md] behind this is called Gestalt. The same works for IronRuby. Until version 0.\\n\\n6, IronPython was released under the terms of Common Public License. Following recruitment of the project lead in August 2004, IronPython was made available as part of Microsoft's Shared Source initiative. This license is not OSI-approved but the authors claim it meets the open-source definition. With the 2. 0 alpha release, the license was changed to the Microsoft Public License,which the OSI has approved. The latest versions are released under the terms of the Apache License 2.\\n\\n0. One of IronPython's key advantages is in its function as an extensibility layer to application frameworks written in a. NET language. It is relatively simple to integrate an IronPython interpreter into an existing. NET application framework.\\n\\nOnce in place, downstream developers can use scripts written in IronPython that interact with. NET objects in the framework, thereby extending the functionality in the framework's interface, without having to change any of the framework's code base. IronPython makes extensive use of reflection. When passed in a reference to a.\\n\\nNET object, it will automatically import the types and methods available to that object. This results in a highly intuitive experience when working with. NET objects from within an IronPython script. The following IronPython script manipulates. NET Framework objects.\\n\\nThis script can be supplied by a third-party client-side application developer and passed into the server-side framework through an interface. Note that neither the interface, nor the server-side code is modified to support the analytics required by the client application. In this case, assume that the. NET Framework implements a class, BookDictionary, in a module called BookService, and publishes an interface into which IronPython scripts can be sent and executed.\\n\\nThis script, when sent to that interface, will iterate over the entire list of books maintained by the framework, and pick out those written by Booker Prize-winning authors. What's interesting is that the responsibility for writing the actual analytics reside with the client-side developer. The demands on the server-side developer are minimal, essentially just providing access to the (data)[https://data-science-blog.github.io/Big-Data.md] maintained by the server. This design pattern greatly simplifies the deployment and maintenance of complex application frameworks.\\n\\nThe following script uses the. NET Framework to create a simple Hello World message. The performance characteristics of IronPython compared to CPython, the reference implementation of Python, depends on the exact benchmark used. IronPython performs worse than CPython on most benchmarks taken with the PyStone script but better on other benchmarks. IronPython may perform better in Python programs that use threads or multiple cores, as it has a JIT, and also because it doesn't have the Global Interpreter Lock. • Boo – a language for the.\\n\\nNET Framework and Mono with Python-inspired syntax and features borrowed from C# and Ruby • Jython – an implementation of Python for the Java Virtual Machine • Unladen Swallow – A (now-defunct) branch of CPython that aimed to provide superior performance using an LLVM-based just-in-time compiler.\",\n",
       "    'local_folder': 'site\\\\files\\\\python_software',\n",
       "    'local_filename': 'site\\\\files\\\\python_software\\\\2019-11-13-Ironpython.md',\n",
       "    'site_host': 'https://python-software.github.io',\n",
       "    'site_topic': 'Python Software',\n",
       "    'page_slug': 'Ironpython.md',\n",
       "    'page_filename': '2019-11-13-Ironpython.md',\n",
       "    'page_url': 'https://python-software.github.io/Ironpython.md'}],\n",
       "  'site\\\\files\\\\data_science': [{'page_topic': 'Berkeley Institute For Data Science',\n",
       "    'page_title': 'Berkeley Institute For Data Science | Data Science',\n",
       "    'page_content': 'The Berkeley Institute for (Data)[https://data-science-blog.github.io/Big-Data.md] Science (BIDS) is a central hub of research and education within UC Berkeley designed to facilitate data-intensive science and earn grants to be disseminated within the sciences. BIDS was initially funded by grants from the Gordon and Betty Moore Foundation and the Sloan Foundation as part of a three-year grant with (data)[https://data-science-blog.github.io/Data.md] science institutes at New York University and the University of Washington. The objective of the three-university initiative is to bring together domain experts from the natural and social sciences, along with methodological experts from computer science, statistics, and applied mathematics. The organization has an executive director and a faculty director, Saul Perlmutter, who won the 2011 Nobel Prize in Physics.\\n\\nThe initiative was announced at a White House Office of Science and (Technology)[https://data-science-blog.github.io/Committee-On-Data-For-Science-And-Technology.md] Policy event to highlight and promote advances in data-driven scientific discovery, and is a core component of the National Science Foundation\\'s strategic plan for building national capacity in data science. There are six working groups that are common across the three universities included in the original Moore/Sloan grant. The working groups are intended to \"address the major challenges facing advances in data-intensive research\" and include Career Paths and Alternative Metrics, Reproducibility and Open Science, Education and Training, Ethnography and Evaluation, (Software)[https://python-software.github.io/Eric-Software.md] Tools and Environments, and Working Spaces and Culture. and while all three are separate aspects of one division, they were awarded different grant money.\\n\\nA primary objective of BIDS is to build a community of data science fellows and senior fellows across academic disciplines. The 23 current fellows constitute the majority of the onsite liveware at the Institute, which supports a number of notable initiatives (via Fellow support). The following list is a subset of notable fellows to date: • Dan Hammer, fellow, Presidential Innovation Fellow and former Chief Data Scientist at the World Resources Institute.',\n",
       "    'local_folder': 'site\\\\files\\\\data_science',\n",
       "    'local_filename': 'site\\\\files\\\\data_science\\\\2019-11-13-Berkeley-Institute-For-Data-Science.md',\n",
       "    'site_host': 'https://data-science-blog.github.io',\n",
       "    'site_topic': 'Data Science',\n",
       "    'page_slug': 'Berkeley-Institute-For-Data-Science.md',\n",
       "    'page_filename': '2019-11-13-Berkeley-Institute-For-Data-Science.md',\n",
       "    'page_url': 'https://data-science-blog.github.io/Berkeley-Institute-For-Data-Science.md'},\n",
       "   {'page_topic': 'Big Data',\n",
       "    'page_title': 'Big Data | Data Science',\n",
       "    'page_content': 'Information assets characterized by such a high volume, velocity, and variety to require specific (technology)[https://data-science-blog.github.io/Committee-On-Data-For-Science-And-Technology.md] and analytical methods for its transformation into value Big (data)[https://data-science-blog.github.io/Data.md] is a field that treats ways to analyze, systematically extract information from, or otherwise deal with data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many cases (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Big data challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy and data source. Big data was originally associated with three key concepts: volume, variety, and velocity.\\n\\nWhen we handle big data, we may not sample but simply observe and track what happens. Therefore, big data often includes data with sizes that exceed the capacity of traditional (software)[https://python-software.github.io/Eric-Software.md] to process within an acceptable timeand value. Current usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from data, and seldom to a particular size of data set. \"There is little doubt that the quantities of data now available are indeed large, but that\\'s not the most relevant characteristic of this new data ecosystem.\\n\\n\"Analysis of data sets can find new correlations to \"spot business trends, prevent diseases, combat crime and so on. \"Scientists, business executives, practitioners of medicine, (advertising)[https://search-engine-optimization-blog.github.io/Contextual-Advertising.md] and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics,connectomics, complex physics simulations, biology and environmental research. Data sets grow rapidly, in part because they are increasingly gathered by cheap and numerous information-sensing Internet of things devices such as mobile devices, aerial (remote sensing), software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks.\\n\\nThe world\\'s technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s;as of 2012 , every day 2. 5 exabytes (2. 5×1018) of data are generated. Based on an IDC report prediction, the global data volume will grow exponentially from 4. 4 zettabytes to 44 zettabytes between 2013 and 2020.\\n\\nBy 2025, IDC predicts there will be 163 zettabytes of data. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization. Relational database management systems, desktop statisticsand software packages used to visualize data often have difficulty handling big data. The work may require \"massively parallel software running on tens, hundreds, or even thousands of servers\". What qualifies as being \"big data\" varies depending on the capabilities of the users and their tools, and expanding capabilities make big data a moving target. \"For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options.\\n\\nFor others, it may take tens or hundreds of terabytes before data size becomes a significant consideration. \"The term has been in use since the 1990s, with some giving credit to John Mashey for popularizing the term. Big data usually includes data sets with sizes beyond the ability of commonly used software tools to capture, curate, manage, and process data within a tolerable elapsed time. Big data philosophy encompasses unstructured, semi-structured and structured data, however the main focus is on unstructured data. Big data \"size\" is a constantly moving target, as of 2012 ranging from a few dozen terabytes to many zettabytes of data.\\n\\nBig data requires a set of techniques and technologies with new forms of integration to reveal insights from datasets that are diverse, complex, and of a massive scale. \"Variety\", \"veracity\" and various other \"Vs\" are added by some organizations to describe it, a revision challenged by some industry authorities. A 2018 definition states \"Big data is where parallel computing tools are needed to handle data\", and notes, \"This represents a distinct and clearly defined change in the computer science used, via parallel (programming)[https://python-software.github.io/Core-Python-Programming.md] theories, and losses of some of the guarantees and capabilities made by Codd\\'s relational model. \"The growing maturity of the concept more starkly delineates the difference between \"big data\" and \"Business Intelligence\":• Business Intelligence uses applied mathematics tools and descriptive statistics with data with high information density to measure things, detect trends, etc. • Big data uses mathematical analysis, optimization, inductive statistics and concepts from nonlinear system identificationto infer laws (regressions, nonlinear relationships, and causal effects) from large sets of data with low information densityto reveal relationships and dependencies, or to perform predictions of outcomes and behaviors.\\n\\nBig data can be described by the following characteristics: Data must be processed with advanced tools (analytics and algorithms) to reveal meaningful information. For example, to manage a factory one must consider both visible and invisible issues with various components. Information generation algorithms must detect and address invisible issues such as machine degradation, component wear, etc. on the factory floor. Big data repositories have existed in many forms, often built by corporations with a special need. Commercial vendors historically offered parallel database management systems for big data beginning in the 1990s.\\n\\nFor many years, WinterCorp published the largest database report. Teradata Corporation in 1984 marketed the parallel processing DBC 1012 system. Teradata systems were the first to store and analyze 1 terabyte of data in 1992. Hard disk drives were 2.\\n\\n5 GB in 1991 so the definition of big data continuously evolves according to Kryder\\'s Law. Teradata installed the first petabyte class RDBMS based system in 2007. As of 2017 , there are a few dozen petabyte class Teradata relational databases installed, the largest of which exceeds 50 PB. Systems up until 2008 were 100% structured relational data.\\n\\nSince then, Teradata has added unstructured data types including XML, JSON, and Avro. In 2000, Seisint Inc. (now LexisNexis Risk Solutions) developed a C++-based distributed platform for data processing and querying known as the HPCC Systems platform. This system automatically partitions, distributes, stores and delivers structured, semi-structured, and unstructured data across multiple commodity servers. Users can write data processing pipelines and queries in a declarative dataflow programming language called ECL. Data analysts working in ECL are not required to define data schemas upfront and can rather focus on the particular problem at hand, reshaping data in the best possible manner as they develop the solution.\\n\\nIn 2004, LexisNexis acquired Seisint Inc. and their high-speed parallel processing platform and successfully utilized this platform to integrate the data systems of Choicepoint Inc. when they acquired that company in 2008. In 2011, the HPCC systems platform was open-sourced under the Apache v2.\\n\\n0 License. CERN and other physics experiments have collected big data sets for many decades, usually analyzed via high-performance computing (supercomputers) rather than the commodity map-reduce architectures usually meant by the current \"big data\" movement. In 2004, (Google)[https://search-engine-optimization-blog.github.io/Google-Custom-Search.md] published a paper on a process called MapReduce that uses a similar architecture. The MapReduce concept provides a parallel processing model, and an associated implementation was released to process huge amounts of data.\\n\\nWith MapReduce, queries are split and distributed across parallel nodes and processed in parallel (the Map step). The results are then gathered and delivered (the Reduce step). The framework was very successful,so others wanted to replicate the algorithm. Therefore, an implementation of the MapReduce framework was adopted by an Apache open-source project named Hadoop. Apache Spark was developed in 2012 in response to limitations in the MapReduce paradigm, as it adds the ability to set up many operations (not just map followed by reducing). MIKE2.\\n\\n0 is an open approach to information management that acknowledges the need for revisions due to big data implications identified in an article titled \"Big Data Solution Offering\". The methodology addresses handling big data in terms of useful permutations of data sources, complexity in interrelationships, and difficulty in deleting (or modifying) individual records. 2012 studies showed that a multiple-layer architecture is one option to address the issues that big data presents. A distributed parallel architecture distributes data across multiple servers; these parallel execution (environments)[https://python-software.github.io/Comparison-Of-Integrated-Development-Environments.md] can dramatically improve data processing speeds.\\n\\nThis type of architecture inserts data into a parallel DBMS, which implements the use of MapReduce and Hadoop frameworks. This type of framework looks to make the processing power transparent to the end-user by using a front-end application server. The data lake allows an organization to shift its focus from centralized control to a shared model to respond to the changing dynamics of information management. This enables quick segregation of data into the data lake, thereby reducing the overhead time. Big data analytics for manufacturing applications is marketed as a \"5C architecture\" (connection, conversion, cyber, cognition, and configuration).\\n\\nFactory work and Cyber-physical systems may have an extended \"6C system\": A 2011 McKinsey Global (Institute)[https://data-science-blog.github.io/Berkeley-Institute-For-Data-Science.md] report characterizes the main components and ecosystem of big data as follows:• Techniques for analyzing data, such as A/B testing, machine learning and natural language processing • Visualization, such as charts, graphs and other displays of the data Multidimensional big data can also be represented as data cubes or, mathematically, tensors. Array Database Systems have set out to provide storage and high-level query support on this data type. Additional technologies being applied to big data include efficient tensor-based computation,such as multilinear subspace learning. ,massively parallel-processing (MPP) databases, search-based applications, data mining,distributed file systems, distributed cache (e. g.\\n\\n, burst buffer and Memcached), distributed databases, cloud and HPC-based infrastructure (applications, storage and computing resources)and the Internet. Although, many approaches and technologies have been developed, it still remains difficult to carry out machine learning with big data. Some MPP relational databases have the ability to store and manage petabytes of data. Implicit is the ability to load, monitor, back up, and optimize the use of the large data tables in the RDBMS. DARPA\\'s Topological Data Analysis program seeks the fundamental structure of massive data sets and in 2008 the technology went public with the launch of a company called Ayasdi. The practitioners of big data analytics processes are generally hostile to slower shared storage,preferring direct-attached storage (DAS) in its various forms from solid state drive (SSD) to high capacity SATA disk buried inside parallel processing nodes.\\n\\nThe perception of shared storage architectures—Storage area network (SAN) and Network-attached storage (NAS) —is that they are relatively slow, complex, and expensive. These qualities are not consistent with big data analytics systems that thrive on system performance, commodity infrastructure, and low cost. Real or near-real time information delivery is one of the defining characteristics of big data analytics. Latency is therefore avoided whenever and wherever possible. Data in direct-attached memory or disk is good—data on memory or disk at the other end of a FC SAN connection is not.\\n\\nThe cost of a SAN at the scale needed for analytics applications is very much higher than other storage techniques. There are advantages as well as disadvantages to shared storage in big data analytics, but big data analytics practitioners as of 2011 did not favour it. Big data has increased the demand of information management specialists so much so that Software AG, Oracle Corporation, IBM, Microsoft, SAP, EMC, HP and Dell have spent more than $15 billion on software firms specializing in data management and analytics. In 2010, this industry was worth more than $100 billion and was growing at almost 10 percent a year: about twice as fast as the software business as a whole. Developed economies increasingly use data-intensive technologies. There are 4.\\n\\n6 billion mobile-phone subscriptions worldwide, and between 1 billion and 2 billion people accessing the internet. Between 1990 and 2005, more than 1 billion people worldwide entered the middle class, which means more people became more literate, which in turn led to information growth. The world\\'s effective capacity to exchange information through telecommunication networks was 281 petabytes in 1986, 471 petabytes in 1993, 2. 2 exabytes in 2000, 65 exabytes in 2007and predictions put the amount of internet traffic at 667 exabytes annually by 2014. According to one estimate, one-third of the globally stored information is in the form of alphanumeric text and still image data,which is the format most useful for most big data applications.\\n\\nThis also shows the potential of yet unused data (i. e. in the form of video and audio content). While many vendors offer off-the-shelf solutions for big data, experts recommend the development of in-house solutions custom-tailored to solve the company\\'s problem at hand if the company has sufficient technical capabilities.\\n\\nThe use and adoption of big data within governmental processes allows efficiencies in terms of cost, productivity, and innovation,but does not come without its flaws. Data analysis often requires multiple parts of government (central and local) to work in collaboration and create new and innovative processes to deliver the desired outcome. CRVS (Civil Registration and Vital Statistics) collects all certificates status from birth to death. CRVS is a source of big data for governments. Research on the effective usage of information and communication technologies for development (also known as ICT4D) suggests that big data technology can make important contributions but also present unique challenges to International development. Advancements in big data analysis offer cost-effective opportunities to improve decision-making in critical development areas such as health care, employment, economic productivity, crime, security, and natural disaster and resource management.\\n\\nAdditionally, user-generated data offers new opportunities to give the unheard a voice. However, longstanding challenges for developing regions such as inadequate technological infrastructure and economic and human resource scarcity exacerbate existing concerns with big data such as privacy, imperfect methodology, and interoperability issues. Based on TCS 2013 Global Trend Study, improvements in supply planning and product quality provide the greatest benefit of big data for manufacturing. Big data provides an infrastructure for transparency in the manufacturing industry, which is the ability to unravel uncertainties such as inconsistent component performance and availability. Predictive manufacturing as an applicable approach toward near-zero downtime and transparency requires a vast amount of data and advanced prediction tools for a systematic process of data into useful information.\\n\\nA conceptual framework of predictive manufacturing begins with data acquisition where different type of sensory data is available to acquire such as acoustics, vibration, pressure, current, voltage and controller data. A vast amount of sensory data in addition to historical data construct the big data in manufacturing. The generated big data acts as the input into predictive tools and preventive strategies such as Prognostics and Health Management (PHM). Big data analytics has helped healthcare improve by providing personalized medicine and prescriptive analytics, clinical risk intervention and predictive analytics, waste and care variability reduction, automated external and internal reporting of patient data, standardized medical terms and patient registries and fragmented point solutions. Some areas of improvement are more aspirational than actually implemented.\\n\\nThe level of data generated within healthcare systems is not trivial. With the added adoption of mHealth, eHealth and wearable technologies the volume of data will continue to increase. This includes electronic health record data, imaging data, patient generated data, sensor data, and other forms of difficult to process data. There is now an even greater need for such environments to pay greater attention to data and information quality. \"Big data very often means \\'dirty data\\' and the fraction of data inaccuracies increases with data volume growth.\\n\\n\" Human inspection at the big data scale is impossible and there is a desperate need in health service for intelligent tools for accuracy and believability control and handling of information missed. While extensive information in healthcare is now electronic, it fits under the big data umbrella as most is unstructured and difficult to use. The use of big data in healthcare has raised significant ethical challenges ranging from risks for individual rights, privacy and autonomy, to transparency and trust. A related application sub-area, that heavily relies on big data, within the healthcare field is that of computer-aided diagnosis in medicine.\\n\\nOne only needs to recall that, for instance, for epilepsy monitoring it is customary to create 5 to 10 GB of data daily. Similarly, a single uncompressed image of breast tomosynthesis averages 450 MB of data. These are just few of the many examples where computer-aided diagnosis uses big data. For this reason, big data has been recognized as one of the seven key challenges that computer-aided diagnosis systems need to overcome in order to reach the next level of performance.\\n\\nA McKinsey Global Institute study found a shortage of 1. 5 million highly trained data professionals and managersand a number of universitiesincluding University of Tennessee and UC Berkeley, have created masters programs to meet this demand. Private bootcamps have also developed programs to meet that demand, including free programs like The Data Incubator or paid programs like General Assembly. In the specific field of marketing, one of the problems stressed by Wedel and Kannanis that marketing has several subdomains (e. g. , advertising, promotions, product development, branding) that all use different types of data.\\n\\nBecause one-size-fits-all analytical solutions are not desirable, business schools should prepare marketing managers to have wide knowledge on all the different techniques used in these subdomains to get a big picture and work effectively with analysts. To understand how the media utilizes big data, it is first necessary to provide some context into the mechanism used for media process. It has been suggested by Nick Couldry and Joseph Turow that practitioners in Media and Advertising approach big data as many actionable points of information about millions of individuals. The industry appears to be moving away from the traditional approach of using specific media environments such as newspapers, magazines, or television shows and instead taps into consumers with technologies that reach targeted people at optimal times in optimal locations. The ultimate aim is to serve or convey, a message or content that is (statistically speaking) in line with the consumer\\'s mindset.\\n\\nFor example, publishing environments are increasingly tailoring messages (advertisements) and content (articles) to appeal to consumers that have been exclusively gleaned through various data-mining activities. • Targeting of consumers (for advertising by marketers)• Data journalism: publishers and journalists use big data tools to provide unique and innovative insights and infographics. Channel 4, the British public-service television broadcaster, is a leader in the field of big data and data analysis. Health insurance providers are collecting data on social \"determinants of health\" such as food and TV consumption, marital status, clothing size and purchasing habits, from which they make predictions on health costs, in order to spot health issues in their clients. It is controversial whether these predictions are currently being used for pricing.\\n\\nBig data and the IoT work in conjunction. Data extracted from IoT devices provides a mapping of device interconnectivity. Such mappings have been used by the media industry, companies and governments to more accurately target their audience and increase media efficiency. IoT is also increasingly adopted as a means of gathering sensory data, and this sensory data has been used in medical,manufacturingand transportationcontexts.\\n\\nKevin Ashton, digital innovation expert who is credited with coining the term,defines the Internet of Things in this quote: “If we had computers that knew everything there was to know about things—using data they gathered without any help from us—we would be able to track and count everything, and greatly reduce waste, loss, and cost. We would know when things needed replacing, repairing or recalling, and whether they were fresh or past their best. ” Especially since 2015, big data has come to prominence within business operations as a tool to help employees work more efficiently and streamline the collection and (distribution)[https://python-software.github.io/Anaconda-Python-Distribution.md] of information technology (IT). The use of big data to resolve IT and data collection issues within an enterprise is called IT operations analytics (ITOA). By applying big data principles into the concepts of machine intelligence and deep computing, IT departments can predict potential issues and move to provide solutions before the problems even happen. In this time, ITOA businesses were also beginning to play a major role in systems management by offering platforms that brought individual data silos together and generated insights from the whole of the system rather than from isolated pockets of data.\\n\\n• The Integrated Joint Operations Platform (IJOP, 一体化联合作战平台) is used by the government to monitor the population, particularly Uyghurs. Biometrics, including DNA samples, are gathered through a program of free physicals. • By 2020, China plans to give all its citizens a personal \"Social Credit\" score based on how they behave. The Social Credit System, now being piloted in a number of Chinese cities, is considered a form of mass surveillance which uses big data analysis technology. • Big data analysis was tried out for the BJP to win the Indian General Election 2014. • The Indian government utilizes numerous techniques to ascertain how the Indian electorate is responding to government action, as well as ideas for policy augmentation.\\n\\n• A big data application was designed by Agro Web Lab to aid irrigation regulation. • Personalized diabetic treatments can be created through GlucoMe\\'s big data solution. Examples of uses of big data in public services: • Data on prescription drugs: by connecting origin, location and the time of each prescription, a research unit was able to exemplify the considerable delay between the release of any given drug, and a UK-wide adaptation of the National Institute for Health and Care Excellence guidelines. This suggests that new or most up-to-date drugs take some time to filter through to the general patient. • Joining up data: a local authority blended data about services, such as road gritting rotas, with services for people at risk, such as \\'meals on wheels\\'. The connection of data allowed the local authority to avoid any weather-related delay.\\n\\n• In 2012, the Obama administration announced the Big Data Research and Development Initiative, to explore how big data could be used to address important problems faced by the government. The initiative is composed of 84 different big data programs spread across six departments. • The United States Federal Government owns five of the ten most powerful supercomputers in the world. • The Utah Data Center has been constructed by the United States National Security Agency.\\n\\nWhen finished, the facility will be able to handle a large amount of information collected by the NSA over the Internet. The exact amount of storage space is unknown, but more recent sources claim it will be on the order of a few exabytes. This has posed security concerns regarding the anonymity of the data collected. • Walmart handles more than 1 million (customer)[https://data-science-blog.github.io/Customer-Data-Platform.md] transactions every hour, which are imported into databases estimated to contain more than 2. 5 petabytes (2560 terabytes) of data—the equivalent of 167 times the information contained in all the books in the US Library of Congress.\\n\\n• Windermere Real Estate uses location information from nearly 100 million drivers to help new home buyers determine their typical drive times to and from work throughout various times of the day. • The Large Hadron Collider experiments represent about 150 million sensors delivering data 40 million times per second. There are nearly 600 million collisions per second. After filtering and refraining from recording more than 99. 99995%of these streams, there are 1,000 collisions of interest per second.\\n\\n• As a result, only working with less than 0. 001% of the sensor stream data, the data flow from all four LHC experiments represents 25 petabytes annual rate before replication (as of 2012 ). This becomes nearly 200 petabytes after replication. • If all sensor data were recorded in LHC, the data flow would be extremely hard to work with. The data flow would exceed 150 million petabytes annual rate, or nearly 500 exabytes per day, before replication.\\n\\nTo put the number in perspective, this is equivalent to 500 quintillion (5×1020) bytes per day, almost 200 times more than all the other sources combined in the world. • The Square Kilometre Array is a radio telescope built of thousands of antennas. It is expected to be operational by 2024. Collectively, these antennas are expected to gather 14 exabytes and store one petabyte per day. It is considered one of the most ambitious scientific projects ever undertaken.\\n\\n• When the Sloan Digital Sky Survey (SDSS) began to collect astronomical data in 2000, it amassed more in its first few weeks than all data collected in the (history)[https://python-software.github.io/History-Of-Python.md] of astronomy previously. Continuing at a rate of about 200 GB per night, SDSS has amassed more than 140 terabytes of information. When the Large Synoptic Survey Telescope, successor to SDSS, comes online in 2020, its designers expect it to acquire that amount of data every five days. • Decoding the human genome originally took 10 years to process; now it can be achieved in less than a day. The DNA sequencers have divided the sequencing cost by 10,000 in the last ten years, which is 100 times cheaper than the reduction in cost predicted by Moore\\'s Law.\\n\\n• The NASA Center for Climate Simulation (NCCS) stores 32 petabytes of climate observations and simulations on the Discover supercomputing cluster. • Google\\'s DNAStack compiles and organizes DNA samples of genetic data from around the world to identify diseases and other medical defects. These fast and exact calculations eliminate any \\'friction points,\\' or human errors that could be made by one of the numerous science and biology experts working with the DNA. DNAStack, a part of (Google)[https://search-engine-optimization-blog.github.io/Google-Search.md] Genomics, allows scientists to use the vast sample of resources from Google\\'s (search)[https://search-engine-optimization-blog.github.io/Audio-Search-Engine.md] server to scale social experiments that would usually take years, instantly. • 23andme\\'s DNA database contains genetic information of over 1,000,000 people worldwide.\\n\\nThe company explores selling the \"anonymous aggregated genetic data\" to other researchers and pharmaceutical companies for research purposes if patients give their consent. Ahmad Hariri, professor of psychology and neuroscience at Duke University who has been using 23andMe in his research since 2009 states that the most important aspect of the company\\'s new service is that it makes genetic research accessible and relatively cheap for scientists. A study that identified 15 genome sites linked to depression in 23andMe\\'s database lead to a surge in demands to access the repository with 23andMe fielding nearly 20 requests to access the depression data in the two weeks after publication of the paper. • Computational Fluid Dynamics (CFD) and hydrodynamic turbulence research generate massive datasets.\\n\\nThe Johns Hopkins Turbulence Databases (JHTDB) contains over 350 terabytes of spatiotemporal fields from Direct Numerical simulations of various turbulent flows. Such data have been difficult to share using traditional methods such as downloading flat simulation output files. The data within JHTDB can be accessed using \"virtual sensors\" with various access modes ranging from direct web-browser queries, access through Matlab, Python, Fortran and C programs executing on clients\\' platforms, to cut out services to download raw data. The data have been used in over 150 scientific publications.\\n\\nBig data can be used to improve training and understanding competitors, using sport sensors. It is also possible to predict winners in a match using big data analytics. Future performance of players could be predicted as well. Thus, players\\' value and salary is determined by data collected throughout the season.\\n\\nIn Formula One races, race cars with hundreds of sensors generate terabytes of data. These sensors collect data points from tire pressure to fuel burn efficiency. Based on the data, engineers and data analysts decide whether adjustments should be made in order to win a race. Besides, using big data, race teams try to predict the time they will finish the race beforehand, based on simulations using data collected over the season.\\n\\n• eBay. com uses two data warehouses at 7. 5 petabytes and 40PB as well as a 40PB Hadoop cluster for search, consumer recommendations, and merchandising. • Amazon. com handles millions of back-end operations every day, as well as queries from more than half a million third-party sellers. The core technology that keeps Amazon running is Linux-based and as of 2005 they had the world\\'s three largest Linux databases, with capacities of 7.\\n\\n8 TB, 18. 5 TB, and 24. 7 TB. • Facebook handles 50 billion photos from its user base.\\n\\nAs of June 2017 , Facebook reached 2 billion monthly active users. • Google was handling roughly 100 billion searches per month as of August 2012. Encrypted search and cluster formation in big data was demonstrated in March 2014 at the American Society of Engineering Education. Gautam Siwach engaged at Tackling the challenges of Big Data by MIT Computer Science and Artificial Intelligence Laboratory and Dr. Amir Esmailpour at UNH Research Group investigated the key features of big data as the formation of clusters and their interconnections.\\n\\nThey focused on the security of big data and the orientation of the term towards the presence of different types of data in an encrypted form at cloud interface by providing the raw definitions and real-time examples within the technology. Moreover, they proposed an approach for identifying the encoding technique to advance towards an expedited search over encrypted text leading to the security enhancements in big data. In March 2012, The White House announced a national \"Big Data Initiative\" that consisted of six Federal departments and agencies committing more than $200 million to big data research projects. The initiative included a National Science Foundation \"Expeditions in Computing\" grant of $10 million over 5 years to the AMPLabat the University of California, Berkeley.\\n\\nThe AMPLab also received funds from DARPA, and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestionto fighting cancer. The White House Big Data Initiative also included a commitment by the Department of Energy to provide $25 million in funding over 5 years to establish the Scalable Data Management, Analysis and Visualization (SDAV) Institute,led by the Energy Department\\'s Lawrence Berkeley National Laboratory. The SDAV Institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the Department\\'s supercomputers. The U.\\n\\nS. state of Massachusetts announced the Massachusetts Big Data Initiative in May 2012, which provides funding from the state government and private companies to a variety of research institutions. The Massachusetts Institute of Technology hosts the Intel Science and Technology Center for Big Data in the MIT Computer Science and Artificial Intelligence Laboratory, combining government, corporate, and institutional funding and research efforts. The European Commission is funding the 2-year-long Big Data Public Private Forum through their Seventh Framework Program to engage companies, academics and other stakeholders in discussing big data issues.\\n\\nThe project aims to define a strategy in terms of research and innovation to guide supporting actions from the European Commission in the successful implementation of the big data economy. Outcomes of this project will be used as input for Horizon 2020, their next framework program. The British government announced in March 2014 the founding of the Alan Turing Institute, named after the computer pioneer and code-breaker, which will focus on new ways to collect and analyze large data sets. At the University of Waterloo Stratford Campus Canadian Open Data Experience (CODE) Inspiration Day, participants demonstrated how using data visualization can increase the understanding and appeal of big data sets and communicate their story to the world.\\n\\nTo make manufacturing more competitive in the United States (and globe), there is a need to integrate more American ingenuity and innovation into manufacturing ; Therefore, National Science Foundation has granted the Industry University cooperative research center for Intelligent Maintenance Systems (IMS) at university of Cincinnati to focus on developing advanced predictive tools and techniques to be applicable in a big data environment. In May 2013, IMS Center held an industry advisory board meeting focusing on big data where presenters from various industrial companies discussed their concerns, issues and future goals in the big data environment. Computational social sciences – Anyone can use Application Programming Interfaces (APIs) provided by big data holders, such as Google and Twitter, to do research in the social and behavioral sciences. Often these APIs are provided for free. Tobias Preis et al.\\n\\nused Google Trends data to demonstrate that Internet users from countries with a higher per capita gross domestic product (GDP) are more likely to search for information about the future than information about the past. The findings suggest there may be a link between online behaviour and real-world economic indicators. The authors of the study examined Google queries logs made by ratio of the volume of searches for the coming year (\\'2011\\') to the volume of searches for the previous year (\\'2009\\'), which they call the \\'future orientation index\\'. They compared the future orientation index to the per capita GDP of each country, and found a strong tendency for countries where Google users inquire more about the future to have a higher GDP.\\n\\nThe results hint that there may potentially be a relationship between the economic success of a country and the information-seeking behavior of its citizens captured in big data. Tobias Preis and his colleagues Helen Susannah Moat and H. Eugene Stanley introduced a method to identify online precursors for stock market moves, using trading strategies based on search volume data provided by Google Trends. Their analysis of Google search volume for 98 terms of varying financial relevance, published in Scientific Reports,suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets. Big data sets come with algorithmic challenges that previously did not exist.\\n\\nHence, there is a need to fundamentally change the processing ways. The Workshops on Algorithms for Modern Massive Data Sets (MMDS) bring together computer scientists, statisticians, mathematicians, and data analysis practitioners to discuss algorithmic challenges of big data. Regarding big data, one needs to keep in mind that such concepts of magnitude are relative. As it is stated \"If the past is of any guidance, then today’s big data most likely will not be considered as such in the near future. \"An important research question that can be asked about big data sets is whether you need to look at the full data to draw certain conclusions about the properties of the data or is a sample good enough. The name big data itself contains a term related to size and this is an important characteristic of big data.\\n\\nBut Sampling (statistics) enables the selection of right data points from within the larger data set to estimate the characteristics of the whole population. For example, there are about 600 million tweets produced every day. Is it necessary to look at all of them to determine the topics that are discussed during the day? Is it necessary to look at all the tweets to determine the sentiment on each of the topics? In manufacturing different types of sensory data such as acoustics, vibration, pressure, current, voltage and controller data are available at short time intervals. To predict downtime it may not be necessary to look at all the data but a sample may be sufficient.\\n\\nBig Data can be broken down by various data point categories such as demographic, psychographic, behavioral, and transactional data. With large sets of data points, marketers are able to create and utilize more customized segments of consumers for more strategic targeting. There has been some work done in Sampling algorithms for big data. A theoretical formulation for sampling Twitter data has been developed. Critiques of the big data paradigm come in two flavors, those that question the implications of the approach itself, and those that question the way it is currently done.\\n\\nOne approach to this criticism is the field of critical data studies. \"A crucial problem is that we do not know much about the underlying empirical micro-processes that lead to the emergence of thetypical network characteristics of Big Data\". In their critique, Snijders, Matzat, and Reips point out that often very strong assumptions are made about mathematical properties that may not at all reflect what is really going on at the level of micro-processes. Mark Graham has leveled broad critiques at Chris Anderson\\'s assertion that big data will spell the end of theory:focusing in particular on the notion that big data must always be contextualized in their social, economic, and political contexts. Even as companies invest eight- and nine-figure sums to derive insight from information streaming in from suppliers and customers, less than 40% of employees have sufficiently mature processes and skills to do so.\\n\\nTo overcome this insight deficit, big data, no matter how comprehensive or well analyzed, must be complemented by \"big judgment,\" according to an article in the Harvard Business Review. Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably \"informed by the world as it was in the past, or, at best, as it currently is\". Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past. If the system\\'s dynamics of the future change (if it is not a stationary process), the past can say little about the future. In order to make predictions in changing environments, it would be necessary to have a thorough understanding of the systems dynamic, which requires theory.\\n\\nAs a response to this critique Alemany Oliver and Vayre suggest to use \"abductive reasoning as a first step in the research process in order to bring context to consumers\\' digital traces and make new theories emerge\". Additionally, it has been suggested to combine big data approaches with computer simulations, such as agent-based modelsand complex systems. Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms. Finally, the use of multivariate methods that probe for the latent structure of the data, such as factor analysis and cluster analysis, have proven useful as analytic approaches that go well beyond the bi-variate approaches (cross-tabs) typically employed with smaller data sets.\\n\\nIn health and biology, conventional scientific approaches are based on experimentation. For these approaches, the limiting factor is the relevant data that can confirm or refute the initial hypothesis. A new postulate is accepted now in biosciences: the information provided by the data in huge volumes (omics) without prior hypothesis is complementary and sometimes necessary to conventional approaches based on experimentation. In the massive approaches it is the formulation of a relevant hypothesis to explain the data that is the limiting factor. The search logic is reversed and the limits of induction (\"Glory of Science and Philosophy scandal\", C.\\n\\nD. Broad, 1926) are to be considered. Privacy advocates are concerned about the threat to privacy represented by increasing storage and integration of personally identifiable information; expert panels have released various policy recommendations to conform practice to expectations of privacy. The misuse of Big Data in several cases by media, companies and even the government has allowed for abolition of trust in almost every fundamental institution holding up society.\\n\\nNayef Al-Rodhan argues that a new kind of social contract will be needed to protect individual liberties in a context of Big Data and giant corporations that own vast amounts of information. The use of Big Data should be monitored and better regulated at the national and international levels. Barocas and Nissenbaum argue that one way of protecting individual users is by being informed about the types of information being collected, with whom it is shared, under what constrains and for what purposes. The \\'V\\' model of Big Data is concerting as it centres around computational scalability and lacks in a loss around the perceptibility and understandability of information. This led to the framework of cognitive big data, which characterises Big Data application according to:• Data completeness: understanding of the non-obvious from data; • Data correlation, causation, and predictability: causality as not essential requirement to achieve predictability; • Explainability and interpretability: humans desire to understand and accept what they understand, where algorithms don\\'t cope with this; • Level of automated decision making: algorithms that support automated decision making and algorithmic self-learning; Large data sets have been analyzed by computing machines for well over a century, including the US census analytics performed by IBM\\'s punch card machines which computed statistics including means and variances of populations across the whole continent.\\n\\nIn more recent decades, science experiments such as CERN have produced data on similar scales to current commercial \"big data\". However science experiments have tended to analyze their data using specialized custom-built high performance computing (supercomputing) clusters and grids, rather than clouds of cheap commodity computers as in the current commercial wave, implying a difference in both culture and technology stack. Ulf-Dietrich Reips and Uwe Matzat wrote in 2014 that big data had become a \"fad\" in scientific research. Researcher Danah Boyd has raised concerns about the use of big data in science neglecting principles such as choosing a representative sample by being too concerned about handling the huge amounts of data. This approach may lead to results bias in one way or another. Integration across heterogeneous data resources—some that might be considered big data and others not—presents formidable logistical as well as analytical challenges, but many researchers argue that such integrations are likely to represent the most promising new frontiers in science.\\n\\nIn the provocative article \"Critical Questions for Big Data\",the authors title big data a part of mythology: \"large data sets offer a higher form of intelligence and knowledge, with the aura of truth, objectivity, and accuracy\". Users of big data are often \"lost in the sheer volume of numbers\", and \"working with Big Data is still subjective, and what it quantifies does not necessarily have a closer claim on objective truth\". Recent developments in BI domain, such as pro-active reporting especially target improvements in usability of big data, through automated filtering of non-useful data and correlations. Big structures are full of spurious correlationseither because of non-causal coincidences (law of truly large numbers), solely nature of big randomness(Ramsey theory) or existence of non included factors so the hope, of early experimenters to make large databases of numbers \"speak for themselves\" and revolutionize scientific method, is questioned. Big data analysis is often shallow compared to analysis of smaller data sets. In many big data projects, there is no large data analysis happening, but the challenge is the extract, transform, load part of data preprocessing.\\n\\nBig data is a buzzword and a \"vague term\",but at the same time an \"obsession\"with entrepreneurs, consultants, scientists and the media. Big data showcases such as Google Flu Trends failed to deliver good predictions in recent years, overstating the flu outbreaks by a factor of two. Similarly, Academy awards and election predictions solely based on Twitter were more often off than on target. Big data often poses the same challenges as small data; adding more data does not solve problems of bias, but may emphasize other problems. In particular data sources such as Twitter are not representative of the overall population, and results drawn from such sources may then lead to wrong conclusions.\\n\\nGoogle Translate—which is based on big data statistical analysis of text—does a good job at translating web pages. However, results from specialized domains may be dramatically skewed. On the other hand, big data may also introduce new problems, such as the multiple comparisons problem: simultaneously testing a large set of hypotheses is likely to produce many false results that mistakenly appear significant. Ioannidis argued that \"most published research findings are false\"due to essentially the same effect: when many scientific teams and researchers each perform many experiments (i. e.\\n\\nprocess a big amount of scientific data; although not with big data technology), the likelihood of a \"significant\" result being false grows fast – even more so, when only positive results are published. Furthermore, big data analytics results are only as good as the model on which they are predicated. In an example, big data took part in attempting to predict the results of the 2016 U. S. Presidential Electionwith varying degrees of success. Big Data has been used in policing and surveillance by institutions like law enforcement and corporations.\\n\\nDue to the less visible nature of data-based surveillance as compared to traditional method of policing, objections to big data policing are less likely to arise. According to Sarah Brayne’s Big Data Surveillance: The Case of Policing, big data policing can reproduce existing societal inequalities in three ways: • Placing suspected criminals under increased surveillance by using the justification of a mathematical and therefore unbiased algorithm; • Increasing the scope and number of people that are subject to law enforcement tracking and exacerbating existing racial overrepresentation in the criminal justice system; • Encouraging members of society to abandon interactions with institutions that would create a digital trace, thus creating obstacles to social inclusion. If these potential problems are not corrected or regulating, the effects of big data policing continue to shape societal hierarchies. Conscientious usage of big data policing could prevent individual level biases from becoming institutional biases, Brayne also notes. • None Peter Kinnaird; Inbal Talgam-Cohen, eds.\\n\\n(2012). \"Big Data\". ACM Crossroads student magazine. XRDS: Crossroads, The ACM Magazine for Students. Vol.\\n\\n19 no. 1. Association for Computing Machinery. ISSN 1528-4980.\\n\\nOCLC 779657714. • None Viktor Mayer-Schönberger; Kenneth Cukier (2013). Big Data: A Revolution that Will Transform how We Live, Work, and Think. Houghton Mifflin Harcourt. ISBN.\\n\\nOCLC 828620988. • None Press, Gil (9 May 2013). \"A Very Short History Of Big Data\". forbes.\\n\\ncom. Jersey City, NJ: Forbes Magazine. • None O\\'Neil, Cathy (2017). Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Broadway Books.\\n\\nISBN. • The dictionary definition of big data at Wiktionary.',\n",
       "    'local_folder': 'site\\\\files\\\\data_science',\n",
       "    'local_filename': 'site\\\\files\\\\data_science\\\\2019-11-13-Big-Data.md',\n",
       "    'site_host': 'https://data-science-blog.github.io',\n",
       "    'site_topic': 'Data Science',\n",
       "    'page_slug': 'Big-Data.md',\n",
       "    'page_filename': '2019-11-13-Big-Data.md',\n",
       "    'page_url': 'https://data-science-blog.github.io/Big-Data.md'},\n",
       "   {'page_topic': 'Black Swan Data',\n",
       "    'page_title': 'Black Swan Data | Data Science',\n",
       "    'page_content': \"Black Swan (Data)[https://data-science-blog.github.io/Big-Data.md] is a London-based (technology)[https://data-science-blog.github.io/Committee-On-Data-For-Science-And-Technology.md] and (data)[https://data-science-blog.github.io/Data.md] science company that produces a social prediction platform called Trendscope. Trendscope uses predictive data science and proprietary Natural Language Processing to analyze Social data conversations that help businesses identify potential trends and (customer)[https://data-science-blog.github.io/Customer-Data-Platform.md] behaviors. Its notable clients include PepsiCo, Unilever, McDonald's, Danone, Disney and numerous others. In 2016, the company raised a total of £9. 2 million in two separate funding rounds led by investors like Mitsui, Albion Ventures, and The Blackstone Group.\\n\\nThe company is headquartered in London and has offices in Lake Forest, New York, Budapest, Szeged, Cape Town and Exeter. Black Swan Data was founded in London by Steve King (CEO) and Hugo Amos (CMO) in 2011. In 2012, the firm raised £2. 5 million from The Blackstone Group. In 2013 the company's aviation division was founded.\\n\\nBy 2014, the company counted Disney, Tesco, Panasonic Avionics, Samsung, Debenhams, Argos, and Vodafone among its clients. It maintained regional offices in Hong Kong and Los Angeles and opened a new office in Exeter, Devon. The company was also listed among The Sunday Times Tech Track Top 10 Ones to Watch. By early 2015, the company had added offices in Budapest, Manchester, and New York and was employing around 150 people. In late 2015, Black Swan Data had around 200 employees and had added several clients, including GlaxoSmithKline, Unilever, and Mars. The company was also listed first on the inaugural Sunday Times Sage Start-up Track 15 which identifies the fastest growing start-ups in Britain.\\n\\nIn March 2016, the company received £3 million in a funding round led by Mitsui. According to the company, the funding was procured to further develop its Nest platform and expand the business into Japan and the United States. In July 2016, Black Swan Data raised an additional £6. 2 million from an investor group that included Albion Ventures, The Blackstone Group, and Mitsui. This capital would again be designated for the development of Nest and further international expansion.\\n\\nAlso in 2016, the company topped The Sunday Times SME Export Track 100. In 2017 the company ranked 6th fastest growing European SME in Financial Times 1000. Again in 2018, the company ranked 79 in Financial Times 1000. The same year its offshoot charity White Swan Charity officially launched. As of early 2018, Black Swan Data has over 260 employees, spread over eight offices in four countries, and counts major brands in the food, beverages and personal care sectors as its key clients for trend prediction and product innovation. Black Swan Data's primary business offering is their proprietary data analytics (software)[https://python-software.github.io/Eric-Software.md] platform called Trendscope.\\n\\nThe platform ingests millions of real-time, publicly available conversations from various Social data sources, including social media information, blogs and news. Trendscope filters and cleans this data, then uses algorithms to predict and forecast information to help businesses make marketing and supply chain decisions. In general, the data that Trendscope analyzes and the predictions it yields vary greatly depending on the client. For instance, for Disney's film Frozen, Black Swan Data analysts identified information from Rotten Tomatoes and IMDb, data about movies released prior to Frozen, and videos that people were watching on YouTube as predictors for the potential success of the film and any related merchandise (and their requisite supply chains). By contrast, Black Swan Data worked with PepsiCo to help them apply predictive technology to their business and keep up with health and lifestyle trends across food and beverages.\\n\\nThe company's aviation arm Fethr combines connectivity, data and predictive analytics to improve travel experiences. It has partnerships with Panasonic and Gate Group. .\",\n",
       "    'local_folder': 'site\\\\files\\\\data_science',\n",
       "    'local_filename': 'site\\\\files\\\\data_science\\\\2019-11-13-Black-Swan-Data.md',\n",
       "    'site_host': 'https://data-science-blog.github.io',\n",
       "    'site_topic': 'Data Science',\n",
       "    'page_slug': 'Black-Swan-Data.md',\n",
       "    'page_filename': '2019-11-13-Black-Swan-Data.md',\n",
       "    'page_url': 'https://data-science-blog.github.io/Black-Swan-Data.md'},\n",
       "   {'page_topic': 'Chief Data Officer',\n",
       "    'page_title': 'Chief Data Officer | Data Science',\n",
       "    'page_content': \"A chief (data)[https://data-science-blog.github.io/Big-Data.md] officer (CDO) is a corporate officer responsible for enterprise-wide governance and utilization of information as an asset, via (data)[https://data-science-blog.github.io/Data.md] processing, analysis, data mining, information trading and other means. CDOs usually report to the chief executive officer (CEO), although depending on the area of expertise this can vary. The CDO is a member of the executive management team and manager of enterprise-wide data processing and data mining. The Chief Data Officer title shares its abbreviation with the Chief Digital Officer but the two are not the same job. The Chief Data Officer has a significant measure of business responsibility for determining what kinds of information the enterprise will choose to capture, retain and exploit and for what purposes. However, the similar-sounding Chief Digital Officer or Chief Digital Information Officer often does not bear that business responsibility, but rather is responsible for the information systems through which data is stored and processed.\\n\\nThe role of manager for data processing was not elevated to that of senior management prior to the 1980s. As organizations have recognized the importance of information (technology)[https://data-science-blog.github.io/Committee-On-Data-For-Science-And-Technology.md] as well as business intelligence, data integration, master data management and data processing to the fundamental functioning of everyday business, this role has become more visible and crucial. This role includes defining strategic priorities for the company in the area of data systems and opportunities, identifying new business opportunities pertaining to data, optimizing revenue generation through data, and generally representing data as a strategic business asset at the executive table. With the rise in service-oriented architectures (SOA), large-scale system integration, and heterogeneous data storage/exchange mechanisms (databases, XML, EDI, etc. ), it is necessary to have a high-level individual, who possesses a combination of business knowledge, technical skills, and people skills, guide data strategy.\\n\\nBesides the revenue opportunities, acquisition strategy, and (customer)[https://data-science-blog.github.io/Customer-Data-Platform.md] data policies, the chief data officer is charged with explaining the strategic value of data and its important role as a business asset and revenue driver to executives, employees, and customers. This contrasts with the older view of data systems as mere back-end IT systems. More recently, with the adoption of data science the Chief Data Officer is sometimes looked upon as the key strategy person either reporting to the Chief Strategy Officer or serving the role of CSO in lieu of one. This person has the responsibility of measurement along various business lines and consequently defining the strategy for the next growth opportunities, product offerings, markets to pursue, competitors to look at etc. This is seen in organizations like Chartis, AllState and Fidelity • Cathryne Clay Doss of Capital One was appointed chief data officer in 2002. • Usama Fayyad, Chief Data Officer and Senior Vice President of Yahoo! in 2005.\\n\\n• John Bottega was CDO for CitiGroup's Corporate and Investment Banking (CIB) unit, and later for the New York Federal Reserve. • Philip Bourne is associate director for data science at the National Institutes of Health• Usama Fayyad was the first CDO for Yahoo! in 2004-2009 and also the CDO of Barclays in London from 2013-2016. He showed how the role can generate value by creating a $500 million new revenue source based on behavioral Targeting of Ads for Yahoo! in 2008. • Maria Villar was appointed CDO at Fannie Mae and Justin Magruder was appointed CDO at Freddie Mac in 2007, in the months leading up to the 2008 Credit Crisis to assist the GSE's in implementing new financial control frameworks for risk management.\\n\\nBeth Hiatt succeeded Maria and Diane Schmidt succeeded Justin in 2009. • Henri Verdier was appointed in September 2014 Administrateur Général des Données (CDO) of the French Administration, probably the first CDO of a Country. • Zachary Townsend was appointed the inaugural CDO of California by Jerry Brown in July 2016. The role has wide authority around transparency, efficiency and accountability in state operations.\\n\\nFollowing the 2008 credit crisis, many major banks and insurance companies created the CDO role to ensure data quality and transparency for regulatory and risk management as well as analytic reporting. .\",\n",
       "    'local_folder': 'site\\\\files\\\\data_science',\n",
       "    'local_filename': 'site\\\\files\\\\data_science\\\\2019-11-13-Chief-Data-Officer.md',\n",
       "    'site_host': 'https://data-science-blog.github.io',\n",
       "    'site_topic': 'Data Science',\n",
       "    'page_slug': 'Chief-Data-Officer.md',\n",
       "    'page_filename': '2019-11-13-Chief-Data-Officer.md',\n",
       "    'page_url': 'https://data-science-blog.github.io/Chief-Data-Officer.md'},\n",
       "   {'page_topic': 'Coding Bootcamp',\n",
       "    'page_title': 'Coding Bootcamp | Data Science',\n",
       "    'page_content': 'Coding bootcamps are intensive programs of (software)[https://python-software.github.io/Eric-Software.md] development which started in 2011. The first coding bootcamps were opened in 2011 with the Code Academy (now Starter League). As of July 2017, there were 95 full-time coding bootcamp courses in the United States. The length of courses typically ranges from between 8 and 36 weeks, with most lasting 10 to 12 (averaging 12. 9) weeks.\\n\\nFollowing the increased popularity of coding bootcamps, some universities have started their own intensive coding programs or partnered with existing private coding bootcamps. There are various online options for online bootcamps. These usually work by matching students with a mentor and are also generally cheaper and more accommodating to specific student needs. Bootcamps that focus less on full stack development and more on producing (data)[https://data-science-blog.github.io/Big-Data.md] scientists and (data)[https://data-science-blog.github.io/Data.md] engineers are known as data science bootcamps. Coding bootcamps may be selective and require minimum skills; some companies such as Career Karma aim to help novices learn prerequisite skills and apply to bootcamps.\\n\\nCoding bootcamps can be part-time or online, they may be funded by employers or qualify for student loans. According to a 2017 market research report, tuition ranged from free to $21,000 for a course, with an average tuition of $11,874. \"Deferred Tuition\" refers to a payment model in which students pay the school a percentage (18%-22. 5%) of their salary for 1–3 years after graduation, instead of upfront tuition. In Europe, coding bootcamps can be free or a couple thousand euros per program.\\n\\nIn contrast to formal university education, private offerings for training appear expensive. On August 16, 2016, the US Department of Education announced up to $17 million in loans or grants for students to study with nontraditional training providers, including coding bootcamps. These grants or loans will be administered through the pilot program, EQUIP which stands for Educational Quality through Innovation Partnerships. Programs must partner with an accredited college and third-party quality assurance entity (QAE) in order to receive federal financial aid. In 2016, there were concerns that partnering private coding bootcamps with federal financial aid could attract less reputable organizations to create coding bootcamp programs.\\n\\nBarriers to entry and exit mean established schools face less competition than in a free market, which can lead to deterioration of quality, and increase in prices. Also, problems within traditional university models could easily transfer to the university/bootcamp partnerships. On the other hand, others believe that enhancing policy around financial aid will help lower income prospective students attend. There are several sentiments of coding bootcamps being accessible only for the rich.\\n\\n.',\n",
       "    'local_folder': 'site\\\\files\\\\data_science',\n",
       "    'local_filename': 'site\\\\files\\\\data_science\\\\2019-11-13-Coding-Bootcamp.md',\n",
       "    'site_host': 'https://data-science-blog.github.io',\n",
       "    'site_topic': 'Data Science',\n",
       "    'page_slug': 'Coding-Bootcamp.md',\n",
       "    'page_filename': '2019-11-13-Coding-Bootcamp.md',\n",
       "    'page_url': 'https://data-science-blog.github.io/Coding-Bootcamp.md'},\n",
       "   {'page_topic': 'Committee On Data For Science And Technology',\n",
       "    'page_title': 'Committee On Data For Science And Technology | Data Science',\n",
       "    'page_content': 'The Committee on (Data)[https://data-science-blog.github.io/Big-Data.md] for Science and Technology (CODATA) was established in 1966 as an interdisciplinary committee of the International Council for Science. It seeks to improve the compilation, critical evaluation, storage, and retrieval of (data)[https://data-science-blog.github.io/Data.md] of importance to science and technology. CODATA sponsors the CODATA international conferenceevery two years. CODATA is best known for (and sometimes confused with) its Task Group on Fundamental Constants (TGFC). Established in 1969, its purpose is to periodically provide the international scientific and technological communities with an internationally accepted set of values of the fundamental physical constants and closely related conversion factors for use worldwide.\\n\\nThe first such CODATA set was published in 1973. Later versions are named based on the year of the data incorporated; the 1986 CODATA (published April 1987) used data up to 1 January 1986. All subsequent releases use data up to the end of the stated year, and are necessarily published a year or two later: 1998 (April 2000),2002 (January 2005),2006 (June 2008)and the sixth in 2010 (November 2012). The latest version is Version 7. 0 called \"2014 CODATA\" published on 25 June 2015.\\n\\nThe CODATA recommended values of fundamental physical constants are published at the NIST Reference on Constants, Units, and Uncertainty. Since 1998, the task group has produced a new version every four years, incorporating results published up to the end of the specified year. In order to support the upcoming redefinition of the SI base units,adopted at the 26th General Conference on Weights and Measures on 16 November 2018, CODATA made a special release that was published in October 2017. It incorporates all data up to 1 July 2017,:4,67and determines the final numerical values of h, e, k, and N that are to be used for the new SI definitions. The next regular version, with a closing date of 31 December 2018,will be used to produce the new 2018 CODATA values that will be made available by the time the revised SI come into force on 20 May 2019.\\n\\nThis is necessary because the redefinitions have a significant (mostly beneficial) effect on the uncertainties and correlation coefficients reported by CODATA. • None International Bureau of Weights and Measures (2019-05-20), SI Brochure: The International System of Units (SI) (9th ed. ), ISBN • None International Bureau of Weights and Measures (BIPM) (2017-08-10). \"Input data for the special CODATA-2017 adjustment\".\\n\\nMetrologia (Updated ed. ). • None \"The NIST references on constants, units, and uncertainty\". Archived from the original on 2018-12-23. .',\n",
       "    'local_folder': 'site\\\\files\\\\data_science',\n",
       "    'local_filename': 'site\\\\files\\\\data_science\\\\2019-11-13-Committee-On-Data-For-Science-And-Technology.md',\n",
       "    'site_host': 'https://data-science-blog.github.io',\n",
       "    'site_topic': 'Data Science',\n",
       "    'page_slug': 'Committee-On-Data-For-Science-And-Technology.md',\n",
       "    'page_filename': '2019-11-13-Committee-On-Data-For-Science-And-Technology.md',\n",
       "    'page_url': 'https://data-science-blog.github.io/Committee-On-Data-For-Science-And-Technology.md'},\n",
       "   {'page_topic': 'Consistency Database Systems',\n",
       "    'page_title': 'Consistency Database Systems | Data Science',\n",
       "    'page_content': 'Consistency in database systems refers to the requirement that any given database transaction must change affected (data)[https://data-science-blog.github.io/Big-Data.md] only in allowed ways. Any (data)[https://data-science-blog.github.io/Data.md] written to the database must be valid according to all defined rules, including constraints, cascades, triggers, and any combination thereof. This does not guarantee correctness of the transaction in all ways the application programmer might have wanted (that is the responsibility of application-level code) but merely that any (programming)[https://python-software.github.io/Core-Python-Programming.md] errors cannot result in the violation of any defined database constraints. Consistency is one of the four guarantees that define ACID transactions; however, significant ambiguity exists about the nature of this guarantee.\\n\\nIt is defined variously as: • The guarantee that any transactions started in the future necessarily see the effects of other transactions committed in the past• The guarantee that database constraints are not violated, particularly once a transaction commits• The guarantee that operations in transactions are performed accurately, correctly, and with validity, with respect to application semanticsAs these various definitions are not mutually exclusive, it is possible to design a system that guarantees \"consistency\" in every sense of the word, as most relational database management systems in common use today arguably do. The CAP theorem is based on three trade-offs, one of which is \"atomic consistency\" (shortened to \"consistency\" for the acronym), about which the authors note, \"Discussing atomic consistency is somewhat different than talking about an ACID database, as database consistency refers to transactions, while atomic consistency refers only to a property of a single request/response operation sequence. And it has a different meaning than the Atomic in ACID, as it subsumes the database notions of both Atomic and Consistent. \".',\n",
       "    'local_folder': 'site\\\\files\\\\data_science',\n",
       "    'local_filename': 'site\\\\files\\\\data_science\\\\2019-11-13-Consistency-Database-Systems.md',\n",
       "    'site_host': 'https://data-science-blog.github.io',\n",
       "    'site_topic': 'Data Science',\n",
       "    'page_slug': 'Consistency-Database-Systems.md',\n",
       "    'page_filename': '2019-11-13-Consistency-Database-Systems.md',\n",
       "    'page_url': 'https://data-science-blog.github.io/Consistency-Database-Systems.md'},\n",
       "   {'page_topic': 'Customer Data Platform',\n",
       "    'page_title': 'Customer Data Platform | Data Science',\n",
       "    'page_content': 'A customer (data)[https://data-science-blog.github.io/Big-Data.md] platform (CDP) is a type of packaged (software)[https://python-software.github.io/Eric-Software.md] which creates a persistent, unified customer database that is accessible to other systems. (Data)[https://data-science-blog.github.io/Data.md] is pulled from multiple sources, cleaned and combined to create a single customer profile. This structured data is then made available to other marketing systems. According to Gartner, customer data platforms have evolved from a variety of mature markets, \"including multichannel campaign management, tag management and data integration.\\n\\n\"The CDP market is currently a $300 million industry and projected to reach $1 billion by 2019. In addition, some CDPs provide additional functions such as marketing performance measurement analytics, predictive modeling, and content marketing. • unified, persistent, single database for customer behavioral, profile and other data, from any internal or external source; • consistent identifier that links all of a customer\\'s data; • accessible by external systems and structured to support marketers\\' needs for campaign management, marketing analyses and business intelligence;• allow users the capability to predict the optimum next move with a customer. A main advantage of a CDP is its ability to collect data from a variety of sources (both online and offline, with a variety of formats and structures) and convert that disparate data into a standardized form.\\n\\nSome of the data types a standard CDP should work with include: • Customer events: Browsing activity, actions on a website or in an app, clicks on a banner, etc. • Customer-company history: data from interactions with customer service, NPS scores, data from chatbots, etc. A CDP is fundamentally different in design and function when compared with marketing automation systems, though CDPs provide some of the functionality of marketing systems and customer engagement platforms. CDP tools are designed to talk to other systems. They retain details from other systems that the engagement or automation tool does not.\\n\\nThis is valuable for trend analysis, predictive analytics, and recommendations that can leverage historical data. A Data Management Platform (DMP) collects anonymous web and digital data. CDPs collect data that is tied to an identifiable individual. Users of CDP can leverage the intelligence to provide more personalized content and delivery. A data warehouse or data lake collects data, usually from the same source and with the same structure of information.\\n\\nWhile this information can be manually synthesized, neither type of system delivers the identity resolution needed to build a consolidated single customer view. Data warehouses are often updated at scheduled intervals whereas CDPs ingest and make available data in real-time. In practice, most CDPs use the same technologies as data lakes; the difference is the CDP has built-in features to do additional processing to make the data usable, while a data lake may not. Although similar tools existed in the past, the term Customer Data Platform was first used in 2013. It was meant to describe a marketing software that could build a single customer view (a collection of all of a customer’s data and events into one file).\\n\\nThese databases were originally used to power some other type of software, such as a marketing automation suite, a personalization engine, or a campaign management tool. The power of the database behind these systems eventually became desirable in its own right. They evolved to become full-fledged software. Simultaneously, some tag management and web analytics providers also transformed their platforms into similar solutions, creating CDPs with a different origin but the same use. These platforms became successful, and by 2016 they had become the CDP industry. This industry experienced quick growth, due to marketers recognizing the shortcomings of alternatives like DMPs and data lakes, as well as the capabilities a CDP could offer them.\\n\\n.',\n",
       "    'local_folder': 'site\\\\files\\\\data_science',\n",
       "    'local_filename': 'site\\\\files\\\\data_science\\\\2019-11-13-Customer-Data-Platform.md',\n",
       "    'site_host': 'https://data-science-blog.github.io',\n",
       "    'site_topic': 'Data Science',\n",
       "    'page_slug': 'Customer-Data-Platform.md',\n",
       "    'page_filename': '2019-11-13-Customer-Data-Platform.md',\n",
       "    'page_url': 'https://data-science-blog.github.io/Customer-Data-Platform.md'},\n",
       "   {'page_topic': 'Data',\n",
       "    'page_title': 'Data | Data Science',\n",
       "    'page_content': 'Data are individual units of information. A datum describes a single quality or quantity of some object or phenomenon. In analytical processes, (data)[https://data-science-blog.github.io/Big-Data.md] are represented by variables. Although the terms \"data\", \"information\" and \"knowledge\" are often used interchangeably, each of these terms has a distinct meaning. In popular publications, data is sometimes said to be transformed into information when it is viewed in context or in post-analysis. .\\n\\nIn academic treatments of the subject, however, data are simply units of information. Data is employed in scientific research, businesses management (e. g. , sales data, revenue, profits, stock price), finance, governance (e. g. , crime rates, unemployment rates, literacy rates), and in virtually every other form of human organizational activity (e.\\n\\ng. , censuses of the number of homeless people by non-profit organizations). Data is measured, collected and reported, and analyzed, whereupon it can be visualized using graphs, images or other analysis tools. Data as a general concept refers to the fact that some existing information or knowledge is represented or coded in some form suitable for better usage or processing. Raw data (\"unprocessed data\") is a collection of numbers or characters before it has been \"cleaned\" and corrected by researchers.\\n\\nRaw data needs to be corrected to remove outliers or obvious instrument or data entry errors (e. g. , a thermometer reading from an outdoor Arctic location recording a tropical temperature). Data processing commonly occurs by stages, and the \"processed data\" from one stage may be considered the \"raw data\" of the next stage. Field data is raw data that is collected in an uncontrolled \"in situ\" environment.\\n\\nExperimental data is data that is generated within the context of a scientific investigation by observation and recording. Data has been described as the new oil of the digital economy. The first English use of the word \"data\" is from the 1640s. The word \"data\" was first used to mean \"transmissible and storable computer information\" in 1946. The expression \"data processing\" was first used in 1954.\\n\\nThe Latin word data is the plural of datum, \"(thing) given,\" neuter past participle of dare \"to give\". Data may be used as a plural noun in this sense, with some writers—usually scientific writers—in the 20th century using datum in the singular and data for plural. However, over the course of time this usage has vanishedfrom the English language, and everyday writing, \"data\" is most commonly used in the singular, as a mass noun (like \"information\", \"sand\" or \"rain\"). The APA manual of style requires \"data\" to be plural. Data, information, knowledge and wisdom are closely related concepts, but each has its own role in relation to the other, and each term has its own meaning.\\n\\nAccording to a common view, data is collected and analyzed; data only becomes information suitable for making decisions once it has been analyzed in some fashion. One can say that the extent to which a set of data is informative to someone depends on the extent to which it is unexpected by that person. The amount of information content in a data stream may be characterized by its Shannon entropy. Knowledge is the understanding based on extensive experience dealing with information on a subject. For example, the height of Mount Everest is generally considered data.\\n\\nThe height can be measured precisely with an altimeter and entered into a database. This data may be included in a book along with other data on Mount Everest to describe the mountain in a manner useful for those who wish to make a decision about the best method to climb it. An understanding based on experience climbing mountains that could advise persons on the way to reach Mount Everest\\'s peak may be seen as \"knowledge\". The practical climbing of Mount Everest\\'s peak based on this knowledge made be seen as \"wisdom\". In other words, wisdom refers to the practical application of a person\\'s knowledge in those circumstances where good may result.\\n\\nThus wisdom complements and completes the series \"data\", \"information\" and \"knowledge\" of increasingly abstract concepts. Data is often assumed to be the least abstract concept, information the next least, and knowledge the most abstract. In this view, data becomes information by interpretation; e. g.\\n\\n, the height of Mount Everest is generally considered \"data\", a book on Mount Everest geological characteristics may be considered \"information\", and a climber\\'s guidebook containing practical information on the best way to reach Mount Everest\\'s peak may be considered \"knowledge\". \"Information\" bears a diversity of meanings that ranges from everyday usage to technical use. This view, however, has also been argued to reverse the way in which data emerges from information, and information from knowledge. Generally speaking, the concept of information is closely related to notions of constraint, communication, control, data, form, instruction, knowledge, meaning, mental stimulus, pattern, perception, and representation. Beynon-Davies uses the concept of a sign to differentiate between data and information; data is a series of symbols, while information occurs when the symbols are used to refer to something. Before the development of computing devices and machines, people had to manually collect data and impose patterns on it.\\n\\nSince the development of computing devices and machines, these devices can also collect data. In the 2010s, computers are widely used in many fields to collect data and sort or process it, in disciplines ranging from marketing, analysis of social services usage by citizens to scientific research. These patterns in data are seen as information which can be used to enhance knowledge. These patterns may be interpreted as \"truth\" (though \"truth\" can be a subjective concept), and may be authorized as aesthetic and ethical criteria in some disciplines or cultures. Events that leave behind perceivable physical or virtual remains can be traced back through data.\\n\\nMarks are no longer considered data once the link between the mark and observation is broken. Mechanical computing devices are classified according to the means by which they represent data. An analog computer represents a datum as a voltage, distance, position, or other physical quantity. A digital computer represents a piece of data as a sequence of symbols drawn from a fixed alphabet.\\n\\nThe most common digital computers use a binary alphabet, that is, an alphabet of two characters, typically denoted \"0\" and \"1\". More familiar representations, such as numbers or letters, are then constructed from the binary alphabet. Some special forms of data are distinguished. A computer program is a collection of data, which can be interpreted as instructions. Most computer languages make a distinction between programs and the other data on which programs operate, but in some languages, notably Lisp and similar languages, programs are essentially indistinguishable from other data. It is also useful to distinguish metadata, that is, a description of other data.\\n\\nA similar yet earlier term for metadata is \"ancillary data. \" The prototypical example of metadata is the library catalog, which is a description of the contents of books. Gathering data can be accomplished through a primary source (the researcher is the first person to obtain the data) or a secondary source (the researcher obtains the data that has already been collected by other sources, such as data disseminated in a scientific journal). Data analysis methodologies vary and include data triangulation and data percolation.\\n\\nThe latter offers an articulate method of collecting, classifying and analyzing data using five possible angles of analysis (at least three) in order to maximize the research\\'s objectivity and permit an understanding of the phenomena under investigation as complete as possible: qualitative and quantitative methods, literature reviews (including scholarly articles), interviews with experts, and computer simulation. The data is thereafter \"percolated\" using a series of pre-determined steps so as to extract the most relevant information. Although data is also increasingly used in other fields, it has been suggested that the highly interpretive nature of them might be at odds with the ethos of data as \"given\". Peter Checkland introduced the term capta (from the Latin capere, “to take”) to distinguish between an immense number of possible data and a sub-set of them, to which attention is oriented. Johanna Drucker has argued that since the humanities affirm knowledge production as \"situated, partial, and constitutive,\" using data may introduce assumptions that are counterproductive, for example that phenomena are discrete or are observer-independent.\\n\\nThe term capta, which emphasizes the act of observation as constitutive, is offered as an alternative to data for visual representations in the humanities. This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the \"relicensing\" terms of the GFDL, version 1. 3 or later. .',\n",
       "    'local_folder': 'site\\\\files\\\\data_science',\n",
       "    'local_filename': 'site\\\\files\\\\data_science\\\\2019-11-13-Data.md',\n",
       "    'site_host': 'https://data-science-blog.github.io',\n",
       "    'site_topic': 'Data Science',\n",
       "    'page_slug': 'Data.md',\n",
       "    'page_filename': '2019-11-13-Data.md',\n",
       "    'page_url': 'https://data-science-blog.github.io/Data.md'},\n",
       "   {'page_topic': 'Dataintensive Computing',\n",
       "    'page_title': 'Dataintensive Computing | Data Science',\n",
       "    'page_content': 'Data-intensive computing is a class of parallel computing applications which use a (data)[https://data-science-blog.github.io/Big-Data.md] parallel approach to process large volumes of (data)[https://data-science-blog.github.io/Data.md] typically terabytes or petabytes in size and typically referred to as big data. Computing applications which devote most of their execution time to computational requirements are deemed compute-intensive, whereas computing applications which require large volumes of data and devote most of their processing time to I/O and manipulation of data are deemed data-intensive. The rapid growth of the Internet and World Wide Web led to vast amounts of information available online. In addition, business and government organizations create large amounts of both structured and unstructured information which needs to be processed, analyzed, and linked. Vinton Cerf described this as an “information avalanche” and stated “we must harness the Internet’s energy before the information it has unleashed buries us”. An IDC white paper sponsored by EMC Corporation estimated the amount of information currently stored in a digital form in 2007 at 281 exabytes and the overall compound growth rate at 57% with information in organizations growing at even a faster rate.\\n\\nIn a 2003 study of the so-called information explosion it was estimated that 95% of all current information exists in unstructured form with increased data processing requirements compared to structured information. The storing, managing, accessing, and processing of this vast amount of data represents a fundamental need and an immense challenge in order to satisfy needs to search, analyze, mine, and visualize this data as information. Data-intensive computing is intended to address this need. Parallel processing approaches can be generally classified as either compute-intensive, or data-intensive. Compute-intensive is used to describe application programs that are compute bound.\\n\\nSuch applications devote most of their execution time to computational requirements as opposed to I/O, and typically require small volumes of data. Parallel processing of compute-intensive applications typically involves parallelizing individual algorithms within an application process, and decomposing the overall application process into separate tasks, which can then be executed in parallel on an appropriate computing platform to achieve overall higher performance than serial processing. In compute-intensive applications, multiple operations are performed simultaneously, with each operation addressing a particular part of the problem. This is often referred to as task parallelism. Data-intensive is used to describe applications that are I/O bound or with a need to process large volumes of data. Such applications devote most of their processing time to I/O and movement and manipulation of data.\\n\\nParallel processing of data-intensive applications typically involves partitioning or subdividing the data into multiple segments which can be processed independently using the same executable application program in parallel on an appropriate computing platform, then reassembling the results to produce the completed output data. The greater the aggregate (distribution)[https://python-software.github.io/Anaconda-Python-Distribution.md] of the data, the more benefit there is in parallel processing of the data. Data-intensive processing requirements normally scale linearly according to the size of the data and are very amenable to straightforward parallelization. The fundamental challenges for data-intensive computing are managing and processing exponentially growing data volumes, significantly reducing associated data analysis cycles to support practical, timely applications, and developing new algorithms which can scale to (search)[https://search-engine-optimization-blog.github.io/Audio-Search-Engine.md] and process massive amounts of data. Researchers coined the term BORPS for \"billions of records per second\" to measure record processing speed in a way analogous to how the term MIPS applies to describe computers\\' processing speed. Computer system architectures which can support data parallel applications were promoted in the early 2000s for large-scale data processing requirements of data-intensive computing.\\n\\nData-parallelism applied computation independently to each data item of a set of data, which allows the degree of parallelism to be scaled with the volume of data. The most important reason for developing data-parallel applications is the potential for scalable performance, and may result in several orders of magnitude performance improvement. The key issues with developing applications using data-parallelism are the choice of the algorithm, the strategy for data decomposition, load balancing on processing nodes, message passing communications between nodes, and the overall accuracy of the results. The development of a data parallel application can involve substantial (programming)[https://python-software.github.io/Core-Python-Programming.md] complexity to define the problem in the context of available programming tools, and to address limitations of the target architecture. Information extraction from and indexing of Web documents is typical of data-intensive computing which can derive significant performance benefits from data parallel implementations since Web and other types of document collections can typically then be processed in parallel.\\n\\nThe US National Science Foundation (NSF) funded a research program from 2009 through 2010. Areas of focus were: • Approaches to parallel programming to address the parallel processing of data on data-intensive systems • Programming abstractions including models, languages, and algorithms which allow a natural expression of parallel processing of data • Design of data-intensive computing platforms to provide high levels of reliability, efficiency, availability, and scalability. • Identifying applications that can exploit this computing paradigm and determining how it should evolve to support emerging data-intensive applications Pacific Northwest National Labs defined data-intensive computing as “capturing, managing, analyzing, and understanding data at volumes and rates that push the frontiers of current technologies”. Data-intensive computing platforms typically use a parallel computing approach combining multiple processors and disks in large commodity computing clusters connected using high-speed communications switches and networks which allows the data to be partitioned among the available computing resources and processed independently to achieve performance and scalability based on the amount of data. A cluster can be defined as a type of parallel and distributed system, which consists of a collection of inter-connected stand-alone computers working together as a single integrated computing resource. This approach to parallel processing is often referred to as a “shared nothing” approach since each node consisting of processor, local memory, and disk resources shares nothing with other nodes in the cluster.\\n\\nIn parallel computing this approach is considered suitable for data-intensive computing and problems which are “embarrassingly parallel”, i. e. where it is relatively easy to separate the problem into a number of parallel tasks and there is no dependency or communication required between the tasks other than overall management of the tasks. These types of data processing problems are inherently adaptable to various forms of distributed computing including clusters, data grids, and cloud computing. Several common characteristics of data-intensive computing systems distinguish them from other forms of computing: • The principle of collection of the data and programs or algorithms is used to perform the computation.\\n\\nTo achieve high performance in data-intensive computing, it is important to minimize the movement of data. This characteristic allows processing algorithms to execute on the nodes where the data resides reducing system overhead and increasing performance. Newer technologies such as InfiniBand allow data to be stored in a separate repository and provide performance comparable to collocated data. • The programming model utilized. Data-intensive computing systems utilize a machine-independent approach in which applications are expressed in terms of high-level operations on data, and the runtime system transparently controls the scheduling, execution, load balancing, communications, and movement of programs and data across the distributed computing cluster. The programming abstraction and language tools allow the processing to be expressed in terms of data flows and transformations incorporating new dataflow programming languages and shared libraries of common data manipulation algorithms such as sorting.\\n\\n• A focus on reliability and availability. Large-scale systems with hundreds or thousands of processing nodes are inherently more susceptible to hardware failures, communications errors, and (software)[https://python-software.github.io/Eric-Software.md] bugs. Data-intensive computing systems are designed to be fault resilient. This typically includes redundant copies of all data files on disk, storage of intermediate processing results on disk, automatic detection of node or processing failures, and selective re-computation of results.\\n\\n• The inherent scalability of the underlying hardware and software architecture. Data-intensive computing systems can typically be scaled in a linear fashion to accommodate virtually any amount of data, or to meet time-critical performance requirements by simply adding additional processing nodes. The number of nodes and processing tasks assigned for a specific application can be variable or fixed depending on the hardware, software, communications, and distributed file system architecture. A variety of system architectures have been implemented for data-intensive computing and large-scale data analysis applications including parallel and distributed relational database management systems which have been available to run on shared nothing clusters of processing nodes for more than two decades. However most data growth is with data in unstructured form and new processing paradigms with more flexible data models were needed. Several solutions have emerged including the MapReduce architecture pioneered by (Google)[https://search-engine-optimization-blog.github.io/Google-Custom-Search.md] and now available in an open-source implementation called Hadoop used by Yahoo, Facebook, and others.\\n\\nLexisNexis Risk Solutions also developed and implemented a scalable platform for data-intensive computing which is used by LexisNexis. The MapReduce architecture and programming model pioneered by (Google)[https://search-engine-optimization-blog.github.io/Google-Search.md] is an example of a modern systems architecture designed for data-intensive computing. The MapReduce architecture allows programmers to use a functional programming style to create a map function that processes a key-value pair associated with the input data to generate a set of intermediate key-value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Since the system automatically takes care of details like partitioning the input data, scheduling and executing tasks across a processing cluster, and managing the communications between nodes, programmers with no experience in parallel programming can easily use a large distributed processing environment.\\n\\nThe programming model for MapReduce architecture is a simple abstraction where the computation takes a set of input key-value pairs associated with the input data and produces a set of output key-value pairs. In the Map phase, the input data is partitioned into input splits and assigned to Map tasks associated with processing nodes in the cluster. The Map task typically executes on the same node containing its assigned partition of data in the cluster. These Map tasks perform user-specified computations on each input key-value pair from the partition of input data assigned to the task, and generates a set of intermediate results for each key.\\n\\nThe shuffle and sort phase then takes the intermediate data generated by each Map task, sorts this data with intermediate data from other nodes, divides this data into regions to be processed by the reduce tasks, and distributes this data as needed to nodes where the Reduce tasks will execute. The Reduce tasks perform additional user-specified operations on the intermediate data possibly merging values associated with a key to a smaller set of values to produce the output data. For more complex data processing procedures, multiple MapReduce calls may be linked together in sequence. Apache Hadoop is an open source software project sponsored by The Apache Software Foundation which implements the MapReduce architecture. Hadoop now encompasses multiple subprojects in addition to the base core, MapReduce, and HDFS distributed filesystem. These additional subprojects provide enhanced application processing capabilities to the base Hadoop implementation and currently include Avro, Pig, HBase, ZooKeeper, Hive, and Chukwa.\\n\\nThe Hadoop MapReduce architecture is functionally similar to the Google implementation except that the base programming language for Hadoop is Java instead of C++. The implementation is intended to execute on clusters of commodity processors. Hadoop implements a distributed data processing scheduling and execution environment and framework for MapReduce jobs. Hadoop includes a distributed file system called HDFS which is analogous to GFS in the Google MapReduce implementation.\\n\\nThe Hadoop execution environment supports additional distributed data processing capabilities which are designed to run using the Hadoop MapReduce architecture. These include HBase, a distributed column-oriented database which provides random access read/write capabilities; Hive which is a data warehouse system built on top of Hadoop that provides SQL-like query capabilities for data summarization, ad hoc queries, and analysis of large datasets; and Pig – a high-level data-flow programming language and execution framework for data-intensive computing. Pig was developed at Yahoo! to provide a specific language notation for data analysis applications and to improve programmer productivity and reduce development cycles when using the Hadoop MapReduce environment. Pig programs are automatically translated into sequences of MapReduce programs if needed in the execution environment.\\n\\nPig provides capabilities in the language for loading, storing, filtering, grouping, de-duplication, ordering, sorting, aggregation, and joining operations on the data. HPCC (High-Performance Computing Cluster) was developed and implemented by LexisNexis Risk Solutions. The development of this computing platform began in 1999 and applications were in production by late 2000. The HPCC approach also utilizes commodity clusters of hardware running the Linux operating system.\\n\\nCustom system software and middleware components were developed and layered on the base Linux operating system to provide the execution environment and distributed filesystem support required for data-intensive computing. LexisNexis also implemented a new high-level language for data-intensive computing. The ECL programming language is a high-level, declarative, data-centric, implicitly parallel language that allows the programmer to define what the data processing result should be and the dataflows and transformations that are necessary to achieve the result. The ECL language includes extensive capabilities for data definition, filtering, data management, and data transformation, and provides an extensive set of built-in functions to operate on records in datasets which can include user-defined transformation functions. ECL programs are compiled into optimized C++ source code, which is subsequently compiled into executable code and distributed to the nodes of a processing cluster.\\n\\nTo address both batch and online aspects data-intensive computing applications, HPCC includes two distinct cluster environments, each of which can be optimized independently for its parallel data processing purpose. The Thor platform is a cluster whose purpose is to be a data refinery for processing of massive volumes of raw data for applications such as data cleansing and hygiene, extract, transform, load (ETL), record linking and entity resolution, large-scale ad hoc analysis of data, and creation of keyed data and indexes to support high-performance structured queries and data warehouse applications. A Thor system is similar in its hardware configuration, function, execution environment, filesystem, and capabilities to the Hadoop MapReduce platform, but provides higher performance in equivalent configurations. The Roxie platform provides an online high-performance structured query and analysis system or data warehouse delivering the parallel data access processing requirements of online applications through Web services interfaces supporting thousands of simultaneous queries and users with sub-second response times.\\n\\nA Roxie system is similar in its function and capabilities to Hadoop with HBase and Hive capabilities added, but provides an optimized execution environment and filesystem for high-performance online processing. Both Thor and Roxie systems utilize the same ECL programming language for implementing applications, increasing programmer productivity. • List of important publications in concurrent, parallel, and distributed computing.',\n",
       "    'local_folder': 'site\\\\files\\\\data_science',\n",
       "    'local_filename': 'site\\\\files\\\\data_science\\\\2019-11-13-Dataintensive-Computing.md',\n",
       "    'site_host': 'https://data-science-blog.github.io',\n",
       "    'site_topic': 'Data Science',\n",
       "    'page_slug': 'Dataintensive-Computing.md',\n",
       "    'page_filename': '2019-11-13-Dataintensive-Computing.md',\n",
       "    'page_url': 'https://data-science-blog.github.io/Dataintensive-Computing.md'}],\n",
       "  'site\\\\files\\\\search_engine_optimization': [{'page_topic': 'Archie Search Engine',\n",
       "    'page_title': 'Archie Search Engine | Search Engine Optimization',\n",
       "    'page_content': 'Archie is a tool for indexing FTP archives, allowing people to find specific files. It is considered to be the first Internet (search)[https://search-engine-optimization-blog.github.io/Audio-Search-Engine.md] engine. The original implementation was written in 1990 by Alan Emtage, then a postgraduate student at McGill University in Montreal, and Bill Heelan, who studied at Concordia University in Montreal and worked at McGill University at the same time. The archie service began as a project for students and volunteer staff at the McGill University School of Computer Science in 1987,when Peter Deutsch (systems manager for the School), Emtage, and Heelan were asked to connect the School of Computer Science to the Internet.\\n\\nThe earliest versions of Archie, written by Alan Emtage, simply contacted a list of FTP archives on a regular basis (contacting each roughly once a month, so as not to waste too many resources of the remote servers) and requested a listing. These listings were stored in local files to be searched using the Unix command. The name derives from the word \"archive\" without the v. Alan Emtage has said that contrary to popular belief, there was no association with the Archie Comics and that he despised them. Despite this, other early Internet search technologies such as Jughead and Veronica were named after characters from the comics.\\n\\nAnarchie, one of the earliest graphical ftp clients was named for its ability to perform Archie searches. Archie was developed as a tool for mass discovery and the concept was simple. The developers populated the engine\\'s servers with databases of anonymous FTP host directories. This was used to find specific file titles since the list was plugged in to a searchable database of websites. Bill Heelan and Peter Deutsch wrote a script allowing people to log in and search collected information using the Telnet protocol at the host \"archie.\\n\\nmcgill. ca\". Later, more efficient front- and back-ends were developed, and the system spread from a local tool, to a network-wide resource, and a popular service available from multiple sites around the Internet. The collected (data)[https://data-science-blog.github.io/Big-Data.md] would be exchanged between the neighbouring Archie servers. The servers could be accessed in multiple ways: using a local client (such as archie or xarchie); telnetting to a server directly; sending queries by electronic mail; and later via a World Wide Web interface.\\n\\nAt the zenith of its fame the Archie search engine accounted for 50% of Montreal Internet traffic. In 1992, Emtage along with Peter Deutsch and some financial help of McGill University formed Bunyip Information Systems the world\\'s first company expressly founded for and dedicated to providing Internet information services with a licensed commercial version of the Archie search engine used by millions of people worldwide. Bill Heelan followed them into Bunyip soon after, where he together with Bibi Ali and Sandro Mazzucato was a part of so-called Archie Group. The group significantly updated the archie database and indexed web-pages.\\n\\nWork on the search engine was ceased in the late 1990s. A legacy Archie server is still maintained active for historic purposes in Poland at University of Warsaw\\'s Interdisciplinary Centre for Mathematical and Computational Modelling. • P. Deutsch, A. Emtage, A.\\n\\nMarine, How to Use Anonymous FTP (RFC1635, May 1994).',\n",
       "    'local_folder': 'site\\\\files\\\\search_engine_optimization',\n",
       "    'local_filename': 'site\\\\files\\\\search_engine_optimization\\\\2019-11-13-Archie-Search-Engine.md',\n",
       "    'site_host': 'https://search-engine-optimization-blog.github.io',\n",
       "    'site_topic': 'Search Engine Optimization',\n",
       "    'page_slug': 'Archie-Search-Engine.md',\n",
       "    'page_filename': '2019-11-13-Archie-Search-Engine.md',\n",
       "    'page_url': 'https://search-engine-optimization-blog.github.io/Archie-Search-Engine.md'},\n",
       "   {'page_topic': 'Audio Search Engine',\n",
       "    'page_title': 'Audio Search Engine | Search Engine Optimization',\n",
       "    'page_content': 'An audio search engine is a web-based search engine which crawls the web for audio content. The information can consist of web pages, images, audio files, or another type of document. Various techniques exist for research on these engines. Text entered into a search bar by the user is compared to the search engine\\'s database. Matching results are accompanied by a brief description of the audio file and its characteristics such as sample frequency, bit rate, type of file, length, duration, or coding type. The user is given the option of downloading the resulting files.\\n\\nThe Query by Example (QBE) system is a searching algorithm that uses content-based image retrieval (CBIR). Keywords are generated from the analysed image. These keywords are used to search for audio files in the database. The results of the search are displayed according to the user preferences regarding to the type of file (wav, mp3, aiff…) or other characteristics. In audio search from audio, the user must play the audio of a song either with a music player, by singing or by humming to the computer microphone.\\n\\nSubsequently, a sound pattern, A, is derived from the audio waveform, and a frequency representation is derived from its Fourier Transform. This pattern will be matched with a pattern, B, corresponding to the waveform and transform of sound files found in the database. All those audio files in the database whose patterns are similar to the pattern search will be displayed as search results • Audioburst is an AI-powered audio search platform which was created with the mission of organizing the world\\'s audio content. Using NLP (technology)[https://data-science-blog.github.io/Committee-On-Data-For-Science-And-Technology.md] and proprietary AI, the Audioburst platform analyzes millions of minutes of live and pre-recorded audio content each day. Everything from podcasts to radio streams and other audio sources are tagged and indexed, creating audio segment “bursts” which are then searchable by keyword, context and topic.\\n\\n• Audiosear. ch (defunct) was a company which developed technology for indexing and retrieving transcribed text from audio recordings. Audio content was indexed and searchable. • FluidDATA (defunct) which developed technology for indexing and retrieving transcribed text from audio recordings. FluidDATA provided a search engine for millions of podcasts and actively transcribes and indexed all podcasts published in iTunes.\\n\\n• VoiceBase is a company which develops technology for converting audio content to time-stamped text with a server-side speech-recognition engine. After the speech recognition process, the audio content is then indexed, and searchable. • Matoola is a company which develops technology for searching within audio content by generating transcripts using a speech-recognition engine. The transcripts are then indexed, allowing users to search within the content and jump to the point in the audio they searched for. • Everyzing (formerly Podzinger until May 2007) is a company which develops technology for delivering video content.\\n\\nEveryzing has developed to products which are licensed primarily to large media companies. ezSEO is a white labeled, hosted search engine optimization solution for making audio and video discoverable with major search engines such as (Google)[https://search-engine-optimization-blog.github.io/Google-Custom-Search.md] and Yahoo. ezSEARCH is a universal site search product which combines text, images, audio, and video. Everyzing claims to have spent millions of dollars building speech to text audio search.\\n\\nEveryzing takes the user within the actual content by using speech recognition. This enables online video consumers to jump directly to the point in the video for which they are searching. • Picsearch Audio Search has been licensed to search portals since 2006. Picsearch is a search technology provider who powers image, video and audio search for over 100 major search engines around the world. • SoundHound (previously known as Midomi) is a (software)[https://python-software.github.io/Eric-Software.md] and company (both with the same name) that lets users find results with audio.\\n\\nIts features are both an audio-based artificial intelligence service and services to find songs and details about them by singing, humming or recording them. • Shazam is an app for smartphone or Mac best known for its music identification capabilities. It uses a built-in microphone to gather a brief sample of the audio being played. It creates an acoustic fingerprint based on the sample, and compares it against a central database for a match. If it finds a match, it sends information such as the artist, song title, and album back to the user.\\n\\n• Doreso identifies a song by humming or singing the melody using a microphone; and by direct input of the name of a song or singer. The app gives information about the song title, its singer and allows you to purchase the song. • Munax (defunct) is a company that released their all-content search engine in its first version in 2005. Their PlayAudioVideo multimedia search engine, created in July 2007, was the first true search engine for multimedia, providing search on the web for images, video and audio in the same search engine, and allowing users to preview them on the same page. Munax has since shut down.\\n\\nSearch results are modified, or suspect, due to the large hosted video being given preferential treatment in search results. • Listen Notes is a podcast search engine that indexes the meta-data (e. g. , text description) and some transcripts of all podcasts and all episodes on the Internet. • FluidDATA (defunct) was a podcast search engine that indexes all RSS metadata and audio metadata, and automatically generates searchable transcripts of all episodes available on the Internet. Audio search has evolved slowly through several basic search formats which exist today and all use keywords.\\n\\nThe keywords for each search can be found in the title of the media, any text attached to the media and content linked web pages, also defined by authors and users of video hosted resources. Some search engines can search recorded speech such as podcasts, though this can be difficult if there is background noise. Around 40 phonemes exist in every language with about 400 in all spoken languages. Rather than applying a text search algorithm after speech-to-text processing is completed, some engines use a phonetic search algorithm to find results within the spoken word.\\n\\nOthers work by listening to the entire podcast and creating a text transcription. Applications as Munax, use several independent ranking algorithms processes, that the inverted index together with hundreds of search parameters to produce the final ranking for each document. Also like Shazam that works by analyzing the captured sound and seeking a match based on an acoustic fingerprint in a database of more than 11 million songs. Shazam identifies songs based on an audio fingerprint based on a time-frequency graph called a spectrogram.\\n\\nShazam stores a catalogue of audio fingerprints in a database. The user tags a song for 10 seconds and the application creates an audio fingerprint. Once it creates the fingerprint of the audio, Shazam starts the search for matches in the database. If there is a match, it returns the information to the user; otherwise it returns a \"song not known\" dialogue. Shazam can identify prerecorded music being broadcast from any source, such as a radio, television, cinema or music in a club, provided that the background noise level is not high enough to prevent an acoustic fingerprint being taken, and that the song is present in the software\\'s database.\\n\\n.',\n",
       "    'local_folder': 'site\\\\files\\\\search_engine_optimization',\n",
       "    'local_filename': 'site\\\\files\\\\search_engine_optimization\\\\2019-11-13-Audio-Search-Engine.md',\n",
       "    'site_host': 'https://search-engine-optimization-blog.github.io',\n",
       "    'site_topic': 'Search Engine Optimization',\n",
       "    'page_slug': 'Audio-Search-Engine.md',\n",
       "    'page_filename': '2019-11-13-Audio-Search-Engine.md',\n",
       "    'page_url': 'https://search-engine-optimization-blog.github.io/Audio-Search-Engine.md'},\n",
       "   {'page_topic': 'Barry Schwartz Technologist',\n",
       "    'page_title': 'Barry Schwartz Technologist | Search Engine Optimization',\n",
       "    'page_content': \"Barry Schwartz (born 1980) is a blogger who writes about (search)[https://search-engine-optimization-blog.github.io/Audio-Search-Engine.md] engines and search engine marketing. Schwartz is the founderand currently the editor of Search Engine Roundtable, an online news site covering the search engines and search engine marketing. He also is the CEO & President of RustyBrick, Inc. , a New York-based web development company, and a news editor at Search Engine Land, a search engine news site founded by Danny Sullivan. Previously, Schwartz was a writer for Search Engine Watch. He also moderates online and offline panels at Search Engine Watch, Cre8asite Forums and WebmasterWorld's PubCon.\\n\\nHe has been interviewed by NBC Nightly News and USA Today about Search Marketing,and has been quoted by news organizations reporting on the Internet Marketing space. He is also the organizer of the annual Search Marketing Expo (SMX) in Israel,and is a member of the Internet Marketers – New York. He also currently hosts the Search Pulse podcast on WebmasterRadio. FM.\\n\\nSchwartz is a graduate of Baruch College, City University of New Yorkand lives in Rockland County, New York. He is married and has three daughters and two sons. .\",\n",
       "    'local_folder': 'site\\\\files\\\\search_engine_optimization',\n",
       "    'local_filename': 'site\\\\files\\\\search_engine_optimization\\\\2019-11-13-Barry-Schwartz-Technologist.md',\n",
       "    'site_host': 'https://search-engine-optimization-blog.github.io',\n",
       "    'site_topic': 'Search Engine Optimization',\n",
       "    'page_slug': 'Barry-Schwartz-Technologist.md',\n",
       "    'page_filename': '2019-11-13-Barry-Schwartz-Technologist.md',\n",
       "    'page_url': 'https://search-engine-optimization-blog.github.io/Barry-Schwartz-Technologist.md'},\n",
       "   {'page_topic': 'Clean Url',\n",
       "    'page_title': 'Clean Url | Search Engine Optimization',\n",
       "    'page_content': 'Clean URLs, also sometimes referred to as RESTful URLs, user-friendly URLs, or (search)[https://search-engine-optimization-blog.github.io/Audio-Search-Engine.md] engine-friendly URLs, are Uniform Resource Locators (URLs) intended to improve the usability and accessibility of a website or web service by being immediately and intuitively meaningful to non-expert users. Such URL schemes tend to reflect the conceptual structure of a collection of information and decouple the user interface from a server\\'s internal representation of information. Other reasons for using clean URLs include search engine optimization (SEO),conforming to the representational state transfer (REST) style of (software)[https://python-software.github.io/Eric-Software.md] architecture, and ensuring that individual web resources remain consistently at the same URL. This makes the World Wide Web a more stable and useful system, and allows more durable and reliable bookmarking of web resources.\\n\\nClean URLs also do not contain implementation details of the underlying web application. This carries the benefit of reducing the difficulty of changing the implementation of the resource at a later date. For example, many URLs include the filename of a server-side script, such as , or. If the underlying implementation of a resource is changed, such URLs would need to change along with it. Likewise, when URLs are not \"clean\", if the site database is moved or restructured it has the potential to cause broken links, both internally and from external sites, the latter of which can lead to removal from search engine listings.\\n\\nThe use of clean URLs presents a consistent location for resources to user-agents regardless of internal structure. A further potential benefit to the use of clean URLs is that the concealment of internal server or application information can improve the security of a system. A URL will often comprise a path, script name, and query string. The query string parameters dictate the content to show on the page, and frequently include information opaque or irrelevant to users—such as internal numeric identifiers for values in a database, illegibly-encoded data, session IDs, implementation details, and so on.\\n\\nClean URLs, by contrast, contain only the path of a resource, in a hierarchy that reflects some logical structure that users can easily interpret and manipulate. The implementation of clean URLs involves URL mapping via pattern matching or transparent rewriting techniques. As this usually takes place on the server side, the clean URL is often the only form seen by the user. For search engine optimization purposes, web developers often take this opportunity to include relevant keywords in the URL and remove irrelevant words. Common words that are removed include articles and conjunctions, while descriptive keywords are added to increase user-friendliness and improve search engine rankings. A fragment identifier can be included at the end of a clean URL for references within a page, and need not be user-readable.\\n\\nSome systems define a slug as the part of a URL that identifies a page in human-readable keywords. It is usually the end part of the URL, which can be interpreted as the name of the resource, similar to the basename in a filename or the title of a page. The name is based on the use of the word slug in the news media to indicate a short name given to an article for internal use. Slugs are typically generated automatically from a page title but can also be entered or altered manually, so that while the page title remains designed for display and human readability, its slug may be optimized for brevity or for consumption by search engines.\\n\\nLong page titles may also be truncated to keep the final URL to a reasonable length. Slugs may be entirely lowercase, with accented characters replaced by letters from the English alphabet and whitespace characters replaced by a dash or an underscore to avoid being encoded. Punctuation marks are generally removed, and some also remove short, common words such as conjunctions. For example, the original title This, That, and the Other! An Outré Collection would have a generated slug of. .',\n",
       "    'local_folder': 'site\\\\files\\\\search_engine_optimization',\n",
       "    'local_filename': 'site\\\\files\\\\search_engine_optimization\\\\2019-11-13-Clean-Url.md',\n",
       "    'site_host': 'https://search-engine-optimization-blog.github.io',\n",
       "    'site_topic': 'Search Engine Optimization',\n",
       "    'page_slug': 'Clean-Url.md',\n",
       "    'page_filename': '2019-11-13-Clean-Url.md',\n",
       "    'page_url': 'https://search-engine-optimization-blog.github.io/Clean-Url.md'},\n",
       "   {'page_topic': 'Contextual Advertising',\n",
       "    'page_title': 'Contextual Advertising | Search Engine Optimization',\n",
       "    'page_content': 'Contextual advertising is a form of targeted advertising for advertisements appearing on websites or other media, such as content displayed in mobile browsers. The advertisements themselves are selected and served by automated systems based on the identity of the user and the content displayed. A contextual advertising system scans the text of a website for keywords and returns advertisements to the webpage based on those keywords. The advertisements may be displayed on the webpage or as pop-up ads.\\n\\nFor example, if the user is viewing a website pertaining to sports and that website uses contextual advertising, the user may see advertisements for sports-related companies, such as memorabilia dealers or ticket sellers. Contextual advertising is also used by (search)[https://search-engine-optimization-blog.github.io/Audio-Search-Engine.md] engines to display advertisements on their search results pages based on the keywords in the user\\'s query. Contextual advertising is a form of targeted advertising in which the content of an ad is in direct correlation to the content of the web page the user is viewing. An example of contextual advertising is an ad offering a special price on a flight to Italy appearing on a website concerning travelling in Europe. Contextual advertising is also called \"In-Text\" advertising or \"In-Context\" technology.\\n\\nApart from that when a visitor doesn\\'t click on the ad in a go through time (a minimum time a user must click on the ad) the ad is automatically changed to next relevant ad showing the option below of going back to the previous ad. Contextual ads are less irritable than traditional advertising. That is why it influences user more effectively. It shows user area of interest thus increasing the chance of receiving a response.\\n (Google)[https://search-engine-optimization-blog.github.io/Google-Custom-Search.md] AdSense was the first major contextual advertising network. It works by providing webmasters with JavaScript code that, when inserted into webpages, displays relevant advertisements from the (Google)[https://search-engine-optimization-blog.github.io/Google-Search.md] inventory of advertisers. The relevance is calculated by a separate Googlebot, that indexes the content of a webpage. Recent technology/service providers have emerged with more sophisticated systems that use language-independent proximity pattern matching algorithm to increase matching accuracy. Media. net is the other major contextual ad network competing with Google Adsense.\\n\\nContextual advertising has made a major impact on earnings of many websites. Because the advertisements are more targeted, they are more likely to be clicked, thus generating revenue for the owner of the website (and the server of the advertisement). A large part of Google\\'s earnings are from its share of the contextual advertisements served on the millions of webpages running the AdSense program. Contextual advertising has attracted some controversy through the use of techniques such as third-party hyperlinking, where a third-party installs (software)[https://python-software.github.io/Eric-Software.md] onto a user\\'s computer that interacts with the web browser.\\n\\nKeywords on a webpage are displayed as hyperlinks that lead to advertisers. There are several advertising agencies that help brands understand how contextual advertising options affect their advertising plans. There are three main components to online advertising:• creation—what the advertisement looks like • media planning—where the advertisements are to be run; also known as \"placements\" • media buying—how the advertisements are paid for Contextual advertising replaces the media planning component. Instead of humans choosing placement options, that function is replaced with computers facilitating the placement across thousands of websites. • None Kenny, D.\\n\\n; Marshall, J. (November–December 2000). \"Contextual Marketing: The Real Business of the Internet\". Harvard Business Review.\\n\\nArchived from the original on 2014-10-04. • None Revesencio, Jonha (2015-04-09). \"CRM Retargeting: How It Works and Everything You Need to Know Before Using It - Huffington Post\". huffingtonpost. com.\\n\\n.',\n",
       "    'local_folder': 'site\\\\files\\\\search_engine_optimization',\n",
       "    'local_filename': 'site\\\\files\\\\search_engine_optimization\\\\2019-11-13-Contextual-Advertising.md',\n",
       "    'site_host': 'https://search-engine-optimization-blog.github.io',\n",
       "    'site_topic': 'Search Engine Optimization',\n",
       "    'page_slug': 'Contextual-Advertising.md',\n",
       "    'page_filename': '2019-11-13-Contextual-Advertising.md',\n",
       "    'page_url': 'https://search-engine-optimization-blog.github.io/Contextual-Advertising.md'},\n",
       "   {'page_topic': 'Danny Sullivan Technologist',\n",
       "    'page_title': 'Danny Sullivan Technologist | Search Engine Optimization',\n",
       "    'page_content': 'Danny Sullivan is an American technologist, journalist,and entrepreneur. He was the Chief Content (Officer)[https://data-science-blog.github.io/Chief-Data-Officer.md] at Third Door Media, and co-founded (Search)[https://search-engine-optimization-blog.github.io/Audio-Search-Engine.md] Engine Land, an industry publication that covers news and information about search engines, and search marketing, SEO and SEM topics. Third Door Media also produces Marketing Land, a sister website that covers broader digital marketing topics including social media, display advertising, email marketing, analytics, mobile, and marketing technology. Search Engine Land and Marketing Land are owned by Third Door Media, of which Danny Sullivan was partner and chief content officer. He retired from his position as Chief Content Officer at Third Door Media in June 2017. In October 2017, Sullivan announced he will be joining (Google)[https://search-engine-optimization-blog.github.io/Google-Custom-Search.md] as an adviser at the search division of the company.\\n\\nDanny is Google\\'s public Search Liaison, who helps people better understand search and helps (Google)[https://search-engine-optimization-blog.github.io/Google-Search.md] better hear public feedback. Danny was one of the 50 marketing influencers, according to Entrepreneur, in 2015. Sullivan was born in 1965 and raised in California. He graduated from the University of California, Irvine, and was a reporter for the Los Angeles Times and The Orange County Register.\\n\\nDanny helped found Maximized Online with programmer Ken Spreitzer. Later, he married Lorna Harris, and lived for several years in Chitterne, a small village in England. They have two sons. The family moved to Newport Beach, California. Sullivan popularized the term Search Engine Marketing within an article he published on Search Engine Watchalthough he does not take credit for coining the term.\\n\\nSullivan started Search Engine Watch in June 1997 after he posted research about search engines, called A Webmaster\\'s Guide To Search Engines, in April 1996. Search Engine Watch was a website with tips on how to get good search engine results. Shortly after beginning in November that year, he sold it for an undisclosed amount to MecklerMedia (now Jupitermedia). He stayed on to maintain the site, and be the editor-in-chief. In 2006, it was sold to Incisive Media for $43 million. Search Engine Watch was considered by Matt Cutts of Google as \"must reading\", and Tim Mayer of Yahoo! as the \"most authoritative source on search\".\\n\\nHe also staged the Search Engine Strategies conference six times each year, attracting 1,500 to 6,000 attendees each time. On 29 August 2006, Sullivan announced he would be leaving Search Engine Watch on 30 November 2006. He later came to an agreement with Jupitermedia to continue participating in SES through 2007. Search Engine Land is a news website that covers search engine marketing and search engine optimization. One of the well-known blogs which shares information about keyword research, trends in search marketing (SEM), paid search (advertising)[https://search-engine-optimization-blog.github.io/Contextual-Advertising.md] (PPC) and search engine optimization (SEO) as well as analysis, advice, tips, tactics and how-to guides for search marketing.\\n\\nIt was founded in 2006 by Sullivan after he left Search Engine Watch. Search Engine Land stories have been cited numerous times by other media outlets. .',\n",
       "    'local_folder': 'site\\\\files\\\\search_engine_optimization',\n",
       "    'local_filename': 'site\\\\files\\\\search_engine_optimization\\\\2019-11-13-Danny-Sullivan-Technologist.md',\n",
       "    'site_host': 'https://search-engine-optimization-blog.github.io',\n",
       "    'site_topic': 'Search Engine Optimization',\n",
       "    'page_slug': 'Danny-Sullivan-Technologist.md',\n",
       "    'page_filename': '2019-11-13-Danny-Sullivan-Technologist.md',\n",
       "    'page_url': 'https://search-engine-optimization-blog.github.io/Danny-Sullivan-Technologist.md'},\n",
       "   {'page_topic': 'Dragonfly Search Engine',\n",
       "    'page_title': 'Dragonfly Search Engine | Search Engine Optimization',\n",
       "    'page_content': 'The Dragonfly project was an Internet (search)[https://search-engine-optimization-blog.github.io/Audio-Search-Engine.md] engine prototype created by (Google)[https://search-engine-optimization-blog.github.io/Google-Custom-Search.md] that was designed to be compatible with China\\'s state censorship provisions. The public learned of Dragonfly\\'s existence in August 2018, when The Intercept leaked an internal memo written by a (Google)[https://search-engine-optimization-blog.github.io/Google-Search.md] employee about the project. In December 2018, Dragonfly was reported to have \"effectively been shut down\" after a clash with members of the privacy team within Google. However according to employees, work on Dragonfly was still continuing as of March 2019, with some 100 people still allocated to it. In July 2019, Google announced that work on Dragonfly had been terminated. The Dragonfly search engine was reportedly designed to link users\\' phone numbers to their search queriesand censor websites such as Wikipedia and those that publish information about freedom of speech,human rights, democracy, religion, and other issues considered sensitive by the Chinese government.\\n\\nIt is not designed to notify searchers when the information they want has been censored. On September 21, 2018, The Intercept reported on an internal memo authored by a Google engineer which contained details about the project. According to a transcript of a July 18 meeting published by The Intercept, Google\\'s search engine chief Ben Gomes stated that although the future was \"unpredictable\", he wanted the app to be ready to launch in \"six to nine months\". Google executives stated in 2018 that Dragonfly was \"exploratory\", \"in early stages\" and that Google was \"not close to launching a search product in China\". In a mid-October 2018 presentation, Google CEO Sundar Pichai discussed Dragonfly, stating, \"We don\\'t know whether we could or would do this in China, but we felt it was important for us to explore.\\n\\n\" He praised the prototype, saying it would provide better information to users than do the other search engines currently operating in China. He specifically highlighted Google\\'s ability to provide accurate search results regarding the efficacy of certain medical treatments, alluding to the death of Wei Zexi, a Baidu user who died after receiving an experimental cancer treatment that he had learned of via a promoted result on that search engine. He also emphasized that the scope of the censorship carried out by the Dragonfly prototype is quite limited: if launched, the search engine would return results for 99% of queries by Chinese citizens and leave only 1% unanswered. He acknowledged that, at one point, 100+ people were working on Dragonfly. In late November 2018, an engineer who worked on Dragonfly told The Intercept that Google had shut their privacy and security teams out of the Dragonfly.\\n\\nHowever, a director of security and privacy at Google said she \"saw no sidelining whatsoever. \" Google issued a statement, saying privacy reviews were \"non-negotiable\". Google\\'s relations with China have been fraught since the tech giant\\'s arrival there in 2006. Google\\'s first China-specific platform, Google. cn, was also a self-censored one: like the Dragonfly prototype, it was engineered so as not to return results for topics blacklisted by the Chinese government. Unlike Dragonfly, though, Google.\\n\\ncn was set up to notify searchers when the results they sought had been removed. In response to criticism over Google. cn at the time of its launch, Google asserted that \"while removing search results is inconsistent with our mission, providing no information is more inconsistent with our mission,\" referring to the alternative of not servicing Chinese users at all. Google also downplayed the extent of the new search engine\\'s censorship, reminding users that it also removes search results from its German, French, and U. S. platforms in order to comply with local government regulations in those countries.\\n\\nUltimately, Google. cn received tepid acceptance: some commentators even praised the search engine with the logic that Chinese citizens, through conducting searches and observing which results had been removed, could better their understanding of what it was their government did not want them to see. In January 2010, Google fell victim to Operation Aurora, a sophisticated series of cyberattacks carried out by Chinese hackers who targeted a number of major U. S. corporations, including Yahoo, Adobe, Dow Chemical, and Morgan Stanley. The hackers stole Google source code and gained access to the Gmail accounts of several prominent Chinese human rights activists who were living abroad.\\n\\nIn response to both the attack and what then-Google-CEO Sergey Brin called a \"broader pattern\" of China\\'s surveillance of human rights activists, Google discontinued Google. cn and began rerouting Chinese users to Google. hk, an uncensored (at least on Google\\'s end) search engine based out of Hong Kong. Almost immediately, the Chinese government blocked Chinese users\\' access to certain results produced by that engine.\\n\\nBrin justified Google\\'s sudden policy switch by arguing that operating a search engine in China no longer aligned with Google\\'s goals of advancing internet freedom, as the company had been seeing a daily increase in requests for certain topics or search terms to be censored, rather than the other way around. Google faced widespread criticism for the decision which some commentators called a \"face saving capitulation\": an attempt by Google to take a stand for internet freedom while still preserving their share of the Chinese market. Other critics alleged that Google\\'s shuttering of Google. cn was simply a well-timed business move—made because the company had only a 35% market share after four years in China—that had little to do with either Operation Aurora or Beijing\\'s growing demands for censored content.\\n\\nSince March 2010, when Google stopped servicing China via Google. cn, China\\'s internet user population has increased by 70%. It currently clocks in at 772 million users, but could grow to 1. 4 billion users with time. This means that, for Google, who makes most of its revenue from advertisements run on its search engines, the potential profits of reentering the Chinese search engine market are enormous. Yet, analysts have suggested that if Google does reenter China—either with the Dragonfly prototype or a different search model—it might initially struggle to meet its revenue goals.\\n\\nGoogle\\'s (advertising)[https://search-engine-optimization-blog.github.io/Contextual-Advertising.md] strategy is highly targeted: it involves collecting (data)[https://data-science-blog.github.io/Big-Data.md] on users\\' search histories and using that (data)[https://data-science-blog.github.io/Data.md] to present users with advertisements which are applicable to them. Google has missed out on nearly a decade of data on prospective Chinese users, making that advertising strategy difficult to execute, at least immediately. Additionally, it is not clear that Google\\'s search product would be able to outcompete Chinese search engines like Baidu and Sogou, which have coveted partnerships with the (technology)[https://data-science-blog.github.io/Committee-On-Data-For-Science-And-Technology.md] platforms Windows and WeChat, respectively. Project Dragonfly has been subject to harsh criticism, particularly from Google employees and users. Shortly after publication of The Intercept article leaking details of the project, 1,400 Google employees signed a letter demanding more transparency about Dragonfly, as well as more say in the nature of the work done by Google in general.\\n\\nIn September 2018, Amnesty International released an open letter to Google\\'s management condemning the project as an \"alarming capitulation by Google on human rights\" and calling for its cancellation. At the end of November 2018, a number of Google employees authored a Medium article in support of Amnesty International\\'s letter. They argued that a Dragonfly launch would set a precedent for the implementation of censored Google services in other countries, and expressed concern about Dragonfly\\'s potential to contribute to a program of widespread state surveillance in China. China is rumored to have been developing a \"social credit system\" which assigns each citizen a \"score\" based on their actions, conducted both online and offline. Purchasing alcohol and jaywalking reduce a citizen\\'s score, for example, while purchasing diapers increases it.\\n\\nChinese corporations are required by law to disclose the consumer data they collect to the government, presumably in part so it can be used to calculate these scores. Analysts have theorized that, if Dragonfly becomes a reality, Google could be compelled to do the same. Following the publication of a second The Intercept article about the project, which alleged that Google bypassed standard security and privacy checks of Dragonfly, Google engineer Liz Fong-Jones tweeted a proposal for Google employees worldwide to go on strike. She wrote that the \"red line\" for initiating the strike will be crossed if Google launches Dragonfly without conducting a thorough security and privacy review, or if evidence emerges that members of Google\\'s Security and Privacy team were coerced into approving the project. Fong-Jones has started a preemptive \"strike fund\" intended to support Google employees should they leave their positions, to which Google employees have already donated over $200K.\\n\\nPoliticians have also spoken out. In early October 2018, Mike Pence called for an end to the development of Dragonfly, and said that, if launched, it would strengthen Communist Party censorship and compromise the privacy of Chinese customers. In early December 2018, Senator Mark Warner (D-VA) criticized both Beijing and Google over the project, stating that Dragonfly evidences China\\'s success at \"recruitWestern companies to their information control efforts. \"Chairman of the Joint Chiefs of Staff General Joseph Dunford also criticized Google, stating that \"it’s inexplicable\" that Google continues investing in autocratic communist China, which uses censorship technology to restrain freedoms and crackdown on online speech, and has long (history)[https://python-software.github.io/History-Of-Python.md] of intellectual property and patent theft that hurts U.\\n\\nS. companies, while simultaneously not renewing further research and development collaborations with the Pentagon. He said “I’m not sure that people at Google will enjoy a world order that is informed by the norms and standards of Russia or China. ” He urged Google work directly with the U. S. government instead of making controversial inroads into China.\\n\\nAmid widespread backlash, one contingent of Google employees has expressed its support for the project. In late November 2018, a Google employee submitted an unsigned letter to TechCrunch, an online technology news platform, calling for work on Dragonfly to continue because the project aligns with Google\\'s mission to \"organize the world\\'s information and make it universally accessible and useful. \"The letter states that, although Dragonfly has the power to \"do more harm than good,\" it is valuable in that it can shed light on \"how different approaches may work out in China. \"Three anonymous Google employees from China said they supported the project, citing a need for a competitor to the Chinese search engine Baidu.\\n\\nIn testimony given to the U. S. Senate Judiciary Committee in July 2019, Karan Bhatia, the vice president of public policy at Google, announced that work on Dragonfly had been \"terminated\". .',\n",
       "    'local_folder': 'site\\\\files\\\\search_engine_optimization',\n",
       "    'local_filename': 'site\\\\files\\\\search_engine_optimization\\\\2019-11-13-Dragonfly-Search-Engine.md',\n",
       "    'site_host': 'https://search-engine-optimization-blog.github.io',\n",
       "    'site_topic': 'Search Engine Optimization',\n",
       "    'page_slug': 'Dragonfly-Search-Engine.md',\n",
       "    'page_filename': '2019-11-13-Dragonfly-Search-Engine.md',\n",
       "    'page_url': 'https://search-engine-optimization-blog.github.io/Dragonfly-Search-Engine.md'},\n",
       "   {'page_topic': 'Fulltext Search',\n",
       "    'page_title': 'Fulltext Search | Search Engine Optimization',\n",
       "    'page_content': 'In text retrieval, full-text (search)[https://search-engine-optimization-blog.github.io/Audio-Search-Engine.md] refers to techniques for searching a single computer-stored document or a collection in a full-text database. Full-text search is distinguished from searches based on metadata or on parts of the original texts represented in databases (such as titles, abstracts, selected sections, or bibliographical references). In a full-text search, a search engine examines all of the words in every stored document as it tries to match search criteria (for example, text specified by a user). Full-text-searching techniques became common in online bibliographic databases in the 1990s. Many websites and application programs (such as word processing software) provide full-text-search capabilities.\\n\\nSome web search engines, such as AltaVista, employ full-text-search techniques, while others index only a portion of the web pages examined by their indexing systems. When dealing with a small number of documents, it is possible for the full-text-search engine to directly scan the contents of the documents with each query, a strategy called \"serial scanning\". This is what some tools, such as grep, do when searching. However, when the number of documents to search is potentially large, or the quantity of search queries to perform is substantial, the problem of full-text search is often divided into two tasks: indexing and searching. The indexing stage will scan the text of all the documents and build a list of search terms (often called an index, but more correctly named a concordance).\\n\\nIn the search stage, when performing a specific query, only the index is referenced, rather than the text of the original documents. The indexer will make an entry in the index for each term or word found in a document, and possibly note its relative position within the document. Usually the indexer will ignore stop words (such as \"the\" and \"and\") that are both common and insufficiently meaningful to be useful in searching. Some indexers also employ language-specific stemming on the words being indexed. For example, the words \"drives\", \"drove\", and \"driven\" will be recorded in the index under the single concept word \"drive\".\\n\\nRecall measures the quantity of relevant results returned by a search, while precision is the measure of the quality of the results returned. Recall is the ratio of relevant results returned to all relevant results. Precision is the number of relevant results returned to the total number of results returned. The diagram at right represents a low-precision, low-recall search. In the diagram the red and green dots represent the total population of potential search results for a given search. Red dots represent irrelevant results, and green dots represent relevant results.\\n\\nRelevancy is indicated by the proximity of search results to the center of the inner circle. Of all possible results shown, those that were actually returned by the search are shown on a light-blue background. In the example only 1 relevant result of 3 possible relevant results was returned, so the recall is a very low ratio of 1/3, or 33%. The precision for the example is a very low 1/4, or 25%, since only 1 of the 4 results returned was relevant.\\n\\nDue to the ambiguities of natural language, full-text-search systems typically includes options like stop words to increase precision and stemming to increase recall. Controlled-vocabulary searching also helps alleviate low-precision issues by tagging documents in such a way that ambiguities are eliminated. The trade-off between precision and recall is simple: an increase in precision can lower overall recall, while an increase in recall lowers precision. Full-text searching is likely to retrieve many documents that are not relevant to the intended search question.\\n\\nSuch documents are called false positives (see Type I error). The retrieval of irrelevant documents is often caused by the inherent ambiguity of natural language. In the sample diagram at right, false positives are represented by the irrelevant results (red dots) that were returned by the search (on a light-blue background). Clustering techniques based on Bayesian algorithms can help reduce false positives. For a search term of \"bank\", clustering can be used to categorize the document/data universe into \"financial institution\", \"place to sit\", \"place to store\" etc.\\n\\nDepending on the occurrences of words relevant to the categories, search terms or a search result can be placed in one or more of the categories. This technique is being extensively deployed in the e-discovery domain. The deficiencies of free text searching have been addressed in two ways: By providing users with tools that enable them to express their search questions more precisely, and by developing new search algorithms that improve retrieval precision. • Keywords. Document creators (or trained indexers) are asked to supply a list of words that describe the subject of the text, including synonyms of words that describe this subject.\\n\\nKeywords improve recall, particularly if the keyword list includes a search word that is not in the document text. • Field-restricted search. Some search engines enable users to limit free text searches to a particular field within a stored (data)[https://data-science-blog.github.io/Big-Data.md] record, such as \"Title\" or \"Author. \" • Boolean queries.\\n\\nSearches that use Boolean operators (for example, ) can dramatically increase the precision of a free text search. The operator says, in effect, \"Do not retrieve any document unless it contains both of these terms. \" The operator says, in effect, \"Do not retrieve any document that contains this word. \" If the retrieval list retrieves too few documents, the operator can be used to increase recall; consider, for example, \"encyclopedia\" AND \"online\" OR \"Internet\" NOT \"Encarta\".\\n\\nThis search will retrieve documents about online encyclopedias that use the term \"Internet\" instead of \"online. \" This increase in precision is very commonly counter-productive since it usually comes with a dramatic loss of recall. • Phrase search. A phrase search matches only those documents that contain a specified phrase, such as • Concept search. A search that is based on multi-word concepts, for example Compound term processing.\\n\\nThis type of search is becoming popular in many e-discovery solutions. • Concordance search. A concordance search produces an alphabetical list of all principal words that occur in a text with their immediate context. • Proximity search. A phrase search matches only those documents that contain two or more words that are separated by a specified number of words; a search for would retrieve only those documents in which the words occur within two words of each other.\\n\\n• Regular expression. A regular expression employs a complex but powerful querying syntax that can be used to specify retrieval conditions with precision. • Fuzzy search will search for document that match the given terms and some variation around them (using for instance edit distance to threshold the multiple variation) • Wildcard search. A search that substitutes one or more characters in a search query for a wildcard character such as an asterisk. For example, using the asterisk in a search query will find \"sin\", \"son\", \"sun\", etc. in a text.\\n\\nThe PageRank algorithm developed by (Google)[https://search-engine-optimization-blog.github.io/Google-Custom-Search.md] gives more prominence to documents to which other Web pages have linked. See Search engine for additional examples. The following is a partial list of available (software)[https://python-software.github.io/Eric-Software.md] products whose predominant purpose is to perform full-text indexing and searching. Some of these are accompanied with detailed descriptions of their theory of operation or internal algorithms, which can provide additional insight into how full-text search may be accomplished.\\n\\n.',\n",
       "    'local_folder': 'site\\\\files\\\\search_engine_optimization',\n",
       "    'local_filename': 'site\\\\files\\\\search_engine_optimization\\\\2019-11-13-Fulltext-Search.md',\n",
       "    'site_host': 'https://search-engine-optimization-blog.github.io',\n",
       "    'site_topic': 'Search Engine Optimization',\n",
       "    'page_slug': 'Fulltext-Search.md',\n",
       "    'page_filename': '2019-11-13-Fulltext-Search.md',\n",
       "    'page_url': 'https://search-engine-optimization-blog.github.io/Fulltext-Search.md'},\n",
       "   {'page_topic': 'Google Custom Search',\n",
       "    'page_title': 'Google Custom Search | Search Engine Optimization',\n",
       "    'page_content': \"Google Custom (Search)[https://search-engine-optimization-blog.github.io/Audio-Search-Engine.md] (formerly known as (Google)[https://search-engine-optimization-blog.github.io/Google-Search.md] Co-op) is a platform provided by Google that allows web developers to feature specialized information in web searches, refine and categorize queries and create customized search engines, based on Google Search. The service allows users to narrow the 11. 5 billion indexed webpages down to a topical group of pages relevant to the creator's needs. Google launched the service on October 23, 2006. The Google Custom Search platform consists of three services: Released on October 23, 2006, Google Custom Search allows anyone to create their own search engine by themselves. Search engines can be created to search for information on particular topics chosen by the creator.\\n\\nGoogle Custom Search Engine allows creators to select what websites will be used to search for information which helps to eliminate any unwanted websites or information. Creators can also attach their custom search engine to any blog or webpage. Google AdSense results can also be triggered from certain search queries, which would generate revenue for the site owner. Provided as part of the original service, subscribed links were discontinued on 15 September 2011. Subscribed Links were web results that users could manually subscribe to.\\n\\nAnyone was allowed to make a new Subscribed Link, and did not necessarily need knowledge on how to create a feed, as a basic link could be created. Subscriptions were then available in a special directory. Topics are specific areas of search, which can be developed by people with knowledge of a certain subject. These topics are then displayed at the top of relevant Google web searches, so the user can refine the searches to what they want.\\n\\nCurrently, there is a limited number of topics that Google is wanting to develop, namely Health, Destination Guides, Autos, Computer games, Photography and Home Theater. One of the topics with many contributions is Health. They include the National Library of Medicine, Centers for Disease Control and Prevention, Health On the Net Foundation, Harvard Medical School, Mayo Clinic and others. Google CSE's may offer better topical search results than the standard Google search. .\",\n",
       "    'local_folder': 'site\\\\files\\\\search_engine_optimization',\n",
       "    'local_filename': 'site\\\\files\\\\search_engine_optimization\\\\2019-11-13-Google-Custom-Search.md',\n",
       "    'site_host': 'https://search-engine-optimization-blog.github.io',\n",
       "    'site_topic': 'Search Engine Optimization',\n",
       "    'page_slug': 'Google-Custom-Search.md',\n",
       "    'page_filename': '2019-11-13-Google-Custom-Search.md',\n",
       "    'page_url': 'https://search-engine-optimization-blog.github.io/Google-Custom-Search.md'},\n",
       "   {'page_topic': 'Google Search',\n",
       "    'page_title': 'Google Search | Search Engine Optimization',\n",
       "    'page_content': 'Google Search, also referred to as (Google)[https://search-engine-optimization-blog.github.io/Google-Custom-Search.md] Web (Search)[https://search-engine-optimization-blog.github.io/Audio-Search-Engine.md] or simply Google, is a web search engine developed by Google. It is the most used search engine on the World Wide Web across all platforms, with 92. 62% market share as of June 2019,handling more than 5. 4 billion searches each day.\\n\\nThe order of search results returned by Google is based, in part, on a priority rank system called \"PageRank\". Google Search also provides many different options for customized search, using symbols to include, exclude, specify or require certain search behavior, and offers specialized interactive experiences, such as flight status and package tracking, weather forecasts, currency, unit and time conversions, word definitions, and more. The main purpose of Google Search is to hunt for text in publicly accessible documents offered by web servers, as opposed to other data, such as images or (data)[https://data-science-blog.github.io/Big-Data.md] contained in databases. It was originally developed in 1997 by Larry Page, Sergey Brin, and Scott Hassan. In June 2011, Google introduced \"Google Voice Search\" to search for spoken, rather than typed, words.\\n\\nIn May 2012, Google introduced a Knowledge Graph semantic search feature in the U. S. Analysis of the frequency of search terms may indicate economic, social and health trends. (Data)[https://data-science-blog.github.io/Data.md] about the frequency of use of search terms on Google can be openly inquired via Google Trends and have been shown to correlate with flu outbreaks and unemployment levels, and provide the information faster than traditional reporting methods and surveys.\\n\\nAs of mid-2016, Google\\'s search engine has begun to rely on deep neural networks. Competitors of Google include Baidu and Soso. com in China; Naver. com and Daum.\\n\\nnet in South Korea; Yandex in Russia; Seznam. cz in the Czech Republic; Qwant in France; Yahoo in Japan, Taiwan and the US, as well as Bing and DuckDuckGo. Some smaller search engines offer facilities not available with Google, e. g.\\n\\nnot storing any private or tracking information. Within the U. S. , as of July 2018, Bing handled 24. 2 percent of all search queries.\\n\\nDuring the same period of time, Oath (formerly known as Yahoo) had a search market share of 11. 5 percent. Market leader Google generated 63. 2 percent of all core search queries in the U. S.\\n\\nGoogle indexes hundreds of terabytes of information from web pages. For websites that are currently down or otherwise not available, Google provides links to cached versions of the site, formed by the search engine\\'s latest indexing of that page. Additionally, Google indexes some file types, being able to show users PDFs, Word documents, Excel spreadsheets, PowerPoint presentations, certain Flash multimedia content, and plain text files. Users can also activate \"SafeSearch\", a filtering (technology)[https://data-science-blog.github.io/Committee-On-Data-For-Science-And-Technology.md] aimed at preventing explicit and pornographic content from appearing in search results.\\n\\nDespite Google search\\'s immense index, sources generally assume that Google is only indexing less than 5% of the total Internet, with the rest belonging to the deep web, inaccessible through its search tools. In 2012, Google changed its search indexing tools to demote sites that had been accused of piracy. In October 2016, Gary Illyes, a webmaster trends analyst with Google, announced that the search engine would be making a separate, primary web index dedicated for mobile devices, with a secondary, less up-to-date index for desktop use. The change was a response to the continued growth in mobile usage, and a push for web developers to adopt a mobile-friendly version of their websites. In December 2017, Google began rolling out the change, having already done so for multiple websites. In August 2009, Google invited web developers to test a new search architecture, codenamed \"Caffeine\", and give their feedback.\\n\\nThe new architecture provided no visual differences in the user interface, but added significant speed improvements and a new \"under-the-hood\" indexing infrastructure. The move was interpreted in some quarters as a response to Microsoft\\'s recent release of an upgraded version of its own search service, renamed Bing, as well as the launch of Wolfram Alpha, a new search engine based on \"computational knowledge\". Google announced completion of \"Caffeine\" on June 8, 2010, claiming 50% fresher results due to continuous updating of its index. With \"Caffeine\", Google moved its back-end indexing system away from MapReduce and onto Bigtable, the company\\'s distributed database platform. In August 2018, Danny Sullivan from Google announced a broad core algorithm update.\\n\\nAs per current analysis done by the industry leaders Search Engine Watch and Search Engine Land, the update was to drop down the medical and health related websites that were not user friendly and were not providing good user experience. This is why the industry experts named it \"Medic\". Google reserves very high standards for YMYL (Your Money or Your Life) pages. This is because misinformation can affect users financially, physically or emotionally. Therefore, the update targeted particularly those YMYL pages that have low-quality content and misinformation.\\n\\nThis resulted in the algorithm targeting health and medical related websites more than others. However, many other websites from other industries were also negatively affected. Google Search consists of a series of localized websites. The largest of those, the google. com site, is the top most-visited website in the world.\\n\\nSome of its features include a definition link for most searches including dictionary words, the number of results you got on your search, links to other searches (e. g. for words that Google believes to be misspelled, it provides a link to the search results using its proposed spelling), and many more. Google search accepts queries as normal text, as well as individual keywords. It automatically corrects misspelled words, and yields the same results regardless of capitalization.\\n\\nFor more customized results, one can use a wide variety of operators, including, but not limited to:• – Search for webpages containing one of two similar queries, such as marathon OR race • (minus sign) – Exclude a word or a phrase, such as \"apple -tree\" searches where word \"tree\" is not used • – Force inclusion of a word or a phrase, such as \"tallest building\" • – Placeholder symbol allowing for any substitute words in the context of the query, such as \"largest * in the world\" • – Search within a range of numbers, such as \"camera $50. . $100\" • – Search within a specific website, such as \"site:youtube. com\" • – See a definition of a word, such as \"define:phrase\" • – See the stock price of investments, such as \"stocks:googl\" • – Find webpages related to specific URL addresses, such as \"related:www. wikipedia.\\n\\norg\" • – Highlights the search-words within the cached pages, such as \"cache:www. google. com xxx\" shows cached content with word \"xxx\" highlighted. • – Search for a specific word on social media networks, such as \"@twitter\" Google applies query expansion to submitted search queries, using techniques to deliver results that it considers \"smarter\" than the query users actually submitted. This technique involves several steps, including:• Word stemming – Certain words can be reduced so other, similar terms, are also found in results, such as \"translator\" can also search for \"translation\" • Acronyms – Searching for abbreviations can also return results about the name in its full length, such as \"NATO\" can show results for \"North Atlantic Treaty Organization\" • Misspellings – Google will often suggest correct spellings for misspelled words • Synonyms – In most cases where a word is incorrectly used in a phrase or sentence, Google search will show results based on the correct synonym • Translations – The search engine can, in some instances, suggest results for specific words in a different language • Ignoring words – In some search queries containing extraneous or insignificant words, Google search will simply drop those specific words from the query In 2008, Google started to give users autocompleted search suggestions in a list below the search bar while typing.\\n\\nGoogle\\'s homepage includes a button labeled \"I\\'m Feeling Lucky\". This feature originally allowed users to type in their search query, click the button and be taken directly to the first result, bypassing the search results page. With the 2010 announcement of Google Instant, an automatic feature that immediately displays relevant results as users are typing in their query, the \"I\\'m Feeling Lucky\" button disappears, requiring that users opt-out of Instant results through search settings in order to keep using the \"I\\'m Feeling Lucky\" functionality. In 2012, \"I\\'m Feeling Lucky\" was changed to serve as an advertisement for Google services; users hover their computer mouse over the button, it spins and shows an emotion (\"I\\'m Feeling Puzzled\" or \"I\\'m Feeling Trendy\", for instance), and, when clicked, takes users to a Google service related to that emotion.\\n\\nTom Chavez of \"Rapt\", a firm helping to determine a website\\'s (advertising)[https://search-engine-optimization-blog.github.io/Contextual-Advertising.md] worth, estimated in 2007 that Google lost $110 million in revenue per year due to use of the button, which bypasses the advertisements found on the search results page. Besides the main text-based search-engine features of Google search, it also offers multiple quick, interactive experiences. These include, but are not limited to:During Google\\'s developer conference, Google I/O, in May 2013, the company announced that, on Google Chrome and Chrome OS, users would be able to say \"OK Google\", with the browser initiating an audio-based search, with no button presses required. After having the answer presented, users can follow up with additional, contextual questions; an example include initially asking \"OK Google, will it be sunny in Santa Cruz this weekend?\", hearing a spoken answer, and reply with \"how far is it from here?\"An update to the Chrome browser with voice-search functionality rolled out a week later, though it required a button press on a microphone icon rather than \"OK Google\" voice activation. Google released a browser extension for the Chrome browser, named with a \"beta\" tag for unfinished development, shortly thereafter. In May 2014, the company officially added \"OK Google\" into the browser itself;they removed it in October 2015, citing low usage, though the microphone icon for activation remained available.\\n\\nIn May 2016, 20% of search queries on mobile devices were done through voice. \"Universal search\" was launched by Google on May 16, 2007 as an idea that merged the results from different kinds of search types into one. Prior to Universal search, a standard Google search would consist of links only to websites. Universal search, however, incorporates a wide variety of sources, including websites, news, pictures, maps, blogs, videos, and more, all shown on the same search results page. Marissa Mayer, then-vice president of search products and user experience, described the goal of Universal search as \"we\\'re attempting to break down the walls that traditionally separated our various search properties and integrate the vast amounts of information available into one simple set of search results. In June 2017, Google expanded its search results to cover available job listings.\\n\\nThe data is aggregated from various major job boards and collected by analyzing company homepages. Initially only available in English, the feature aims to simplify finding jobs suitable for each user. In May 2009, Google announced that they would be parsing website microformats in order to populate search result pages with \"Rich snippets\". Such snippets include additional details about results, such as displaying reviews for restaurants and social media accounts for individuals.\\n\\nIn May 2016, Google expanded on the \"Rich snippets\" format to offer \"Rich cards\", which, similarly to snippets, display more information about results, but shows them at the top of the mobile website in a swipeable carousel-like format. Originally limited to movie and recipe websites in the United States only, the feature expanded to all countries globally in 2017. Now the web publishers can have greater control over the rich snippets. Preview settings from these meta tags will become effective in mid-to-late October 2019 and may take about a week for the global rollout to complete. The Knowledge Graph is a knowledge base used by Google to enhance its search engine\\'s results with information gathered from a variety of sources. This information is presented to users in a box to the right of search results.\\n\\nKnowledge Graph boxes were added to Google\\'s search engine in May 2012,starting in the United States, with international expansion by the end of the year. The information covered by the Knowledge Graph grew significantly after launch, tripling its original size within seven months,and being able to answer \"roughly one-third\" of the 100 billion monthly searches Google processed in May 2016. The information is often used as a spoken answer in Google Assistantand Google Home searches. The Knowledge Graph has been criticized for providing answers without source attribution. In May 2017, Google enabled a new \"Personal\" tab in Google Search, letting users search for content in their Google accounts\\' various services, including email messages from Gmail and photos from Google Photos.\\n\\nThe Google feed is a personalized stream of articles, videos, and other news-related content. The feed contains a \"mix of cards\" which show topics of interest based on users\\' interactions with Google, or topics they choose to follow directly. Cards include, \"links to news stories, YouTube videos, sports scores, recipes, and other content based on whatdetermined you\\'re most likely to be interested in at that particular moment. \"Users can also tell Google they\\'re not interested in certain topics to avoid seeing future updates.\\n\\nThe Google feed launched in December 2016and received a major update in July 2017. As of May 2018, the Google feed can be found on the Google app and by swiping left on the home screen of certain Android devices. Google\\'s rise was largely due to a patented algorithm called PageRank which helps rank web pages that match a given search string. When Google was a Stanford research project, it was nicknamed BackRub because the technology checks backlinks to determine a site\\'s importance.\\n\\nOther keyword-based methods to rank search results, used by many search engines that were once more popular than Google, would check how often the search terms occurred in a page, or how strongly associated the search terms were within each resulting page. The PageRank algorithm instead analyzes human-generated links assuming that web pages linked from many important pages are also important. The algorithm computes a recursive score for pages, based on the weighted sum of other pages linking to them. PageRank is thought to correlate well with human concepts of importance. In addition to PageRank, Google, over the years, has added many other secret criteria for determining the ranking of resulting pages. This is reported to comprise over 250 different indicators,the specifics of which are kept secret to avoid difficulties created by scammers and help Google maintain an edge over its competitors globally.\\n\\nPageRank was influenced by a similar page-ranking and site-scoring algorithm earlier used for RankDex, developed by Robin Li in 1996. Larry Page\\'s patent for PageRank filed in 1998 includes a citation to Li\\'s earlier patent. Li later went on to create the Chinese search engine Baidu in 2000. In a potential hint of Google\\'s future direction of their Search algorithm, Google\\'s then chief executive Eric Schmidt, said in a 2007 interview with the Financial Times: \"The goal is to enable Google users to be able to ask the question such as \\'What shall I do tomorrow?\\' and \\'What job shall I take?\\'\". Schmidt reaffirmed this during a 2010 interview with the Wall Street Journal: \"I actually think most people don\\'t want Google to answer their questions, they want Google to tell them what they should be doing next.\\n\\n\"In 2013 the European Commission found that Google Search favored Google\\'s own products, instead of the best result for consumers\\' needs. In February 2015 Google announced a major change to its mobile search algorithm which would favor mobile friendly over other websites. Nearly 60% of Google searches come from mobile phones. Google says it wants users to have access to premium quality websites. Those websites which lack a mobile friendly interface would be ranked lower and it is expected that this update will cause a shake-up of ranks.\\n\\nBusinesses who fail to update their websites accordingly could see a dip in their regular websites traffic. Because Google is the most popular search engine, many webmasters attempt to influence their website\\'s Google rankings. An industry of consultants has arisen to help websites increase their rankings on Google and on other search engines. This field, called search engine optimization, attempts to discern patterns in search engine listings, and then develop a methodology for improving rankings to draw more searchers to their clients\\' sites. Search engine optimization encompasses both \"on page\" factors (like body copy, title elements, H1 heading elements and image alt attribute values) and Off Page Optimization factors (like anchor text and PageRank). The general idea is to affect Google\\'s relevance algorithm by incorporating the keywords being targeted in various places \"on page\", in particular the title element and the body copy (note: the higher up in the page, presumably the better its keyword prominence and thus the ranking).\\n\\nToo many occurrences of the keyword, however, cause the page to look suspect to Google\\'s spam checking algorithms. Google has published guidelines for website owners who would like to raise their rankings when using legitimate optimization consultants. It has been hypothesized, and, allegedly, is the opinion of the owner of one business about which there have been numerous complaints, that negative publicity, for example, numerous consumer complaints, may serve as well to elevate page rank on Google Search as favorable comments. The particular problem addressed in The New York Times article, which involved DecorMyEyes, was addressed shortly thereafter by an undisclosed fix in the Google algorithm. According to Google, it was not the frequently published consumer complaints about DecorMyEyes which resulted in the high ranking but mentions on news websites of events which affected the firm such as legal actions against it. Google Search Console helps to check for websites that use duplicate or copyright content.\\n\\nIn 2013, Google significantly upgraded its search algorithm with \"Hummingbird\". Its name was derived from the speed and accuracy of the hummingbird. The change was announced on September 26, 2013, having already been in use for a month. \"Hummingbird\" places greater emphasis on natural language queries, considering context and meaning over individual keywords. It also looks deeper at content on individual pages of a website, with improved ability to lead users directly to the most appropriate page rather than just a website\\'s homepage.\\n\\nThe upgrade marked the most significant change to Google search in years, with more \"human\" search interactionsand a much heavier focus on conversation and meaning. Thus, web developers and writers were encouraged to optimize their sites with natural writing rather than forced keywords, and make effective use of technical web development for on-site navigation. On certain occasions, the logo on Google\\'s webpage will change to a special version, known as a \"Google Doodle\". This is a picture, drawing, animation or interactive game that includes the logo. It is usually done for a special event or day although not all of them are well known. Clicking on the Doodle links to a string of Google search results about the topic.\\n\\nThe first was a reference to the Burning Man Festival in 1998,and others have been produced for the birthdays of notable people like Albert Einstein, historical events like the interlocking Lego block\\'s 50th anniversary and holidays like Valentine\\'s Day. Some Google Doodles have interactivity beyond a simple search, such as the famous \"Google Pacman\" version that appeared on May 21, 2010. Google offers a \"Google Search\" mobile app for Android and iOS devices. The mobile apps exclusively feature a \"feed\", a news feed-style page of continually-updated developments on news and topics of interest to individual users.\\n\\nAndroid devices were introduced to a preview of the feed in December 2016,while it was made official on both Android and iOS in July 2017. In April 2016, Google updated its Search app on Android to feature \"Trends\"; search queries gaining popularity appeared in the autocomplete box along with normal query autocompletion. The update received significant backlash, due to encouraging search queries unrelated to users\\' interests or intentions, prompting the company to issue an update with an opt-out option. In September 2017, the Google Search app on iOS was updated to feature the same functionality.\\n\\nUntil May 2013, Google Search had offered a feature to translate search queries into other languages. A Google spokesperson told Search Engine Land that \"Removing features is always tough, but we do think very hard about each decision and its implications for our users. Unfortunately, this feature never saw much pick up\". Instant search was announced in September 2010 as a feature that displayed suggested results while the user typed in their search query. The primary advantage of the new system was its ability to save time, with Marissa Mayer, then-vice president of search products and user experience, proclaiming that the feature would save 2–5 seconds per search, elaborating that \"That may not seem like a lot at first, but it adds up.\\n\\nWith Google Instant, we estimate that we\\'ll save our users 11 hours with each passing second!\"Matt Van Wagner of Search Engine Land wrote that \"Personally, I kind of like Google Instant and I think it represents a natural evolution in the way search works\", and also praised Google\\'s efforts in public relations, writing that \"With just a press conference and a few well-placed interviews, Google has parlayed this relatively minor speed improvement into an attention-grabbing front-page news story\". The upgrade also became notable for the company switching Google Search\\'s underlying technology from HTML to AJAX. Instant Search could be disabled via Google\\'s \"preferences\" menu for those who didn\\'t want its functionality. The publication 2600: The Hacker Quarterly compiled a list of words that Google Instant did not show suggested results for, with a Google spokesperson giving the following statement to Mashable:PC Magazine discussed the inconsistency in how some forms of the same topic are allowed; for instance, \"lesbian\" was blocked, while \"gay\" was not, and \"cocaine\" was blocked, while \"crack\" and \"heroin\" were not. The report further stated that seemingly normal words were also blocked due to pornographic innuendos, most notably \"scat\", likely due to having two completely separate contextual meanings, one for music and one for a sexual practice.\\n\\nIn July 2017, Google removed Instant results, due to a growing number of searches on mobile devices, where interaction with search, as well as screen sizes, differ significantly from a computer. Various search engines provide encrypted Web search facilities. In May 2010 Google rolled out SSL-encrypted web search. The encrypted search was accessed atHowever, the web search is encrypted via Transport Layer Security (TLS) by default today, thus every search request should be automatically encrypted if TLS is supported by the web browser. On its support website, Google announced that the address would be turned off April 30, 2018, stating that all Google products and most new browsers use HTTPS connections as the reason for the discontinuation.\\n\\nGoogle Real-Time Search was a feature of Google Search in which search results also sometimes included real-time information from sources such as Twitter, Facebook, blogs, and news websites. The feature was introduced on December 7, 2009and went offline on July 2, 2011 after the deal with Twitter expired. Real-Time Search included Facebook status updates beginning on February 24, 2010. A feature similar to Real-Time Search was already available on Microsoft\\'s Bing search engine, which showed results from Twitter and Facebook. The interface for the engine showed a live, descending \"river\" of posts in the main region (which could be paused or resumed), while a bar chart metric of the frequency of posts containing a certain search term or hashtag was located on the right hand corner of the page above a list of most frequently reposted posts and outgoing links.\\n\\nHashtag search links were also supported, as were \"promoted\" tweets hosted by Twitter (located persistently on top of the river) and thumbnails of retweeted image or video links. In January 2011, geolocation links of posts were made available alongside results in Real-Time Search. In addition, posts containing syndicated or attached shortened links were made searchable by the link: query option. In July 2011 Real-Time Search became inaccessible, with the Real-Time link in the Google sidebar disappearing and a custom 404 error page generated by Google returned at its former URL. Google originally suggested that the interruption was temporary and related to the launch of Google+;they subsequently announced that it was due to the expiry of a commercial arrangement with Twitter to provide access to tweets. Searches made by search engines, including Google, leave traces.\\n\\nThis raises concerns about privacy. In principle, if details of a user\\'s searches are found, those with access to the information—principally state agencies responsible for law enforcement and similar matters—can make deductions about the user\\'s activities. This has been used for the detection and prosecution of lawbreakers; for example a murderer was found and convicted after searching for terms such as \"tips with killing with a baseball bat\". A search may leave traces both on a computer used to make the search, and in records kept by the search provider. When using a search engine through a browser program on a computer, search terms and other information may be stored on the computer by default, unless the browser is set not to do this, or they are erased.\\n\\nSaved terms may be discovered on forensic analysis of the computer. An Internet Service Provider (ISP) or search engine provider (e. g. , Google) may store records which relate search terms to an IP address and a time. Whether such logs are kept, and access to them by law enforcement agencies, is subject to legislation in different jurisdictions and working practices; the law may mandate, prohibit, or say nothing about logging of various types of information.\\n\\nSome search engines, located in jurisdictions where it is not illegal, make a feature of not storing user search information. The keywords suggested by the Autocomplete feature show a population of users\\' research which is made possible by an identity management system. Volumes of personal data are collected via Eddystone web and proximity beacons. Google has been criticized for placing long-term cookies on users\\' machines to store these preferences, a tactic which also enables them to track a user\\'s search terms and retain the data for more than a year. Since 2012, Google Inc.\\n\\nhas globally introduced encrypted connections for most of its clients, in order to bypass governative blockings of the commercial and IT services. In late June 2011, Google introduced a new look to the Google home page in order to boost the use of the Google+ social tools. One of the major changes was replacing the classic navigation bar with a (black)[https://data-science-blog.github.io/Black-Swan-Data.md] one. Google\\'s digital creative director Chris Wiggins explains: \"We\\'re working on a project to bring you a new and improved Google experience, and over the next few months, you\\'ll continue to see more updates to our look and feel.\\n\\n\"The new navigation bar has been negatively received by a vocal minority. In November 2013, Google started testing yellow labels for advertisements displayed in search results, to improve user experience. The new labels, highlighted in yellow color, and aligned to the left of each sponsored link help users clearly differentiate between organic and sponsored results. On December 15, 2016, Google rolled out a new desktop search interface that mimics their modular mobile user interface. The mobile design consists of a tabular design that highlights search features in boxes.\\n\\nand works by imitating the desktop Knowledge Graph real estate, which appears in the right-hand rail of the search engine result page, these featured elements frequently feature Twitter carousels, People Also Search For, and Top Stories (vertical and horizontal design) modules. The Local Pack and Answer Box were two of the original features of the Google SERP that were primarily showcased in this manner, but this new layout creates a previously unseen level of design (consistency)[https://data-science-blog.github.io/Consistency-Database-Systems.md] for Google results. In addition to its tool for searching web pages, Google also provides services for searching images, Usenet newsgroups, news websites, videos, searching by locality, maps, and items for sale online. In 2012, Google has indexed over 30 trillion web pages, and received 100 billion queries per month. It also caches much of the content that it indexes.\\n\\nGoogle operates other tools and services including Google News, Google Shopping, Google Maps, Google Custom Search, Google Earth, Google Docs, Picasa, Panoramio, YouTube, Google Translate, Google Blog Search and Google Desktop Search. There are also products available from Google that are not directly search-related. Gmail, for example, is a webmail application, but still includes search features; Google Browser Sync does not offer any search facilities, although it aims to organize your browsing time. Also Google starts many new beta products, like Google Social Search or Google Image Swirl.\\n\\nIn 2009, Google claimed that a search query requires altogether about 1 kJ or 0. 0003 kW·h,which is enough to raise the temperature of one liter of water by 0. 24 °C. According to green search engine Ecosia, the industry standard for search engines is estimated to be about 0.\\n\\n2 grams of CO emission per search. Google\\'s 40,000 searches per second translate to 8 kg CO per second or over 252 million kilos of CO per year. In 2003, The New York Times complained about Google\\'s indexing, claiming that Google\\'s caching of content on its site infringed its copyright for the content. In both Field v. Google and Parker v. Google, the United States District Court of Nevada ruled in favor of Google.\\n\\nGoogle flags search results with the message \"This site may harm your computer\" if the site is known to install malicious (software)[https://python-software.github.io/Eric-Software.md] in the background or otherwise surreptitiously. Google does this to protect users against visiting sites that could harm their computers. For approximately 40 minutes on January 31, 2009, all search results were mistakenly classified as malware and could therefore not be clicked; instead a warning message was displayed and the user was required to enter the requested URL manually. The bug was caused by human error. The URL of \"/\" (which expands to all URLs) was mistakenly added to the malware patterns file.\\n\\nIn 2007, a group of researchers observed a tendency for users to rely on Google Search exclusively for finding information, writing that \"With the Google interface the user gets the impression that the search results imply a kind of totality. . . .\\n\\nIn fact, one only sees a small part of what one could see if one also integrates other research tools. \"In 2011, Google Search query results have been shown to be tailored to users by Internet activist Eli Pariser, effectively isolating users in what he defined as a filter bubble. Pariser holds algorithms used in search engines such as Google Search responsible for catering \"a personal ecosystem of information\". Although contrasting views have mitigated the potential threat of \"informational dystopia\" and questioned the scientific nature of Pariser\\'s claims,filter bubbles have been mentioned to account for the surprising results of the U.\\n\\nS. presidential election in 2016 alongside fake news and echo chambers, suggesting that Facebook and Google have designed personalized online realities in which \"we only see and hear what we like\". In 2012, the US Federal Trade Commission fined Google US$22. 5 million for violating their agreement not to violate the privacy of users of Apple\\'s Safari web browser.\\n\\nThe FTC was also continuing to investigate if Google\\'s favoring of their own services in their search results violated antitrust regulations. Google search engine robots are programmed to use algorithms that understand and predict human behavior. The book, Race After Technology: Abolitionist Tools for the New Jim Codeby Ruha Benjamin talks about human bias as a behavior that the google search engine can recognize. In 2016, some users google searched “three Black teenagers” and images of criminal mugshots of young African American teenagers came up.\\n\\nThen, the users searched “three White teenagers” and were presented with photos of smiling, happy teenagers. They also searched for “three Asian teenagers,” and very revealing photos of asian girls and women appeared. Benjamin came to the conclusion that these results reflect human prejudice and views on different ethnic groups. A group of analysts explained the concept of a racist computer program: “The idea here is that computers, unlike people, can’t be racist but we’re increasingly learning that they do in fact take after their makers. .\\n\\n. Some experts believe that this problem might stem from the hidden biases in the massive piles of data that the algorithms process as they learn to recognize patterns. . . reproducing our worst values”.\\n\\nAs people talk about \"googling\" rather than searching, the company has taken some steps to defend its trademark, in an effort to prevent if from becoming a generic trademark. This has led to lawsuits, threats of lawsuits, and the use of euphemisms, such as calling Google Search a famous web search engine. • Google Hacks from O\\'Reilly is a book containing tips about using Google effectively. Now in its third edition (2006). ISBN 0-596-52706-3.\\n\\n• Google: The Missing Manual by Sarah Milstein and Rael Dornfest (O\\'Reilly, 2004). ISBN 0-596-00613-6 • How to Do Everything with Google by Fritz Schneider, Nancy Blachman, and Eric Fredricksen (McGraw-Hill Osborne Media, 2003). ISBN 0-07-223174-2.',\n",
       "    'local_folder': 'site\\\\files\\\\search_engine_optimization',\n",
       "    'local_filename': 'site\\\\files\\\\search_engine_optimization\\\\2019-11-13-Google-Search.md',\n",
       "    'site_host': 'https://search-engine-optimization-blog.github.io',\n",
       "    'site_topic': 'Search Engine Optimization',\n",
       "    'page_slug': 'Google-Search.md',\n",
       "    'page_filename': '2019-11-13-Google-Search.md',\n",
       "    'page_url': 'https://search-engine-optimization-blog.github.io/Google-Search.md'}]}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
