{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.robots import check_robots_txt\n",
    "from lib.shingling import *\n",
    "from lib.renderer import *\n",
    "from lib.crawler import crawl_url\n",
    "from lib.database import *\n",
    "\n",
    "from web.site_generator import build_site_data, build_sites\n",
    "from web.publish import Publish\n",
    "\n",
    "import config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content1 = \"\"\"Many packages don't create a build for every single release which forces your pip to build from source. If you're happy to use the latest pre-compiled binary version, use --only-binary :all: to allow pip to use an older binary version.\"\"\"\n",
    "content2 = \"\"\"Most packages don't create a build for every single release which forces your pip to build from source. If you're happy to use the latest pre-compiled binary version, use --only-binary :all: to allow pip to use an older binary version.\"\"\"\n",
    "content3 = \"\"\"The C++ Build Tools allow you to build C++ libraries and applications targeting Windows desktop. They are the same tools that you find in Visual Studio 2019, Visual Studio 2017, and Visual Studio 2015 in a scriptable standalone installer. Now you only need to download the MSVC compiler toolset you need to build C++ projects on your build servers.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashdb = HashLookup()\n",
    "\n",
    "hashdb.add_hash('content1', content1)\n",
    "hashdb.add_hash('content2', content2)\n",
    "hashdb.add_hash('content3', content3)\n",
    "\n",
    "len(hashdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hashdb.get_hash('content1')[:5])\n",
    "print(hashdb.get_hash('content2')[:5])\n",
    "print(hashdb.get_hash('content3')[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashdb.th = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashdb.get_similar(content2, threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashdb.get_similarity_df(content2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_data = build_sites(cfg.sg_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.path.basename(site_data['folders'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def add_links(sites, page):\n",
    "\n",
    "    slug    = page['page_slug']\n",
    "    content = page['page_content']\n",
    "\n",
    "    other_pages = [p for fdr in sites for p in sites[fdr] if p['page_slug'] != slug]\n",
    "\n",
    "    for op in other_pages:\n",
    "\n",
    "        op_topic = op['page_topic']\n",
    "        op_url = op['page_url']\n",
    "\n",
    "        content = re.sub(r\"({})+\".format(op_topic), r\"(\\1)[{}]\".format(op_url), content, 1, flags=re.I)\n",
    "\n",
    "\n",
    "    page['page_content'] = content\n",
    "\n",
    "    return page\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = ['aa', 'aaa', 'aaaa', 'bbbbb']\n",
    "print(tt)\n",
    "ttt = [t for t in tt if len(t) == max([len(t) for t in tt])][0]\n",
    "print(ttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in sites:\n",
    "\n",
    "    pages = sites[site]\n",
    "\n",
    "    for page in pages:\n",
    "        \n",
    "        slug    = page['page_slug']\n",
    "        \n",
    "        #other_pages = [p for fdr in sites for p in sites[fdr] if p['page_slug'] != slug]\n",
    "        page = add_links(sites, page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "from lib.bert import *\n",
    "\n",
    "queries = ['trim a chrismas tree', 'jog on a path', 'kindle ebook', 'italian restaurant', 'internet landing page']\n",
    "ngrams = ['decorate a tree', 'run on a road', 'electric book', 'cafe in italy', 'website homepage']\n",
    "correct = ['christmas', 'exercise', 'kindle ebook', 'restaurant', 'realtor']\n",
    "\n",
    "bert = BERT(dims=100)\n",
    "\n",
    "bert.add_terms(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trim a chrismas tree:\n",
      "  decorate a tree\n",
      "  0.5615153312683105\n",
      "\n",
      "jog on a path:\n",
      "  run on a road\n",
      "  0.6457508206367493\n",
      "\n",
      "kindle ebook:\n",
      "  electric book\n",
      "  0.572453498840332\n",
      "\n",
      "italian restaurant:\n",
      "  cafe in italy\n",
      "  0.5768074989318848\n",
      "\n",
      "internet landing page:\n",
      "  website homepage\n",
      "  0.7847411036491394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q in queries:\n",
    "    best, sim = bert.get_most_similar(q)\n",
    "    print(q+':')\n",
    "    print('  ' + str(best))\n",
    "    print('  ' + str(sim))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.get_embedding('chrismas fun').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.LongTensor(bert.tokenizer.encode('exercise')).unsqueeze(0)\n",
    "outputs = bert.model(input_ids)\n",
    "lh = outputs[0]\n",
    "emb = torch.sum(lh, dim=1)\n",
    "\n",
    "input_ids = torch.LongTensor(bert.tokenizer.encode('kindle ebook')).unsqueeze(0)\n",
    "outputs2 = bert.model(input_ids)\n",
    "lh = outputs2[0]\n",
    "emb2 = torch.sum(lh, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emb.shape)\n",
    "print(emb2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.get_similar_df('sweat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in queries:\n",
    "    best, sim = bert.get_most_similar(q)\n",
    "    print(q+':')\n",
    "    print('  ' + str(best))\n",
    "    print('  ' + str(sim))\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
